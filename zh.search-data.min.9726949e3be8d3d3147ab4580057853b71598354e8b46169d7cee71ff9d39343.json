[{"id":0,"href":"/docs/section/","title":"home","section":"Docs","content":" home # 当前博客地址 -\u0026gt; Exfly blog\n"},{"id":1,"href":"/links/","title":"友链","section":"","content":" draveness # Draveness\u0026rsquo;s Blog面向信仰编程 docker k8s\n胡伟煌 # 胡伟煌 k8s\nzhile.io # zhile.io\n"},{"id":2,"href":"/about/","title":"关于","section":"","content":" ExFly Keep It Simple \u0026amp; Stupid.\nDescription # Focusing on computer technology, has basic operating system, computer network, data structure and algorithm knowledge, good at back-end, distributed, master certain data analysis knowledge, understand basic Linux server maintenance, understand the basic maintenance of the database, understand Basic network security, convinced that technology changes the world, and hopes to have a place in the computer industry.\nSkills # Golang \u0026gt; Java \u0026gt; Python \u0026gt; others Distribution, Microservice Linux Docker DevOps k8s Project # newdb # Implement RDBMS\nwiki wor2vec # DistributedSystems-MIT6.824 # pubsub # gracefully # money # jsonrpc boilerplate # dockerc # container-lab # go-mod-helper # ucore 操作系统 # ghttp a http server written by pure c # awesome blog springboot-ssm # graphql-mongo-boilerplate # openapi boilerplate # go cache in mem # "},{"id":3,"href":"/posts/db/postgres-internal/","title":"Postgres Internal","section":"Blog","content":"Postgres 内部工作机制\n介绍 # TODO: Postgres 介绍\nDatabase 和 Tables # 一个 Postgres server 中会有多个 database，每个 database 中会有多张 table，每个 table 中会有多条数据记录 (tuple), 每个数据记录会有多个字段。除此之外，每个 database 中还会有很多其他被管理的对象.\n多有的数据库对象多有唯一的 OID, 各数据库对象被保存在各自的 system catalogs 中. 例如 database 和 heap tuple 被保存在 pg_database 和 pg_class 中, 所以可以通过如下 sql 查询到\npostgres=# select datname, oid from pg_database where datname = \u0026#39;postgres\u0026#39;; postgres | 5 postgres=# select relname, oid from pg_class where relname=\u0026#39;actor\u0026#39;; actor | 16475 物理结构 # 当前我们介绍 文件目录存储结构。 Postgres 的一个数据库实例的数据存储在环境变量 PGDATA 中，通常 PGDATA 的值为 /var/lib/pgsql/data。同一台机器中可以部署多个 Postgres 服务实例，不同的服务实例可以使用不同的 PGDATA 以及不同的端口. PGDATA 子目录下包含数据库控制配置文件及数据文件。控制数据库服务实例运行的配置文件 postgresql.conf、pg_hba.conf 和 and pg_ident.conf 通常情况下也存储在 PGDATA 中，也可以把它们放到其他的地方(具体可以看 postgres 命令的启动参数或者 pg_ctl 的启动参数) PGDATA 中的文件结构如下:\n# tree -L 2 $PGDATA . ├── PG_VERSION ├── base │ ├── 1 │ └── pgsql_tmp ├── global │ ├── 1213 │ ├── 1213_fsm │ ├── 1213_vm │ ├── pg_control │ ├── pg_filenode.map │ └── pg_internal.init ├── pg_commit_ts ├── pg_dynshmem ├── pg_hba.conf ├── pg_ident.conf ├── pg_logical │ ├── mappings │ ├── replorigin_checkpoint │ └── snapshots ├── pg_multixact │ ├── members │ └── offsets ├── pg_notify ├── pg_replslot ├── pg_serial ├── pg_snapshots ├── pg_stat ├── pg_stat_tmp ├── pg_subtrans │ └── 0000 ├── pg_tblspc ├── pg_twophase ├── pg_wal │ ├── 000000010000000000000004 │ ├── 000000010000000000000005 │ └── archive_status ├── pg_xact │ └── 0000 ├── postgresql.auto.conf ├── postgresql.conf ├── postmaster.opts └── postmaster.pid https://www.postgresql.org/docs/devel/storage-file-layout.html\nItem Description PG_VERSION PostgreSQL 主要版本号的文件 base 每个数据库的子目录 global 系统表, 比如 pg_database pg_commit_ts 事务提交时间戳数据, Version 9.5 or later. pg_dynshmem dynamic shared memory subsystem 使用的文件, Version 9.4 or later. pg_logical status data for logical decoding pg_multixact multitransaction status data (used for shared row locks) pg_notify LISTEN/NOTIFY status data pg_replslot replication slot data pg_serial information about committed serializable transactions pg_snapshots exported snapshots pg_stat permanent files for the statistics subsystem pg_stat_tmp temporary files for the statistics subsystem pg_subtrans subtransaction status data pg_tblspc symbolic links to tablespaces pg_twophase state files for prepared transactions pg_wal WAL (Write Ahead Log) files. It is renamed from pg_xlog in Version 10. pg_xact transaction commit status data, It is renamed from pg_clog in Version 10. postgresql.auto.conf A file used for storing configuration parameters that are set by ALTER SYSTEM postmaster.opts A file recording the command-line options the server was last started with postmaster.pid A lock file recording the current postmaster process ID (PID), cluster data directory path, postmaster start timestamp, port number, Unix-domain socket directory path (could be empty), first valid listen_address (IP address or *, or empty if not listening on TCP), and shared memory segment ID (this file is not present after server shutdown) 数据库对象在文件存储中的布局 # 表结构如下:\ncreate database test; CREATE TABLE sal_emp ( id int primary key, name text, pay_by_quarter integer[], schedule text[][] ); INSERT INTO sal_emp VALUES ( 1, \u0026#39;Bill\u0026#39;, \u0026#39;{10000, 10000, 10000, 10000}\u0026#39;, \u0026#39;{{\u0026#34;meeting\u0026#34;, \u0026#34;lunch\u0026#34;}, {\u0026#34;training\u0026#34;, \u0026#34;presentation\u0026#34;}}\u0026#39; ); create index sal_emp_btree ON sal_emp (pay_by_quarter); create index sal_emp_gin ON sal_emp USING gin(pay_by_quarter); database、tables、index 等文件布局 # 在 9.0 版本之后，可以通过如下命令查看当前登录数据库的数据目录\ntest=# show data_directory; data_directory --------------------------- /home/vagrant/pgdata/data (1 row) 数据库位于 base 子目录下，数据库目录名字是其 oid。例如 test 的 oid 为 16966，他的目录名字为 16966.\ntest=# select oid,datname from pg_database; oid | datname -------+----------- 5 | postgres 16966 | test 1 | template1 4 | template0 $ ls -ld base/16966/ drwx------ 2 vagrant vagrant 12288 Sep 16 01:59 base/16966/ 每个不超过 1GB 的 table 或者 index 存储在其所属的 database 目录下的一个文件下.\ntest=# SELECT relname, oid, relfilenode FROM pg_class WHERE relname = \u0026#39;sal_emp\u0026#39;; relname | oid | relfilenode ---------+-------+------------- sal_emp | 16967 | 16967 test=# select * from pg_relation_filepath(\u0026#39;sal_emp\u0026#39;); pg_relation_filepath ---------------------- base/16966/16967 $ ls -alh base/16966/16967 -rw------- 1 vagrant vagrant 8.0K Sep 16 02:03 base/16966/16967 当一个 table 或者 index 的文件大小超过 1GB，PostgreSQL 会创建名字类似于 relfilenode.1, 并使用它。如果新的文件被填满了，下一个新的文件 relfilenode.2 将被创建，以此类推。\nls -alh base/16966/16967* -rw------- 1 vagrant vagrant 1.0G Sep 16 02:03 base/16966/16967 -rw------- 1 vagrant vagrant 1M Sep 16 02:03 base/16966/16967.1 单个 table/index 索引文件大小是可以配置的, PostgreSQL 编译选项是 \u0026ndash;with-segsize.\n仔细看数据库子目录，可以看到一些文件的后缀是 _fsm 和 _vm. 它们分别是对应文件的 free space map visibility map, 分别存储着每个文件中每个 page 的 空闲空间及是否可见。indexs 只有 fsm，没有 vm.\n可以使用如下命令反向查看文件对应的数据库对象\ntest=# SELECT pg_filenode_relation(0, 16967); pg_filenode_relation ---------------------- sal_emp (1 row) Tablespaces # PostgreSQL 中的 Tablespaces 是 base 目录之外的附加数据区域。 该功能已在 8.0 版本中实现。\nTODO: 补充更多细节\nHeap Table File 的内部文件格式 # storage-page-layout\n数据文件内部存储着大量的 pages, 每个 page 的固定大小为 8192 byte(8KB)。每个数据文件中的 page 从 0 开始编号，这些编号叫 block numbers.\npage 大小可以执行 sql 查看, SELECT current_setting('block_size');，其值可以在编译时添加参数修改\n\u0026ndash;with-blocksize=BLOCKSIZE\neq:\n./configure --with-blocksize=BLOCKSIZE --with-wal-blocksize=BLOCKSIZE\n不同种类的文件的内部布局不相同。如下为 heap table file 的文件布局\nheap table 中的 page包含如下三部分：\nheap tuple(s): 一个 heap tuple 代表 tables 中的一行记录。heap tuple 以栈入顺序添加到 page 中，即新的 tuple 回被插入到旧的前边; line pointer(s): line pointer 持有 tuple 的指针。line pointer 是一个变长指针，每个 line pointer 的序号从 1开始，当新的 tuple 被插入到 page 中时，指向新 tuple 的 line pointer 也会被从插入到 line pointer 数组的尾部; header data: 位于 page 的开始位置，记录了当前 page 的元数据。其详细构成定义在 PageHeaderData 中.其主要信息如下: pd_lsn: 保存了 WAL 的 LSN，于 WAL 机制相关； pd_checksum: 保存了当前 page 的 checksum； pd_lower, pd_upper: pd_lower 指向 linepointer 的末尾，pd_upper 指向最新的 tuple 的开始； pd_special, 这个变量是给 index 使用的，他指向 page 的末尾最后一个 byte lin pointer 的最后到最新 tuple 的开始是当前 page 的 空闲空间。 tuple identifier（TID）唯一标识一个 tuple. TID 包含两个指，block number 标识它所在的 page，offset number 标识它在 page 中的位置. 它的典型使用场景的 index.\n进程模型与内存模型 # 并发控制 # VACUUM Processing # Buffer Manager # Write Ahead Logging (WAL) # references # The Internals of PostgreSQL for database administrators and system developers "},{"id":4,"href":"/posts/docker/docker-change-default-bridge-ip/","title":"Docker 修改内置 bridge ip 不生效, 原因及解决办法","section":"Blog","content":"文章简介: docker 内置 bridge 的 ip 修改方法\nConfigure the default bridge network\ndocker daemon 的配置文件 /etc/docker/daemon.json 提供了自定义 docker builtin bridge 配置项.\n# 修改 daemon.json 中 \u0026#34;bip\u0026#34; { .... \u0026#34;bip: \u0026#34;100.100.1.1/24\u0026#34;, # bip: 修改 builtin bridge 的 ip \u0026#34;default-address-pools\u0026#34;:[{\u0026#34;base\u0026#34;:\u0026#34;100.100.0.0/16\u0026#34;,\u0026#34;size\u0026#34;:24}} ... } docker stop $(docker ps -qa) \u0026amp;\u0026amp; systemctl restart docker \u0026amp;\u0026amp; docker start $(docker ps -qa) 注意:\ndocker 如果已经有容器在运行，bip 的配置是不会生效。配置添加 {\u0026quot;debug\u0026quot;: true} 后会有如下日志:\nJun 11 18:31:45 hostname dockerd[323728]: time=\u0026#34;2022-06-11T18:31:45.251000544+08:00\u0026#34; level=debug msg=\u0026#34;RequestPool(LocalDefault, 172.17.0.0/16, , map[], false)\u0026#34; Jun 11 18:31:45 hostname dockerd[323728]: time=\u0026#34;2022-06-22T18:31:45.251092001+08:00\u0026#34; level=debug msg=\u0026#34;RequestAddress(LocalDefault/172.17.0.0/16, 172.17.0.1, map[RequestAddressType:com.docker.network.gateway])\u0026#34; Jun 11 18:31:45 hostname dockerd[323728]: time=\u0026#34;2022-06-11T18:31:45.251140234+08:00\u0026#34; level=debug msg=\u0026#34;Request address PoolID:172.17.0.0/16 App: ipam/default/data, ID: LocalDefault/172.17.0.0/16, DBIndex: 0x0, Bits: 65536, Unselected: 65534, Sequence: (0x80000000, 1)-\u0026gt;(0x0, 2046)-\u0026gt;(0x1, 1)-\u0026gt;end Curr:0 Serial:false PrefAddress:172.17.0.1 \u0026#34; Jun 11 18:31:45 hostname dockerd[323728]: time=\u0026#34;2022-06-11T18:31:45.272456556+08:00\u0026#34; level=info msg=\u0026#34;There are old running containers, the network config will not take affect\u0026#34; 解决方法是先将所有启动的容器 stop，然后重启 docker 配置生效后，start 容器\n"},{"id":5,"href":"/posts/linux/linux_network/","title":"Linux Network 如何配置及工作原理","section":"Blog","content":"文章简介：Linux Network 是如何使用及工作的\nLinux Network 非常复杂，其本身的机制非常复杂，每一种 Linux 发行版的网络也有差别，管理工具也纷繁复杂。这里先简单的描述在 Centos7 和 Ubuntu20.04 下如何配置网络，后简单的描述一下 network 是怎样工作的.\nCentos6 使用 network.service Centos7 network.service 和 NetworkManager.service 并存 Centos8 之后只使用 NetworkManager.service Ubuntu 使用 netplan 前端，NetworkManager/networkd 后端 Linux 网络设备 # TODO: 网络设备架构\nTODO: 网络接口命名\nnetwork.service 使用方法 # 修改 /etc/sysconfig/network-scripts 下的 ifcfg-*文件，之后执行 service network restart，配置即可生效。\n注意: 网络配置修改可能会停掉当前的 ssh 连接\n原理： network.service 会执行 /etc/rc.d/init.d/network start 启动网络服务，启动的过程中，使用 ip 命令启动各 device，使用 route 配置系统路由，使用 sysctl 配置等。\nNetworkNanager 使用方法 # Managing IP Networking 包含优点及如何使用\nD-Bus 的 unix socket: /var/run/dbus/system_bus_socket\nudev 配置: /etc/udev/udev.conf\n配置文件目录\n/etc/NetworkManager /etc/sysconfig/network-scripts 命令\nsystemctl restart NetworkManager.service # 重启服务 journalctl -u NetworkManager.service -f -n 100 # 查看日志 # 查询连接信息 nmcli conn # 添加连接 nmcli conn add type ethernet con-name eth0-static ifname eth0 ipv4.method manual ipv4.addresses \u0026#34;10.0.2.16/24\u0026#34; ipv4.gateway 10.0.2.2 ipv4.dns 114.114.114.114 ipv6.method auto # 修改连接 nmcli conn modify eth0-static ipv4.method manual ipv4.addresses 10.0.2.254/24 ipv4.gateway 10.0.2.2 # interactive edit connection nmcli conn edit \u0026lt;conn-name\u0026gt; # 系统 link(设备) nmcli dev # 重新应用配置 nmcli dev reapply eth0 # TUI nmtui 我们可以使用 nmcli 配置网络，当出现错误的时候，通过 journalctl 查看日志，分析配置错误原因，继续使用 nmcli 配置网络，直到网络服务配置好.\nNetworkNanager 工作原理 # NetworkNanager 有两个组件:\nNetworkManager daemon, 管理连接和监听并报告网络变化 一些管理前端（front-ends）, 比如 nmcli,一些 GUI 程序等 NetworkManager daemon 依赖于 D-Bus, 实现了管理连接的接口，底层使用 netlink 跟 linux 内核交互，获得设备信息，配置连接（比如修改 ipv4 的地址等）。nmcli 跟 D-Bus 交互连接 NetworkManager daemon，获得信息以及配置网络\n# ip 命令也是使用 netlink 跟 kernel 交互的 strace -fff ip a socket(AF_NETLINK, SOCK_RAW|SOCK_CLOEXEC, NETLINK_ROUTE) = 3 sendto(3, {{len=40, type=RTM_GETLINK, flags=NLM_F_REQUEST|NLM_F_DUMP, seq=1641054708, pid=0}, {ifi_family=AF_UNSPEC, ifi_type=ARPHRD_NETROM, ifi_index=0, ifi_flags=0, ifi_change=0}, {{nla_len=8, nla_type=IFLA_EXT_MASK}, 1}}, 40, 0, NULL, 0) = 40 recvmsg(3, {msg_name={sa_family=AF_NETLINK, nl_pid=0, nl_groups=00000000}, msg_namelen=12, msg_iov=[{iov_base=NULL, iov_len=0}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_TRUNC}, MSG_PEEK|MSG_TRUNC) recvmsg(3, {msg_name={sa_family=AF_NETLINK, nl_pid=0, nl_groups=00000000} sendto(3, {{len=40, type=RTM_GETADDR, flags=NLM_F_REQUEST|NLM_F_DUMP, seq=1641054709, pid=0}, {ifa_family=AF_UNSPEC, ifa_prefixlen=0, ifa_flags=0, ifa_scope=RT_SCOPE_UNIVERSE, ifa_index=0}, {nla_len=0, nla_type=IFA_UNSPEC}}, 40, 0, NULL, 0) = 40 recvmsg(3, {msg_name={sa_family=AF_NETLINK, nl_pid=0, nl_groups=00000000}, msg_namelen=12, msg_iov=[{iov_base=NULL, iov_len=0}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_TRUNC}, MSG_PEEK|MSG_TRUNC) TODO: 简单的 demo 使用 netlink 获得 ip 地址和配置 ip 地址 netlink-demo\nNetworkNanager 和 network.service 如何并存 # TODO:\nnetplan # The network configuration abstraction renderer\nnetplan 是一个方便在 linux 系统下配置网络的工具。我们通过 yaml 文件描述网络配置，\u0008netplan 会为我们生成我们选择的网络后端所需要的配置。当前支持后端 Systemd-networkd(default) 和 NetworkManager. Ubuntu 20.04 使用 netplan 作为网络配置工具.\n配置文件在 ls /{lib,etc,run}/netplan/*.yaml.\nls /etc/netplan\nnetplan design\nnetplan reference\nls /{lib,etc,run}/netplan/*.yaml # 配置文件位置 netplan generate # 生成配置 ls /run/systemd/network/ ip a # 各接口信息 lshw -class network # 硬件更详细的信息 ethtool enp0s3 # 修改配置 /etc/netplan/*.yaml netplan try # 测试是否生效 netplan apply # 应用配置 resolvectl status # resolve name netplan 一些通用的配置模版\nnetplan 配置 dhcp 自动获取 ip\ncat /etc/netplan/50-cloud-init.yaml # This file is generated from information provided by the datasource. Changes # to it will not persist across an instance reboot. To disable cloud-init\u0026#39;s # network configuration capabilities, write a file # /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg with the following: # network: {config: disabled} network: ethernets: enp0s3: dhcp4: true match: macaddress: 02:f6:7f:02:5c:72 set-name: enp0s3 version: 2 netplan 配置 static ip:\nnetwork: version: 2 ethernets: enp0s3: match: macaddress: 02:f6:7f:02:5c:72 addresses: - 10.10.10.2/24 gateway4: 10.10.10.1 set-name: enp0s3 nameservers: search: [mydomain, otherdomain] addresses: [10.10.10.1, 1.1.1.1] routes: - to: default via: 10.10.10.1 # netplan generate --root-dir /vagrant Name resolve\n# /run/systemd/resolve/stub-resolv.conf man 8 systemd-resolved systemd-resolve --statistics # resolve 服务 systemd-resolve --reset-statistics # 重置统计信息 systemctl restart systemd-resolved # 重启 ubuntu resolved 服务 resolvectl flush-caches # flush dns cache (old: systemd-resolve --flush-caches) # flush dns cache # https://www.linuxuprising.com/2019/07/how-to-flush-dns-cache-on-linux-for.html 总结 # 网络配置最终都是跟内核用 netlink 沟通，告诉 \u0008linux 内核网络配置，由 kernel 生效并工作。用户态管理程序典型代表： NetworkManager 和 systemd-networkd。熟悉了这些，便有了手段指挥 linux kernel 网络如何工作。剩下的就需要我们慢慢了解和学习 linux 网络提供了哪些功能。\nref # network_configuration linux kernel network docs Netplan configuration examples "},{"id":6,"href":"/posts/linux/boot_script/","title":"Linux 开机自启动脚本机器实现原理","section":"Blog","content":"文章简介：linux 设置开机自启动脚本的几种方式\nrc.local # 使用方法 # # 文件 /etc/rc.d/rc.local # centos 或者已经配置过此文件 echo \u0026#34;echo \u0026#39;do HelloWord\u0026#39;\u0026#34; \u0026gt;\u0026gt; /etc/rc.local # 已经有这个文件的时候 # ubuntu 没有 /etc/rc.local，需要创建文件，正确的配置 shebang printf \u0026#39;%s\\n\u0026#39; \u0026#39;#!/bin/bash\u0026#39; \u0026#39;exit 0\u0026#39; | sudo tee -a /etc/rc.local # 没有这个文件的时候 # Please note that you must run \u0026#39;chmod +x /etc/rc.d/rc.local\u0026#39; to ensure # that this script will be executed during boot. chmod +x /etc/rc.local systemctl enable --now rc-local systemctl status rc-local.service 工作原理 # systemctl cat rc-local.service [Unit] Description=/etc/rc.local Compatibility Documentation=man:systemd-rc-local-generator(8) ConditionFileIsExecutable=/etc/rc.local After=network.target [Service] Type=forking ExecStart=/etc/rc.local start TimeoutSec=0 RemainAfterExit=yes GuessMainPID=no 可以看到，\u0008ConditionFileIsExecutable=/etc/rc.local，当 /etc/rc.local 可执行的时候，systemd 拉起服务的过程中，会执行 /etc/rc.local.\ncrontab reboot # echo \u0026#39;@reboot /root/helloworld.sh\u0026#39; \u0026gt;\u0026gt; /etc/crontab crontab /etc/crontab crontab -e @reboot /root/helloworld.sh "},{"id":7,"href":"/posts/linux/systemd/","title":"Systemd 简单介绍，使用及如何 Debug","section":"Blog","content":"文章简介：学习 systemd 笔记\nLinux 的启动 很长一段时间采用 init 进程。init 是系统启动后的第一个用户级程序，pid=1.\ninit 程序有两个缺点:\n启动时间长。init 进程是串行启动，只有前一个进程启动完，才会启动下一个进程; 启动脚本复杂。init 进程只是执行启动脚本，不管其他事情。脚本需要自己处理各种情况，这往往使得脚本变得很长; 为了解决这个问题， Lennart Poettering 设计了 systemd。\nsystemd 是 Linux 电脑操作系统之下的一套中央化系统及设置管理程序（init），包括有守护进程（daemon）、程序库以及应用软件.已知的主流 Linux 发行版都把 systemd 作为 init. systemd 还附带很多基础工具，包括 timesync, hostnamectl 等一系列管理系统的工具。具体使用方法 这里 已经讲了很多，在这里不再赘述。\nsystemd 使用情况如下:\n系统 版本 备注 Ubuntu 15.04 及后续版本 1 Debian GNU/Linux Debian 8“Jessie”之后 Red Hat 及其派生品 7 及后续版本 Fedora 15 及后续版本 如何 debug # systemctl 提供了一些工具用于 debug。\nsystemctl cat multi-user.service # 查看 unit 配置 systemctl show multi-user.service # 显示某个 Unit 的所有底层参数 systemctl show -p CPUShares multi-user.service # 设置某个 Unit 的指定属性 systemctl list-dependencies --all multi-user.service # 列出依赖项 systemctl list-dependencies --all multi-user.service # 列出依赖项 systemd-analyze critical-chain # cli 显示瀑布状的启动过程流 systemd-analyze blame # 分析当前系统各项服务启动时间 systemd-analyze plot \u0026gt; boot.svg # 分析当前系统各项服务启动顺序 ls /etc/systemd/system/ # 修改 unit 文件 # 重新加载 service 信息 # systemd 会将所有 unit 信息缓存在内存，只有重新加载，修改内容才会生效 systemctl daemon-reload journalctl -u ssh.service -f # 查看 unit 日志 ref # Systemd 入门教程：命令篇 Linux 的启动流程 "},{"id":8,"href":"/posts/Go/go-scheduler/","title":"Go Scheduler","section":"Blog","content":"文章简介：golang scheduler 相关的内容\n简介 # 现在主流的线程模型分三种：内核级线程模型、用户级线程模型和两级线程模型（也称混合型线程模型），他们都很复杂。golang 属于 两级线程模型，是由 goroutine 实现的, goroutine 是 golang 最重要的特性之一，具有使用成本低、消耗资源低、能效高等特点，官方宣称原生 goroutine 并发成千上万不成问题. CSP(通信顺序进程) 是 golang 中的并发模型。本片文档将带领读者深入理解 golang 的 goroutine 的工作方式。\nstack 大小：\ngoroutine：2KB 线程：8MB 线程切换需要陷入内核；goroutine 只需要 getcontext 和 setcontext，30+ 个寄存器的读合写，非常快速\ncoroutine 的工作方式 # 说到 goroutine，就不得不说一下 coroutine。goroutine 是coroutine 在 golang 中的实现和优化。golang 作者之一 Russ Cox 有实现轻量的 libtask ，400 行左右代码描述清楚了 coroutine 是怎么工作的。\n建议亲自下载代码，看一下具体的实现逻辑, 需要比较简单的 c 基础\n一个 coroutine 比较核心的工作包括：\ntask 管理, 包括创建，销毁，切换等 coroutine 调度算法 先看一个简单的使用例子:\n// https://github.com/mixinlib/libtask/blob/651d7b69a4f7b10c798dcd544e6a25fd7505632c/primes.c#L12 void primetask(void *arg) { Channel *c, *nc; int p, i; c = arg; p = chanrecvul(c); if(p \u0026gt; goal) taskexitall(0); if(!quiet) printf(\u0026#34;%d\\n\u0026#34;, p); nc = chancreate(sizeof(unsigned long), buffer); taskcreate(primetask, nc, 32768); for(;;){ i = chanrecvul(c); if(i%p) chansendul(nc, i); } } void taskmain(int argc, char **argv) { int i; Channel *c; if(argc\u0026gt;1) goal = atoi(argv[1]); else goal = 100; printf(\u0026#34;goal=%d\\n\u0026#34;, goal); c = chancreate(sizeof(unsigned long), buffer); taskcreate(primetask, c, 32768); // 创建 coroutine，开始执行任务 primetask for(i=2;; i++) chansendul(c, i); } 会发现程序中没有常见的 c main 函数，实际是在 libtask 中实现了。整个程序是在一个线程中执行的，main 中创建了第一个 task，并启动 task 调度器\n// https://github.com/mixinlib/libtask/blob/main/task.c#L353 static void taskmainstart(void *v) { taskname(\u0026#34;taskmain\u0026#34;); taskmain(taskargc, taskargv); } int main(int argc, char **argv) { struct sigaction sa, osa; memset(\u0026amp;sa, 0, sizeof sa); sa.sa_handler = taskinfo; sa.sa_flags = SA_RESTART; sigaction(SIGQUIT, \u0026amp;sa, \u0026amp;osa); #ifdef SIGINFO sigaction(SIGINFO, \u0026amp;sa, \u0026amp;osa); #endif argv0 = argv[0]; taskargc = argc; taskargv = argv; if(mainstacksize == 0) mainstacksize = 256*1024; taskcreate(taskmainstart, nil, mainstacksize); taskscheduler(); // task 调度器启动 fprint(2, \u0026#34;taskscheduler returned in main!\\n\u0026#34;); abort(); return 0; } \u0008调度器的工作是从 runnable_queue 中获得一个可执行的任务，然后切换过去\nstatic void taskscheduler(void) { int i; Task *t; taskdebug(\u0026#34;scheduler enter\u0026#34;); for(;;){ if(taskcount == 0) exit(taskexitval); t = taskrunqueue.head; if(t == nil){ fprint(2, \u0026#34;no runnable tasks! %d tasks stalled\\n\u0026#34;, taskcount); exit(1); } deltask(\u0026amp;taskrunqueue, t); t-\u0026gt;ready = 0; taskrunning = t; tasknswitch++; taskdebug(\u0026#34;run %d (%s)\u0026#34;, t-\u0026gt;id, t-\u0026gt;name); contextswitch(\u0026amp;taskschedcontext, \u0026amp;t-\u0026gt;context); //print(\u0026#34;back in scheduler\\n\u0026#34;); taskrunning = nil; if(t-\u0026gt;exiting){ if(!t-\u0026gt;system) taskcount--; i = t-\u0026gt;alltaskslot; alltask[i] = alltask[--nalltask]; alltask[i]-\u0026gt;alltaskslot = i; free(t); } } } coroutine 是如何切换的呢？由计算机的工作原理知道，程序的执行是根据 pc 寄存器中的位置取指令执行的，执行 task 需要 stack 和寄存器。寄存器中的数据就是一个 coroutine 的 上下文，记录下来，set 成其他的 上下文的 寄存器的值，即可达到切换 coroutine 的目的。由于不同平台的寄存器不同，指令集不同，需要区分平台做不同的事情:\n// linux x86 平台的指令 // https://github.com/mixinlib/libtask/blob/651d7b69a4f7b10c798dcd544e6a25fd7505632c/asm.S#L43 #ifdef NEEDX86CONTEXT .globl SET SET: movl\t4(%esp), %eax movl\t8(%eax), %fs movl\t12(%eax), %es movl\t16(%eax), %ds movl\t76(%eax), %ss movl\t20(%eax), %edi movl\t24(%eax), %esi movl\t28(%eax), %ebp movl\t36(%eax), %ebx movl\t40(%eax), %edx movl\t44(%eax), %ecx movl\t72(%eax), %esp pushl\t60(%eax)\t/* new %eip */ movl\t48(%eax), %eax ret .globl GET GET: movl\t4(%esp), %eax movl\t%fs, 8(%eax) movl\t%es, 12(%eax) movl\t%ds, 16(%eax) movl\t%ss, 76(%eax) movl\t%edi, 20(%eax) movl\t%esi, 24(%eax) movl\t%ebp, 28(%eax) movl\t%ebx, 36(%eax) movl\t%edx, 40(%eax)\tmovl\t%ecx, 44(%eax) movl\t$1, 48(%eax)\t/* %eax */ movl\t(%esp), %ecx\t/* %eip */ movl\t%ecx, 60(%eax) leal\t4(%esp), %ecx\t/* %esp */ movl\t%ecx, 72(%eax) movl\t44(%eax), %ecx\t/* restore %ecx */ movl\t$0, %eax ret #endif // c 中的 Context struct Context { ucontext_t\tuc; }; 到此，用户级 task( coroutine )切换/调度/运行工作基本可以正常工作了。\n总结一下： coroutine 是单线程程序，可以串行的跑多个任务；多个任务通过一个 scheduler task 切换；切换的方式是通过替换cpu的寄存器实现的；切换的过程 os kernel 是不知道的; 由于 context switch 只是执行 30+ 个汇编指令，（os context switch 需要至少两次 context switch，以及一些状态维护，相比较来说）开销更小，cpu 的利用率会提高不少；使用同步的方式写出可以异步执行的代码，符合人的普遍思考方式。\nchannel 的工作方式 # 这一节内容和协程机理没有直接联系，但是因为channel总是伴随着goroutine出现，所以我们 顺便了解一下channel的原理也颇有好处。幸运的是，libtask 中也提供了channel的参考实现.\nchannel 分为有 buf 和 无 buf channel\n对于无 buf 的 channel # \u0008\u0008某一个 task 执行 chansendul 会直接将当前 task 加入 channel 的 send 队列中，然后触发一次 context switch。\u0008由于此时 channel 的 task 已经不在 taskrunqueue 中，发送数据到 channel 的 task 不会再被 taskscheduler 调度到，直到 chanrecvul task 被调度，且消费 channel 中的数据，此时 发送端 task 重新被放到可执行队列中。\n对于 有 buf 的 channel # 某一个 task 执行 chansendul，如果 channel buf 没有满，则会将数据复制到 channel buf 中，task 会继续执行。 如果 channel buf 已满，会将 send task 进 channel send 等待队列，并让出 cpu。 chanrecvul task 接受数据时候，如果 channel buf 不为空，则会直接消费 buf 中的数据；如果 buf 为空，则会 加入 channel 的 recv 等待队列，并让出 cpu，直到有新的数据被send 到 channel 为止。\ngolang goroutine 工作方式 # 了解了传统 coroutine 的实现方式，我们来看一下 golang 的 goroutine 的工作方式.\ngolang 实现了一种叫做 工作窃取任务调度.\n具体的功能介绍见: [翻译]The Go scheduler 具体的更详细的源码分析见： Go 调度器\n基本的组成元素和工作方式 # G、P、M\nG: 表示 Goroutine，每个 Goroutine 对应一个 G 结构体，G 存储 Goroutine 的运行堆栈、状态以及任务函数，可重用。G 并非执行体，每个 G 需要绑定到 P 才能被调度执行。 P: Processor，表示逻辑处理器， 对 G 来说，P 相当于 CPU 核，G 只有绑定到 P(在 P 的 local runq 中)才能被调度。对 M 来说，P 提供了相关的执行环境(Context)，如内存分配状态(mcache)，任务队列(G)等，P 的数量决定了系统内最大可并行的 G 的数量（前提：物理 CPU 核数 \u0026gt;= P 的数量），P 的数量由用户设置的 GOMAXPROCS 决定，但是不论 GOMAXPROCS 设置为多大，P 的数量最大为 256。 M: Machine，OS 线程抽象，代表着真正执行计算的资源，在绑定有效的 P 后，进入 schedule 循环；而 schedule 循环的机制大致是从 Global 队列、P 的 Local 队列以及 wait 队列中获取 G，切换到 G 的执行栈上并执行 G 的函数，调用 goexit 做清理工作并回到 M，如此反复。M 并不保留 G 状态，这是 G 可以跨 M 调度的基础，M 的数量是不定的，由 Go Runtime 调整，为了防止创建过多 OS 线程导致系统调度不过来，目前默认最大限制为 10000 个。 任务队列共有两种，一种是 p 上的本地队列，另一种是 global queue。当使用 go 关键词启动一个新的 goroutine 的时候，这既可能是全局的运行队列，也可能是处理器本地的运行队列。 runtime.runqput 可知，\n当 next 为 true 时，会将 Goroutine 设置到处理器的 runnext 作为下一个处理器执行的任务； 当 next 为 false 并且本地队列还有剩余空间，Goroutine 会被放到本地运行队列上； 当处理器的本地运行队列已经没有剩余空间时就会把本地队列中的一部分 Goroutine 和待加入的 Goroutine 通过 runtime.runqputslow 添加到调度器持有的全局运行队列上； 调度可执行任务的时候。 runtime.schedule 函数会从下面几个地方查找待执行的 Goroutine：\n为了保证公平，当全局运行队列中有待执行的 Goroutine 时，通过 schedtick 保证有一定几率会从全局的运行队列中查找对应的 Goroutine； 从处理器本地的运行队列中查找待执行的 Goroutine； 如果前两种方法都没有找到 Goroutine，会通过 runtime.findrunnable 进行阻塞地查找 Goroutine. 如果处理器没有任务可处理，它会按以下规则来执行，直到满足某一条规则：\n从本地队列获取任务 从全局队列获取任务 从网络轮询器获取任务 从其它的处理器的本地队列窃取任务 基于 goroutine 工作方式，有哪些优点点缺点，缺点如何规避 # 优缺点 # 优点：\n由于 golang goroutine 实现的 scheduler 比较简单，在大量 goroutine 调度时候性能消耗相比较 os thread 的调度来说会低至少一个数量级。 缺点：\nscheduler 的实现细节非常多，复杂 用户想 hack 自己的线程模型基本不可能 (当然 golang 的 scheduler 已经足够优秀了) goroutine pool # 10M(百万) goroutine 执行轻量操作会出现瓶颈，需要考虑 goroutine pool 来做优化了\ngolang 有 gc，goroutine 的创建与删除还是需要申请内存的，如果可以，尽量使用 pool，让 goroutine调度压力尽量小一点，用户程序复用 goroutine。可以使用 goroutine pool 实现 goroutine 的复用。同时不好的 pool 实现也会让一把锁破坏掉好的性能。是否使用，需要视情况而定.\nref # GMP 并发调度器深度解析之手撸一个高性能 goroutine pool\nScheduler structures\ngolang/proposal\nScalable Go Scheduler Design Doc\nScalable Go Scheduler Design Doc -zh Scalable Go Scheduler Design Doc -en\n第一代 Go Preemptive Scheduler Design Doc\nScheduling Multithreaded Computations by Work Stealing\n万字长文深入浅出 Golang Runtime\nGo\u0026rsquo;s work-stealing scheduler\nThe Go runtime scheduler source\ngo-scheduler go-scheduler-zh\nGo 语言十年而立，Go2 蓄势待发\n"},{"id":9,"href":"/posts/Go/high-performance-go-workshop/","title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","section":"Blog","content":"文章简介：优化Go程序\n原文地址 forked from here\nOverview # 本次研讨讲座的目标是让您能够诊断 Go 应用程序中的性能问题，并且修复这些问题。\n这一天，我们将从小做起 - 学习如何编写基准测试，然后分析一小段代码。然后讨论代码执行跟踪器，垃圾收集器和跟踪运行的应用程序。最后会有剩下的时间，您可以提出问题，并尝试编写您自己的代码。\nSchedule # 这里是这一天的时间安排表（大概）。\n开始时间 描述 09:00 欢迎 and 介绍 09:30 Benchmarking 10:45 休息 (15 分钟) 11:00 性能评估和分析 12:00 午餐 (90 分钟) 13:30 编译优化 14:30 执行追踪器 15:30 休息 (15 分钟) 15:45 内存和垃圾回收器 16:15 提示和旅行 16:30 练习 16:45 最后的问题和结论 17:00 结束 欢迎 # 你好，欢迎! 🎉\n该研讨的目的是为您提供诊断和修复 Go 应用程序中的性能问题所需的工具。\n在这一天里，我们将从一小部分开始 - 学习如何编写基准测试，然后分析一小段代码。然后扩展到，讨论 执行跟踪器，垃圾收集器 和跟踪正在运行的应用程序。剩下的将是提问的时间，尝试自己使用代码来实践。\n讲师 # Dave Cheney dave@cheney.net 开源许可和材料 # 该研讨会是 David Cheney 和 Francesc Campoy。\n此文章以 Creative Commons Attribution-ShareAlike 4.0 International 作为开源协议。\n预先工作 # 下面是您今天需要下载的几个软件\n讲习代码库 # 将源代码下载到本文档，并在以下位置获取代码示例 high-performance-go-workshop\n电脑执行环境 # 该项目工作环境目标为 Go 1.12。\nDownload Go 1.12\n如果您已经升级到 Go 1.13，也可以了。在次要的 Go 版本之间，优化选择总是会有一些小的变化，我会在继续进行时指出。\nGraphviz # 在 pprof 的部分需要 dot 程序，它附带的工具 graphviz 套件。\nLinux: [sudo] apt-get install graphviz OSX: MacPorts: sudo port install graphviz Homebrew: brew install graphviz Windows (untested) Google Chrome # 执行跟踪器上的这一部分需要使用 Google Chrome。它不适用于 Safari，Edge，Firefox 或 IE 4.01。\nDownload Google Chrome\n您的代码以进行分析和优化 # 当天的最后部分将是公开讲座，您可以在其中试验所学的工具。\n还有一些事 …​ # 这不是演讲，而是对话。我们将有很多时间来提问。\n如果您听不懂某些内容，或认为听不正确，请提出询问。\n1. 微处理器性能的过去，现在和未来 # 这是一个有关编写高性能代码的研讨会。在其他研讨会上，我谈到了分离的设计和可维护性，但是今天我们在这里谈论性能。\n今天，我想做一个简短的演讲，内容是关于我如何思考计算机发展历史以及为什么我认为编写高性能软件很重要。\n现实是软件在硬件上运行，因此要谈论编写高性能代码，首先我们需要谈论运行代码的硬件。\n1.1. Mechanical Sympathy # 目前有一个常用术语，您会听到像马丁·汤普森（Martin Thompson）或比尔·肯尼迪（Bill Kennedy）这样的人谈论 Mechanical Sympathy。 Mechanical Sympathy 这个名字来自伟大的赛车 手杰基·斯图尔特（Jackie Stewart），他曾三度获得世界一级方程式赛车冠军。他认为，最好的驾驶员对机器的工作原理有足够的了解，以便他们可以与机器和谐地工作。\n要成为一名出色的赛车手，您不需要成为一名出色的机械师，但您需要对汽车的工作原理有一个粗略的了解。\n我相信我们作为软件工程师也是如此。我认为会议室中的任何人都不会是专业的 CPU 设计人员，但这并不意味着我们可以忽略 CPU 设计人员面临的问题。\n1.2. 六个数量级 # 有一个常见的网络模型是这样的；\n当然这是荒谬的，但是它强调了计算机行业发生了多少变化。\n作为软件作者，我们这个会议室的所有人都受益于摩尔定律，即 40 年来，每 18 个月将芯片上可用晶体管的数量增加一倍。没有其他行业经历过 六个数量级 [ 1] 在一生的时间内改进其工具。\n但这一切都在改变。\n1.3. 计算机还在变快吗？ # 因此，最基本的问题是，面对上图所示的统计数据，我们应该问这个问题吗 计算机还在变快吗 ?\n如果计算机的速度仍在提高，那么也许我们不需要关心代码的性能，只需稍等一下，硬件制造商将为我们解决性能问题。\n1.3.1. 让我们看一下数据 # 这是经典的数据，您可以在 John L. Hennessy 和 David A. Patterson 的 Computer Architecture, A Quantitative Approach 等教科书中找到。该图摘自第 5 版\n在第 5 版中，轩尼诗（Hennessy）和帕特森（Patterson）提出了计算性能的三个时代\n首先是 1970 年代和 80 年代初期，这是形成性的年代。我们今天所知道的微处理器实际上并不存在，计算机是由分立晶体管或小规模集成电路制造的。成本，尺寸以及对材料科学理解的限制是限制因素。 从 80 年代中期到 2004 年，趋势线很明显。 计算机整数性能每年平均提高 52％。 计算机能力每两年翻一番，因此人们将摩尔定律（芯片上的晶体管数量增加一倍）与计算机性能混为一谈。 然后我们进入计算机性能的第三个时代。 事情变慢了。 总变化率为每年 22％。 之前的图表仅持续到 2012 年，但幸运的是在 2012 年 Jeff Preshing 写了 tool to scrape the Spec website and build your own graph.\n因此，这是使用 1995 年 到 2017 年 的 Spec 数据的同一图。\n对我而言，与其说我们在 2012 年 的数据中看到的步伐变化，不如说是 单核 性能已接近极限。 这些数字对于浮点数来说稍好一些，但是对于我们在做业务应用程序的房间中来说，这可能并不重要。\n1.3.2. 是的，计算机仍在变得越来越慢 # 关于摩尔定律终结的第一件事要记住，就是戈登·摩尔告诉我的事情。他说：\u0026ldquo;所有指数都结束了\u0026rdquo;。 — John Hennessy\n这是轩尼诗在 Google Next 18 及其图灵奖演讲中的引文。 他的观点是肯定的，CPU 性能仍在提高。 但是，单线程整数性能仍在每年提高 2-3％ 左右。 以这种速度，它将需要 20 年的复合增长才能使整数性能翻倍。 相比之下，90 年代的表现每天每两年翻一番。\n为什么会这样呢？\n1.4. 时针速度 # 2015 年的这张图很好地说明了这一点。 第一行显示了芯片上的晶体管数量。 自 1970 年代以来，这种趋势一直以大致线性的趋势线持续。 由于这是对数/林线图，因此该线性序列表示指数增长。\n但是，如果我们看中线，我们看到时钟速度十年来没有增加，我们看到 CPU 速度在 2004 年左右停滞了。\n下图显示了散热功率； 即变成电能的电能遵循相同的模式-时钟速度和 cpu 散热是相关的。\n1.5. 发热 # 为什么 CPU 会发热？ 这是一台固态设备，没有移动组件，因此此处的摩擦等效果并不（直接）相关。\n该图摘自 data sheet produced by TI。 在此模型中，N 型设备中的开关被吸引到正电压，P 型设备被正电压击退。\nCMOS 设备的功耗是三个因素的总和，CMOS 功耗是房间，办公桌上和口袋中每个晶体管的功率。\n静态功率。当晶体管是静态的，即不改变其状态时，会有少量电流通过晶体管泄漏到地。 晶体管越小，泄漏越多。 泄漏量随温度而增加。当您拥有数十亿个晶体管时，即使是很小的泄漏也会加起来！ 动态功率。当晶体管从一种状态转换到另一种状态时，它必须对连接到栅极的各种电容进行充电或放电。 每个晶体管的动态功率是电压乘以电容和变化频率的平方。 降低电压可以减少晶体管消耗的功率，但是较低的电压会使晶体管的开关速度变慢。 撬棍或短路电流。我们喜欢将晶体管视为数字设备，无论其处于开启状态还是处于原子状态，都占据一种状态或另一种状态。 实际上，晶体管是模拟设备。 当开关时，晶体管开始几乎全部截止，并转变或切换到几乎全部导通的状态。 这种转换或切换时间非常快，在现代处理器中约为皮秒，但是当从 Vcc 到地的电阻路径很低时，这仍然代表了一段时间。 晶体管切换的速度越快，其频率就会耗散更多的热量。 1.6. Dennard 扩展的终结 # 要了解接下来发生的事情，我们需要查看 Robert H. Dennard 于 1974 年共同撰写的论文。 丹纳德的缩放定律大致上指出，随着晶体管的变小，它们的 power density 保持恒定。 较小的晶体管可以在较低的电压下运行，具有较低的栅极电容，并且开关速度更快，这有助于减少动态功率。\n那么，结果如何呢？\n事实并非如此。 当晶体管的栅极长度接近几个硅原子的宽度时，晶体管尺寸，电压与重要的泄漏之间的关系就破裂了。\n它是在 Micro-32 conference in 1999 假定的，如果我们遵循了提高时钟速度和缩小晶体管尺寸的趋势线，那么在处理器一代之内晶体管结将接近核反应堆堆芯的温度。显然，这是荒谬的。奔腾 4 marked the end of the line 适用于单核高频消费类 CPU。\n返回此图，我们看到时钟速度停止的原因是 CPU 超出了我们冷却时钟的能力。 到 2006 年，减小晶体管的尺寸不再提高其功率效率。\n现在我们知道，减小 CPU 功能的大小主要是为了降低功耗。 降低功耗不仅意味着“绿色”，例如回收利用，还可以拯救地球。 主要目标是保持功耗，从而保持散热， below levels that will damage the CPU.\n但是，图中的一部分在不断增加，即管芯上的晶体管数量。cpu 的行进具有尺寸特征，在相同的给定面积内有更多的晶体管，既有正面影响，也有负面影响。\n同样，如您在插入物中所看到的，直到大约 5 年前，每个晶体管的成本一直在下降，然后每个晶体管的成本又开始回升。\n制造较小的晶体管不仅变得越来越昂贵，而且变得越来越困难。 2016 年 的这份报告显示了芯片制造商认为在 2013 年 会发生什么的预测。两年后，他们错过了所有预测，尽管我没有此报告的更新版本，但没有迹象表明他们将能够扭转这一趋势。\n英特尔，台积电，AMD 和三星都要花费数十亿美元，因为它们必须建造新的晶圆厂，购买所有新的工艺工具。因此，尽管每个芯片的晶体管数量持续增加，但其单位成本却开始增加。\n甚至以纳米为单位的术语 栅极长度 也变得模棱两可。 各种制造商以不同的方式测量其晶体管的尺寸，从而使它们在没有交付的情况下可以展示比竞争对手少的数量。这是 CPU 制造商的非 GAAP 收益报告模型。\n1.7. 更多的核心 # 达到温度和频率限制后，不再可能使单个内核的运行速度快两倍。 但是，如果添加另一个内核，则可以提供两倍的处理能力-如果软件可以支持的话。\n实际上，CPU 的核心数量主要由散热决定。 Dennard 缩放的末尾意味着 CPU 的时钟速度是 1 到 4 Ghz 之间的任意数字，具体取决于它的温度。在谈论基准测试时，我们会很快看到这一点。\n1.8. 阿姆达尔定律 # CPU 并没有变得越来越快，但是随着超线程和多核它们变得越来越宽。 移动部件为双核，台式机部件为四核，服务器部件为数十个内核。 这将是计算机性能的未来吗？ 不幸的是没有。\n阿姆达尔定律以 IBM/360 的设计者吉姆·阿姆达尔（Gene Amdahl）的名字命名，它是一个公式，它给出了在固定工作负载下任务执行延迟的理论上的加速，这可以通过改善资源的系统来实现。\n阿姆达尔定律告诉我们，程序的最大速度受程序顺序部分的限制。 如果您编写的程序的执行力的 95％ 可以并行运行，即使有成千上万的处理器，则程序执行的最大速度也将限制为 20 倍。\n考虑一下您每天使用的程序，它们的执行量中有多少是可以解析的？\n1.9. 动态优化 # 由于时钟速度停滞不前，并且由于抛出额外的内核而产生的回报有限，因此，提速来自何处？ 它们来自芯片本身的体系结构改进。 这些是五到七年的大型项目，名称如下 Nehalem, Sandy Bridge, and Skylake.\n在过去的二十年中，性能的改善大部分来自体系结构的改善:\n1.9.1. 乱序执行 # 乱序，也称为超标量，执行是一种从 CPU 正在执行的代码中提取所谓的 指令级并行性 的方法。 现代 CPU 在硬件级别有效地执行 SSA，以识别操作之间的数据依赖性，并在可能的情况下并行运行独立的指令。\n但是，任何一段代码固有的并行性数量是有限的。它也非常耗电。大多数现代 CPU 在每个内核上都部署了六个执行单元，因为在流水线的每个阶段将每个执行单元连接到所有其他执行单元的成本为 n 平方。\n1.9.2. 预测执行 # 除最小的微控制器外，所有 CPU 均使用 指令管道 来重叠指令 获取/解码/执行/提交 周期中的部分。\n指令流水线的问题是分支指令，平均每 5 到 8 条指令出现一次。当 CPU 到达分支时，它不能在分支之外寻找其他指令来执行，并且直到知道程序计数器也将在何处分支之前，它才能开始填充其管道。推测执行使 CPU 可以“猜测”分支仍要处理的路径，同时仍在处理分支指令！\n如果 CPU 正确预测了分支，则它可以保持其指令流水线满。如果 CPU 无法预测正确的分支，则当它意识到错误时，必须回滚对其 architectural state 所做的任何更改。由于我们都在通过 Spectre 样式漏洞进行学习，因此有时这种回滚并没有像希望的那样无缝。\n当分支预测率较低时，投机执行可能会非常耗电。如果分支预测错误，不仅 CPU 回溯到预测错误的地步，而且浪费在错误分支上的能量也被浪费了。\n所有这些优化导致我们看到的单线程性能的提高，但要付出大量晶体管和功率的代价。\nCliff Click 的 精彩演讲 认为乱序，并且推测性执行对于尽早开始缓存未命中最有用，从而减少了观察到的缓存延迟。\n1.10. 现代 CPU 已针对批量操作进行了优化 # 现代处理器就像是由硝基燃料驱动的有趣的汽车，它们在四分之一英里处表现出色。不幸的是，现代编程语言就像蒙特卡洛一样，充满了曲折。- 大卫·昂加（David Ungar）\n这是来自有影响力的计算机科学家，SELF 编程语言的开发人员 David Ungar 的引言，在很旧的演讲中就引用了 I found online.\n因此，现代 CPU 已针对批量传输和批量操作进行了优化。 在每个级别，操作的设置成本都会鼓励您进行大量工作。 一些例子包括\n内存不是按字节加载，而是按高速缓存行的倍数加载，这就是为什么对齐变得不再像以前的计算机那样成为问题的原因。 MMX 和 SSE 等向量指令允许一条指令同时针对多个数据项执行，前提是您的程序可以以这种形式表示。 1.11. 现代处理器受内存延迟而不是内存容量的限制 # 如果 CPU 占用的状况还不够糟，那么从内存方面来的消息就不会好多了。\n连接到服务器的物理内存在几何上有所增加。 我在 1980 年代的第一台计算机具有数千字节的内存。 当我读高中时，我所有的论文都是用 3.8 MB 的 386 写在 386 上的。 现在，查找具有数十或数百 GB RAM 的服务器已变得司空见惯，而云提供商则将其推向了 TB 的 TB。\n但是，处理器速度和内存访问时间之间的差距继续扩大。\n但是，就丢失处理器等待内存的处理器周期而言，物理内存仍与以往一样遥不可及，因为内存无法跟上 CPU 速度的提高。\n因此，大多数现代处理器受内存延迟而不是容量的限制。\n1.12. 缓存控制着我们周围的一切 # 几十年来，解决处理器/内存上限的解决方案是添加缓存-一块较小的快速内存，位置更近，现在直接集成到 CPU 中。\n但;\n数十年来，L1 一直停留在每个核心 32kb L2 在最大的英特尔部分上已缓慢爬升到 512kb L3 现在在 4-32mb 范围内，但其访问时间可变 受高速缓存限制的大小是因为它们 physically large on the CPU die，会消耗大量功率。 要使缓存未命中率减半，您必须将缓存大小提高 四倍。\n1.13. 免费午餐结束了 # 2005 年，C++ 委员会负责人 Herb Sutter 撰写了一篇题为 免费午餐结束 的文章。 萨特（Sutter）在他的文章中讨论了我涵盖的所有要点，并断言未来的程序员将不再能够依靠较快的硬件来修复较慢的程序或较慢的编程语言。\n十多年后的今天，毫无疑问，赫伯·萨特（Herb Sutter）是正确的。内存很慢，缓存太小，CPU 时钟速度倒退了，单线程 CPU 的简单世界早已一去不复返了。\n摩尔定律仍然有效，但是对于我们这个房间里的所有人来说，免费午餐已经结束。\n1.14. 结束 # 我要引用的数字是到 2010 年：30GHz，100 亿个晶体管和每秒 1 兆指令。— Pat Gelsinger, Intel CTO, April 2002\n很明显，如果没有材料科学方面的突破，CPU 性能恢复到同比 52％ 增长的可能性几乎很小。普遍的共识是，故障不在于材料科学本身，而在于晶体管的使用方式。用硅表示的顺序指令流的逻辑模型导致了这种昂贵的最终结果。\n在线上有许多演示文稿可以重述这一点。 它们都具有相同的预测-将来的计算机将不会像今天这样编程。 有人认为它看起来更像是带有数百个非常笨拙，非常不连贯的处理器的图形卡。 其他人则认为，超长指令字（VLIW）计算机将成为主流。 所有人都同意，我们当前的顺序编程语言将与此类处理器不兼容。\n我的观点是这些预测是正确的，此时硬件制造商挽救我们的前景严峻。 但是，我们可以为今天拥有的硬件优化当前程序的范围是 巨大的。 里克·哈德森（Rick Hudson）在 GopherCon 2015 大会上谈到 以\u0026quot;良好的循环\u0026quot;重新参与，该软件可以与我们今天拥有的硬件一起工作，而不是仅适用于这种硬件 。\n查看我之前显示的图表，从 2015 年到 2018 年，整数性能最多提高了 5-8％，而内存延迟却最多，Go 团队将垃圾收集器的暂停时间减少了 两个数量级。 与使用 Go 1.6 的相同硬件上的同一程序相比，Go 1.11 程序具有更好的 GC 延迟。 这些都不是来自硬件。\n因此，为了在当今世界的当今硬件上获得最佳性能，您需要一种编程语言，该语言应：\n之所以编译而不是解释，是因为解释后的编程语言与 CPU 分支预测变量和推测性执行之间的交互作用很差。 您需要一种语言来允许编写有效的代码，它需要能够谈论位和字节，并且必须有效地说明整数的长度，而不是假装每个数字都是理想的浮点数。 您需要一种使程序员能够有效地谈论内存，思考结构与 Java 对象的语言，因为所有的指针追逐都会给 CPU 高速缓存带来压力，而高速缓存未命中会消耗数百个周期。 随应用程序的性能而扩展到多个内核的编程语言取决于它使用缓存的效率以及在多个内核上并行化工作的效率。 显然，我们在这里谈论 Go，我相信 Go 拥有了我刚才描述的许多特征。\n1.14.1. 这对我们意味着什么？ # 只有三种优化：少做些。少做一次。更快地做。\n最大的收益来自 1，但我们将所有时间都花在 3 上。 — Michael Fromberger\n本讲座的目的是说明，当您谈论程序或系统的性能时，完全是在软件中。等待更快的硬件来挽救一天真是愚蠢的事情。\n但是有个好消息，我们可以在软件上进行大量改进，而这就是我们今天要讨论的。\n1.14.2. 进一步阅读 # The Future of Microprocessors, Sophie Wilson JuliaCon 2018 50 Years of Computer Architecture: From Mainframe CPUs to DNN TPUs, David Patterson The Future of Computing, John Hennessy The future of computing: a conversation with John Hennessy (Google I/O \u0026lsquo;18) 2. Benchmarking # 测量两次，取一次。 — Ancient proverb\n在尝试改善一段代码的性能之前，首先我们必须了解其当前性能。\n本节重点介绍如何使用 Go 测试框架构建有用的基准，并提供了避免陷阱的实用技巧。\n2.1. 标杆基准规则 # 在进行基准测试之前，必须具有稳定的环境才能获得可重复的结果。\n机器必须处于闲置状态-不要在共享硬件上进行配置，不要在等待较长基准测试运行时浏览网络。 注意节能和热缩放。这些在现代笔记本电脑上几乎是不可避免的。 避免使用虚拟机和共享云托管；对于一致的测量，它们可能太嘈杂。 如果负担得起，请购买专用的性能测试硬件。机架安装，禁用所有电源管理和热量缩放功能，并且永远不要在这些计算机上更新软件。 最后一点是从系统管理的角度来看糟糕的建议，但是如果软件更新改变了内核或库的执行方式 -想想 Spectre 补丁- 这将使以前的任何基准测试结果无效。\n对于我们其他人，请进行前后采样，然后多次运行以获取一致的结果。\n2.2. Using the testing package for benchmarking # testing 包内置了对编写基准测试的支持。 如果我们有一个简单的函数，像这样：\nfunc Fib(n int) int { switch n { case 0: return 0 case 1: return 1 default: return Fib(n-1) + Fib(n-2) } } 我们可以使用 testing 包通过这种形式为函数编写一个 基准。\nfunc BenchmarkFib20(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { Fib(20) // run the Fib function b.N times } } 基准测试功能与您的测试一起存在于 _test.go 文件中。\nBenchmarks are similar to tests, the only real difference is they take a *testing.B rather than a *testing.T. Both of these types implement the testing.TB interface which provides crowd favorites like Errorf(), Fatalf(), and FailNow().\n基准测试类似于测试，唯一的不同是基准测试采用的是 *testing.B，而不是 *testing.T。这两种类型都实现了 testing.TB 接口，该接口提供了诸如 Errorf()，Fatalf() 和 FailNow() 之类的方法。\n2.2.1. 运行软件包的基准测试 # As benchmarks use the testing package they are executed via the go test subcommand. However, by default when you invoke go test, benchmarks are excluded.\n当基准测试使用 测试 软件包时，它们通过 go test 子命令执行。 但是，默认情况下，当您调用 go test 时，将排除基准测试。\n要在包中显式运行基准测试，请使用 -bench 标志。-bench 采用与您要运行的基准测试名称匹配的正则表达式，因此调用包中所有基准测试的最常见方法是 -bench=. 这是一个例子：\n% go test -bench=. ./examples/fib/ goos: darwin goarch: amd64 BenchmarkFib20-8 30000 40865 ns/op PASS ok _/Users/dfc/devel/high-performance-go-workshop/examples/fib 1.671s go test 还将在匹配基准之前运行软件包中的所有测试，因此，如果软件包中有很多测试，或者它们花费很长时间，则可以通过 go test 提供的 -run 参数来排除它们，正则表达式不匹配； 即。\ngo test -run=^$ 2.2.2. 基准如何运作 # 每个基准函数调用的 b.N 值都不同，这是基准应运行的迭代次数。\n如果基准功能在 1 秒内（默认值）在 1 秒内完成，则 b.N 从 1 开始，然后 b.N 增加，基准功能再次运行。\nb.N increases in the approximate sequence; 1, 2, 3, 5, 10, 20, 30, 50, 100, and so on. The benchmark framework tries to be smart and if it sees small values of b.N are completing relatively quickly, it will increase the the iteration count faster.\nb.N 以近似顺序增加；1, 2, 3, 5, 10, 20, 30, 50, 100 等。基准框架试图变得聪明，如果看到较小的 b.N 值相对较快地完成，它将更快地增加迭代次数。\n查看上面的示例，BenchmarkFib20-8 发现循环的大约 30,000 次迭代花费了超过一秒钟的时间。从那里基准框架计算得出，每次操作的平均时间为 40865ns。\n后缀 -8 与用于运行此测试的 GOMAXPROCS 的值有关。 该数字 GOMAXPROCS 默认为启动时 Go 进程可见的 CPU 数。 您可以使用 -cpu 标志来更改此值，该标志带有一个值列表以运行基准测试。\n% go test -bench=. -cpu=1,2,4 ./examples/fib/ goos: darwin goarch: amd64 BenchmarkFib20 30000 39115 ns/op BenchmarkFib20-2 30000 39468 ns/op BenchmarkFib20-4 50000 40728 ns/op PASS ok _/Users/dfc/devel/high-performance-go-workshop/examples/fib 5.531s 这显示了以 1、2 和 4 核运行基准测试。 在这种情况下，该标志对结果几乎没有影响，因为该基准是完全顺序的。\n2.2.3. 提高基准精度 # fib 功能是一个稍作设计的示例-除非您编写 TechPower Web 服务器基准测试-否则您的业务不太可能会因您能够快速计算出斐波那契序列中的第 20 个数字而受到限制。但是，基准确实提供了有效基准的忠实示例。\n具体来说，您希望基准测试可以运行数万次迭代，以便每个操作获得良好的平均值。如果您的基准测试仅运行 100 或 10 次迭代，则这些运行的平均值可能会有较高的标准偏差。如果您的基准测试运行了数百万或数十亿次迭代，则平均值可能非常准确，但会受到代码布局和对齐方式的影响。\n为了增加迭代次数，可以使用 -benchtime 标志来增加基准时间。 例如：\n% go test -bench=. -benchtime=10s ./examples/fib/ goos: darwin goarch: amd64 BenchmarkFib20-8 300000 39318 ns/op PASS ok _/Users/dfc/devel/high-performance-go-workshop/examples/fib 20.066s 运行相同的基准，直到达到 b.N 的值，并花费了超过 10 秒的时间才能返回。由于我们的运行时间增加了 10 倍，因此迭代的总数也增加了 10 倍。 结果并没有太大变化，这就是我们所期望的。\n为什么报告的总时间是 20 秒，而不是 10 秒？ 如果您有一个基准运行数百万或数十亿次迭代，导致每次操作的时间在微秒或纳秒范围内，则您可能会发现基准值不稳定，因为热缩放，内存局部性，后台处理，gc 活动等。\n对于每次操作以 10 纳秒或一位数纳秒为单位的时间，指令重新排序和代码对齐的相对论效应将对基准时间产生影响。\n要使用 -count 标志来多次运行基准测试：\n% go test -bench=Fib1 -count=10 ./examples/fib/ goos: darwin goarch: amd64 BenchmarkFib1-8 2000000000 1.99 ns/op BenchmarkFib1-8 1000000000 1.95 ns/op BenchmarkFib1-8 2000000000 1.99 ns/op BenchmarkFib1-8 2000000000 1.97 ns/op BenchmarkFib1-8 2000000000 1.99 ns/op BenchmarkFib1-8 2000000000 1.96 ns/op BenchmarkFib1-8 2000000000 1.99 ns/op BenchmarkFib1-8 2000000000 2.01 ns/op BenchmarkFib1-8 2000000000 1.99 ns/op BenchmarkFib1-8 1000000000 2.00 ns/op Fib(1) 基准测试大约需要 2 纳秒，方差为 +/- 2％。\nGo 1.12 中的新增功能是 -benchtime 标志，现在需要进行多次迭代，例如。-benchtime=20x，它将准确地运行您的代码 benchtime 的时间。\n尝试以10倍，20倍，50倍，100倍和300倍的 `-benchtime` 运行上面的 fib 测试。 你看到了什么？ If you find that the defaults that go test applies need to be tweaked for a particular package, I suggest codifying those settings in a Makefile so everyone who wants to run your benchmarks can do so with the same settings.\n如果您发现需要针对特定的软件包调整 go test 的默认设置，我建议将这些设置编入 Makefile 中，这样，每个想要运行基准测试的人都可以使用相同的设置。\n2.3. 将基准与 Benchstat 进行比较 # 在上一节中，我建议多次运行基准测试以使更多数据平均。由于本章开头提到的电源管理，后台过程和热管理的影响，这对于任何基准测试都是很好的建议。\n我将介绍 Russ Cox 的一个工具 benchstat.\n% go get golang.org/x/perf/cmd/benchstat Benchstat 可以进行一系列基准测试，并告诉您它们的稳定性。 这是有关电池供电的 Fib(20) 示例。\n% go test -bench=Fib20 -count=10 ./examples/fib/ | tee old.txt goos: darwin goarch: amd64 BenchmarkFib20-8 50000 38479 ns/op BenchmarkFib20-8 50000 38303 ns/op BenchmarkFib20-8 50000 38130 ns/op BenchmarkFib20-8 50000 38636 ns/op BenchmarkFib20-8 50000 38784 ns/op BenchmarkFib20-8 50000 38310 ns/op BenchmarkFib20-8 50000 38156 ns/op BenchmarkFib20-8 50000 38291 ns/op BenchmarkFib20-8 50000 38075 ns/op BenchmarkFib20-8 50000 38705 ns/op PASS ok _/Users/dfc/devel/high-performance-go-workshop/examples/fib 23.125s % benchstat old.txt name time/op Fib20-8 38.4µs ± 1% benchstat 告诉我们平均值为 38.8 微秒，样本之间的变化为 +/- 2％。这对于电池供电来说相当不错。\n第一次运行是最慢的，因为操作系统已降低 CPU 时钟以节省电量。 接下来的两次运行是最快的，因为操作系统确定这不是工作的短暂高峰，并且提高了时钟速度以尽快完成工作，从而希望能够返回睡觉。 其余运行是操作系统和供热生产的 BIOS 交互功耗。 2.3.1. Improve Fib # 确定两组基准之间的性能差异可能是乏味且容易出错的。benchstat 可以帮助我们。\nSaving the output from a benchmark run is useful, but you can also save the binary that produced it. This lets you rerun benchmark previous iterations. To do this, use the -c flag to save the test binary—​I often rename this binary from .test to .golden.\n保存来自基准运行的输出很有用，但是您也可以保存产生它的 二进制文件。 这使您可以重新运行基准测试以前的迭代。 为此，请使用 -c标志保存测试二进制文件我经常将此二进制文件从 .test 重命名为 .golden。\n% go test -c % mv fib.test fib.golden 先前的 Fib 功能具有斐波那契系列中第 0 和第 1 个数字的硬编码值。之后，代码以递归方式调用自身。今天晚些时候，我们将讨论递归的成本，但是目前，假设递归的成本是很高的，尤其是因为我们的算法使用的是指数时间。\n对此的简单解决方法是对斐波那契数列中的另一个数字进行硬编码，从而将每个可回溯调用的深度减少一个。\nfunc Fib(n int) int { switch n { case 0: return 0 case 1: return 1 case 2: return 1 default: return Fib(n-1) + Fib(n-2) } } 该文件还包含对 Fib 的综合测试。如果没有通过验证当前行为的测试，请勿尝试提高基准。\n为了比较我们的新版本，我们编译了一个新的测试二进制文件并对其进行了基准测试，并使用 benchstat 来比较输出。\n% go test -c % ./fib.golden -test.bench=. -test.count=10 \u0026gt; old.txt % ./fib.test -test.bench=. -test.count=10 \u0026gt; new.txt % benchstat old.txt new.txt name old time/op new time/op delta Fib20-8 44.3µs ± 6% 25.6µs ± 2% -42.31% (p=0.000 n=10+10) 比较基准时需要检查三件事\n旧时代和新时期的方差 ±。 1-2％ 是好的，3-5％ 是可以的，大于 5％，并且您的某些样本将被认为不可靠。 比较一侧差异较大的基准时，请注意不要有所改善。 p 值。 p 值小于 0.05 表示良好，大于 0.05 表示基准可能没有统计学意义。 缺少样本。benchstat 将报告它认为有效的旧样本和新样本中的多少个，有时即使您执行了 -count=10，也可能只报告了 9 个。 10％ 或更低的拒绝率是可以的，高于 10％ 可能表明您的设置不稳定，并且您可能比较的样本太少。 2.4. 避免基准化启动成本 # 有时，您的基准测试具有一次运行设置成本。b.ResetTimer() 将用于忽略设置中的时间。\nfunc BenchmarkExpensive(b *testing.B) { boringAndExpensiveSetup() b.ResetTimer() // (1) for n := 0; n \u0026lt; b.N; n++ { // function under test } } | 1 | 重置基准计时器 |\n如果 每个循环 迭代都有一些昂贵的设置逻辑，请使用 b.StopTimer() 和 b.StartTimer() 暂停基准计时器。\nfunc BenchmarkComplicated(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { b.StopTimer() // (1) complicatedSetup() b.StartTimer() // (2) // function under test } } | 1 | 暂停基准测试计时器 | | 2 | 恢复计时器 |\n2.5. 基准分配 # 分配数量和大小与基准时间密切相关。 您可以告诉 testing 框架记录被测代码分配的数量。\nfunc BenchmarkRead(b *testing.B) { b.ReportAllocs() for n := 0; n \u0026lt; b.N; n++ { // function under test } } 这是一个使用 bufio 软件包基准测试的示例。\n% go test -run=^$ -bench=. bufio goos: darwin goarch: amd64 pkg: bufio BenchmarkReaderCopyOptimal-8 20000000 103 ns/op BenchmarkReaderCopyUnoptimal-8 10000000 159 ns/op BenchmarkReaderCopyNoWriteTo-8 500000 3644 ns/op BenchmarkReaderWriteToOptimal-8 5000000 344 ns/op BenchmarkWriterCopyOptimal-8 20000000 98.6 ns/op BenchmarkWriterCopyUnoptimal-8 10000000 131 ns/op BenchmarkWriterCopyNoReadFrom-8 300000 3955 ns/op BenchmarkReaderEmpty-8 2000000 789 ns/op 4224 B/op 3 allocs/op BenchmarkWriterEmpty-8 2000000 683 ns/op 4096 B/op 1 allocs/op BenchmarkWriterFlush-8 100000000 17.0 ns/op 0 B/op 0 allocs/op You can also use the go test -benchmem flag to force the testing framework to report allocation statistics for all benchmarks run.\n% go test -run=^$ -bench=. -benchmem bufio goos: darwin goarch: amd64 pkg: bufio BenchmarkReaderCopyOptimal-8 20000000 93.5 ns/op 16 B/op 1 allocs/op BenchmarkReaderCopyUnoptimal-8 10000000 155 ns/op 32 B/op 2 allocs/op BenchmarkReaderCopyNoWriteTo-8 500000 3238 ns/op 32800 B/op 3 allocs/op BenchmarkReaderWriteToOptimal-8 5000000 335 ns/op 16 B/op 1 allocs/op BenchmarkWriterCopyOptimal-8 20000000 96.7 ns/op 16 B/op 1 allocs/op BenchmarkWriterCopyUnoptimal-8 10000000 124 ns/op 32 B/op 2 allocs/op BenchmarkWriterCopyNoReadFrom-8 500000 3219 ns/op 32800 B/op 3 allocs/op BenchmarkReaderEmpty-8 2000000 748 ns/op 4224 B/op 3 allocs/op BenchmarkWriterEmpty-8 2000000 662 ns/op 4096 B/op 1 allocs/op BenchmarkWriterFlush-8 100000000 16.9 ns/op 0 B/op 0 allocs/op PASS ok bufio 20.366s 2.6. 注意编译器的优化 # 这个例子来自 issue 14813.\nconst m1 = 0x5555555555555555 const m2 = 0x3333333333333333 const m4 = 0x0f0f0f0f0f0f0f0f const h01 = 0x0101010101010101 func popcnt(x uint64) uint64 { x -= (x \u0026gt;\u0026gt; 1) \u0026amp; m1 x = (x \u0026amp; m2) + ((x \u0026gt;\u0026gt; 2) \u0026amp; m2) x = (x + (x \u0026gt;\u0026gt; 4)) \u0026amp; m4 return (x * h01) \u0026gt;\u0026gt; 56 } func BenchmarkPopcnt(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { popcnt(uint64(i)) } } 您认为该功能将以多快的速度进行基准测试？ 让我们找出答案。\n% go test -bench=. ./examples/popcnt/ goos: darwin goarch: amd64 BenchmarkPopcnt-8 2000000000 0.30 ns/op PASS\u0026lt;/pre\u0026gt; 0.3 纳秒；这基本上是一个时钟周期。即使假设每个时钟周期中 CPU 可能正在运行一些指令，该数字似乎也过低。发生了什么？\nTo understand what happened, we have to look at the function under benchmake, popcnt. popcnt is a leaf function — it does not call any other functions — so the compiler can inline it.\n要了解发生了什么，我们必须查看 benchmake 下的函数 popcnt。popcnt 是 叶函数(它不调用任何其他函数) 因此编译器可以内联它。\n因为该函数是内联的，所以编译器现在可以看到它没有副作用。 popcnt 不会影响任何全局变量的状态。 因此，消除了该调用。这是编译器看到的：\nfunc BenchmarkPopcnt(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { // optimised away } } 在我测试过的所有 Go 编译器版本中，仍然会生成循环。 但是英特尔 CPU 确实擅长优化循环，尤其是空循环。\n2.6.1. 练习，实践 # 在继续之前，让我们实践一下以确认我们所看到的\n% go test -gcflags=-S 使用 `gcflags=\u0026#34;-l -S\u0026#34;` 禁用内联，这可以影响程序输出 要消除的事情是与通过消除不必要的计算来使实际代码相同的优化，即消除了没有可观察到的副作用的基准测试。 随着 Go 编译器的改进，这只会变得越来越普遍。\n2.6.2. 修复基准 # 禁用内联以使基准测试有效是不现实的；我们希望在优化的基础上构建代码。\n为了修正这个基准，我们必须确保编译器无法 证明 BenchmarkPopcnt 的主体不会引起全局状态的改变。\nvar Result uint64 func BenchmarkPopcnt(b *testing.B) { var r uint64 for i := 0; i \u0026lt; b.N; i++ { r = popcnt(uint64(i)) } Result = r } 这是确保编译器无法优化循环主体的推荐方法。\n首先，我们通过将其存储在 r 中来使用调用 popcnt 的结果。其次，由于一旦基准测试结束，r 就在 BenchmarkPopcnt 的范围内局部声明，所以 r 的结果对于程序的另一部分永远是不可见的，因此作为最后的动作，我们将 r 的值赋值储存到包公共变量 Result。\nBecause Result is public the compiler cannot prove that another package importing this one will not be able to see the value of Result changing over time, hence it cannot optimise away any of the operations leading to its assignment.\nWhat happens if we assign to Result directly? Does this affect the benchmark time? What about if we assign the result of popcnt to _?\n由于 Result 是公开的，因此编译器无法证明导入该软件包的另一个包将无法看到 Result 的值随时间变化，因此它无法优化导致其赋值的任何操作。\n如果直接分配给 Result 会怎样？这会影响基准时间吗？如果将 popcnt 的结果赋给 _ 会怎么样？\n在我们之前的 Fib 基准测试中，我们没有采取这些预防措施，应该这样做吗？\n2.7. 基准误差 # for 循环对于基准测试的运行至关重要。\n这是两个不正确的基准，您能解释一下它们有什么问题吗？\nfunc BenchmarkFibWrong(b *testing.B) { Fib(b.N) } func BenchmarkFibWrong2(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { Fib(n) } } 运行这些基准测试，您会看到什么？\n2.8. 分析基准 # testing 包内置了对生成 CPU，内存和块配置文件的支持。\n-cpuprofile=\u0026lt;span class=\u0026quot;katex\u0026quot;\u0026gt;$1$\u0026lt;/span\u0026gt;FILE. -memprofile=\u0026lt;span class=\u0026quot;katex\u0026quot;\u0026gt;$1$\u0026lt;/span\u0026gt;FILE, -memprofilerate=N 将配置文件速率调整为 1/N. -blockprofile=\u0026lt;span class=\u0026quot;katex\u0026quot;\u0026gt;$1$\u0026lt;/span\u0026gt;FILE. 使用这些标志中的任何一个也会保留二进制文件。\n% go test -run=XXX -bench=. -cpuprofile=c.p bytes % go tool pprof c.p 2.9. 讨论 # 有没有问题？\n也许是时候休息一下了。\n3. 性能评估和性能分析 # 在上一节中，我们研究了对单个函数进行基准测试，这对您提前知道瓶颈在哪里很有用。但是，通常您会发现自己有一个问题\n为什么该程序需要这么长时间才能运行？\n对 整个 程序进行概要分析，对于回答诸如此类的高级问题很有用。在本部分中，我们将使用 Go 内置的性能分析工具从内部调查程序的运行情况。\n3.1. pprof # 今天我们要讨论的第一个工具是 pprof. pprof 来自 Google Perf Tools 这套工具套件，自最早的公开发布以来已集成到 Go 运行时中。\npprof 由两部分组成:\nruntime/pprof 每个 Go 程序内置的软件包 go tool pprof 用于调查性能分析。 3.2. 性能分析文件类型 # pprof 支持多种类型的性能分析，今天我们将讨论其中的三种:\nCPU profiling. Memory profiling. Block (or blocking) profiling. Mutex contention profiling. 3.2.1. CPU 分析 # CPU 分析文件是最常见的配置文件类型，也是最明显的配置文件。\n启用 CPU 性能分析后，运行时将每 10 毫秒中断一次，并记录当前正在运行的 goroutine 的堆栈跟踪。\n分析文件完成后，我们可以对其进行分析以确定最热门的代码路径。\n函数在分析文件中出现的次数越多，代码路径花费的时间就越多。\n3.2.2. 内存分析 # 进行 堆 分配时，内存分析记录堆栈跟踪。\n堆栈分配假定为空闲，并且在内存性能分析文件中 未跟踪。\n像 CPU 分析一样，内存分析都是基于样本的，默认情况下，每 1000 个分配中的内存分析样本为 1。 此速率可以更改。\n由于内存分析是基于样本的，并且由于它跟踪未 使用 的 分配，因此很难使用内存分析来确定应用程序的整体内存使用情况。\n个人想法: 我发现内存分析对发现内存泄漏没有帮助。有更好的方法来确定您的应用程序正在使用多少内存。 我们将在演示文稿的后面讨论这些。\n3.2.3. 块性能分析 # 块分析是 Go 特有的。\n块概要文件类似于 CPU 概要文件，但是它记录 goroutine 等待共享资源所花费的时间。\n这对于确定应用程序中的 并发 瓶颈很有用。\n块性能分析可以向您显示何时有大量 goroutine 可以 取得进展，但被 阻塞 了。包括阻止\n在无缓冲的通道上发送或接收。 正在发送到完整频道，从空频道接收。 试图 锁定 被另一个 goroutine 锁定的 sync.Mutex。 块分析是一种非常专业的工具，在您确信消除了所有 CPU 和内存使用瓶颈之后，才应该使用它。\n3.2.4. Mutex profiling # 互斥锁概要分析与块概要分析类似，但专门针对导致互斥锁争用导致延迟的操作。\n我对这种类型的个人资料没有很多经验，但是我建立了一个示例来演示它。我们将很快看一下该示例。\n3.3. 同一时间只使用一种性能分析 # 性能分析不是免费的。\n分析对程序性能有中等但可测量的影响，尤其是如果您增加内存配置文件采样率。\n大多数工具不会阻止您一次启用多个性能分析。\n一次不要启用多种性能分析。\n如果您同时启用多个个人资料，他们将观察自己的互动并放弃您的结果。\n3.4. 收集性能分析 # Go 运行时的配置文件界面位于 runtime/pprof 包中。runtime/pprof 是一个非常基础的工具，由于历史原因，与各种配置文件的接口并不统一。\n正如我们在上一节中看到的那样，pprof 概要分析内置于 testing 包中，但是有时将您要分析的代码放在 testing.B 基准测试环境中是不便或困难的，并且必须使用直接使用 runtime/pprof API。\n几年前，我写了一个 small package，以便更轻松地描述现有应用程序。\nimport \u0026#34;github.com/pkg/profile\u0026#34; func main() { defer profile.Start().Stop() // ... } 在本节中，我们将使用 profile 包。 稍后，我们将直接使用 runtime/pprof 接口。\n3.5. Analysing a profile with pprof # 既然我们已经讨论了 pprof 可以测量的内容以及如何生成配置文件，那么我们就来讨论如何使用 pprof 分析配置文件。\n分析由 go pprof 子命令驱动\n% go tool pprof /path/to/your/profile 该工具提供了概要数据的几种不同表示形式。文字，图形甚至火焰图。\n如果您使用 Go 已有一段时间，则可能会被告知 pprof 有两个参数。从 Go 1.9 开始，配置文件包含渲染配置文件所需的所有信息。您不再需要生成性能分析的二进制文件。🎉\n3.5.1. 进一步阅读 # Profiling Go programs (Go Blog) Debugging performance issues in Go programs 3.5.2. CPU profiling (exercise) # 让我们编写一个计算单词数的程序:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;unicode\u0026#34; ) func readbyte(r io.Reader) (rune, error) { var buf [1]byte _, err := r.Read(buf[:]) return rune(buf[0]), err } func main() { f, err := os.Open(os.Args[1]) if err != nil { log.Fatalf(\u0026#34;could not open file %q: %v\u0026#34;, os.Args[1], err) } words := 0 inword := false for { r, err := readbyte(f) if err == io.EOF { break } if err != nil { log.Fatalf(\u0026#34;could not read file %q: %v\u0026#34;, os.Args[1], err) } if unicode.IsSpace(r) \u0026amp;\u0026amp; inword { words++ inword = false } inword = unicode.IsLetter(r) } fmt.Printf(\u0026#34;%q: %d words\\n\u0026#34;, os.Args[1], words) } 让我们看看赫尔曼·梅尔维尔经典小说中有多少个单词 Moby Dick (来自古腾堡计划)\n% go build \u0026amp;\u0026amp; time ./words moby.txt \u0026#34;moby.txt\u0026#34;: 181275 words real 0m2.110s user 0m1.264s sys 0m0.944s 让我们将其与 unix 的 wc -w 进行比较\n% time wc -w moby.txt 215829 moby.txt real 0m0.012s user 0m0.009s sys 0m0.002s 所以数字不一样。 wc 大约高出 19％，因为它认为单词与我的简单程序不同。这并不重要-两个程序都将整个文件作为输入，并在一次通过中计算从单词到非单词的过渡次数。\n让我们研究一下为什么使用 pprof 这些程序的运行时间不同。\n3.5.3. 添加 CPU 分析 # 首先，编辑 main.go 并启用分析\nimport ( \u0026#34;github.com/pkg/profile\u0026#34; ) func main() { defer profile.Start().Stop() // ... 现在，当我们运行程序时，将创建一个 cpu.pprof 文件。\n% go run main.go moby.txt 2018/08/25 14:09:01 profile: cpu profiling enabled, /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile239941020/cpu.pprof \u0026#34;moby.txt\u0026#34;: 181275 words 2018/08/25 14:09:03 profile: cpu profiling disabled, /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile239941020/cpu.pprof 现在我们有了配置文件，可以使用 go pprof 工具对其进行分析。\n% go tool pprof /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile239941020/cpu.pprof Type: cpu Time: Aug 25, 2018 at 2:09pm (AEST) Duration: 2.05s, Total samples = 1.36s (66.29%) Entering interactive mode (type \u0026#34;help\u0026#34; for commands, \u0026#34;o\u0026#34; for options) (pprof) top Showing nodes accounting for 1.42s, 100% of 1.42s total flat flat% sum% cum cum% 1.41s 99.30% 99.30% 1.41s 99.30% syscall.Syscall 0.01s 0.7% 100% 1.42s 100% main.readbyte 0 0% 100% 1.41s 99.30% internal/poll.(*FD).Read 0 0% 100% 1.42s 100% main.main 0 0% 100% 1.41s 99.30% os.(*File).Read 0 0% 100% 1.41s 99.30% os.(*File).read 0 0% 100% 1.42s 100% runtime.main 0 0% 100% 1.41s 99.30% syscall.Read 0 0% 100% 1.41s 99.30% syscall.read top 命令是您最常使用的命令。 我们可以看到该程序有 99％ 的时间花费在 syscall.Syscall 中，而一小部分花费在main.readbyte 中。\n我们还可以使用 web 命令来可视化此调用。这将从配置文件数据生成有向图。 在后台，这使用了 Graphviz 的 dot 命令。\n但是，在 Go 1.10(也可能是 1.11)中，Go 附带了本身支持 HTTP 服务器的 pprof 版本\n% go tool pprof -http=:8080 /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile239941020/cpu.pprof 将会打开网络浏览器;\n图形模式 火焰图模式 在图形上，占用 最多 CPU 时间的框是最大的框，我们看到 syscall.Syscall 占程序总时间的 99.3％。导致 syscall.Syscall 的字符串表示立即调用者，如果多个代码路径在同一函数上收敛，则可以有多个。箭头的大小代表在一个盒子的子元素上花费了多少时间，我们可以看到，从 main.readbyte 开始，它们占了该图分支中 1.41 秒所用时间的接近 0。\n问题: 谁能猜出为什么我们的版本比 wc 慢得多?\n3.5.4. 改进 # 我们的程序运行缓慢的原因不是因为 Go 的 syscall.Syscall 运行缓慢。 这是因为系统调用通常是昂贵的操作（并且随着发现更多 Spectre 系列漏洞而变得越来越昂贵）。\n每次对 readbyte 的调用都会导致一个 syscall.Read 的缓冲区大小为 1。因此，我们的程序执行的 syscall 数量等于输入的大小。我们可以看到，在 pprof 图中，读取输入的内容占主导地位。\nfunc main() { defer profile.Start(profile.MemProfile, profile.MemProfileRate(1)).Stop() // defer profile.Start(profile.MemProfile).Stop() f, err := os.Open(os.Args[1]) if err != nil { log.Fatalf(\u0026#34;could not open file %q: %v\u0026#34;, os.Args[1], err) } b := bufio.NewReader(f) words := 0 inword := false for { r, err := readbyte(b) if err == io.EOF { break } if err != nil { log.Fatalf(\u0026#34;could not read file %q: %v\u0026#34;, os.Args[1], err) } if unicode.IsSpace(r) \u0026amp;\u0026amp; inword { words++ inword = false } inword = unicode.IsLetter(r) } fmt.Printf(\u0026#34;%q: %d words\\n\u0026#34;, os.Args[1], words) } 通过在 readbyte 之前使用 bufio.Reader 包装输入文件\n将此修订程序的时间与 `wc` 比较。有多少差距？进行性能分析，看看还剩下什么。 3.5.5. 内存分析 # 新的单词配置文件表明在 readbyte 函数内部分配了一些东西。我们可以使用 pprof 进行调查。\ndefer profile.Start(profile.MemProfile).Stop() 然后照常运行程序\n% go run main2.go moby.txt 2018/08/25 14:41:15 profile: memory profiling enabled (rate 4096), /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile312088211/mem.pprof \u0026#34;moby.txt\u0026#34;: 181275 words 2018/08/25 14:41:15 profile: memory profiling disabled, /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile312088211/mem.pprof 由于我们怀疑分配来自 readbyte，这不是那么复杂，readbyte 只有三行:\n使用 `pprof` 确定分配来自何处。 func readbyte(r io.Reader) (rune, error) { var buf [1]byte // (1) _, err := r.Read(buf[:]) return rune(buf[0]), err } | 1 | 分配在这里 |\n在下一节中，我们将详细讨论为什么会发生这种情况，但是目前，我们看到对 readbyte 的每个调用都在分配一个新的一字节长的 array ，并且该数组正在堆上分配。\n有什么方法可以避免这种情况？试试看，并使用 CPU 和内存性能分析进行验证。\nAlloc 对象与 inuse 对象 # 内存性能分析有两种，分别以 go tool pprof 标志区分。\n-alloc_objects 报告进行分配的呼叫站点。 -inuse_objects 报告在性能分析末尾可以访问的呼叫站点。 为了演示这一点，这是一个人为设计的程序，它将以受控方式分配一堆内存。\nconst count = 100000 var y []byte func main() { defer profile.Start(profile.MemProfile, profile.MemProfileRate(1)).Stop() y = allocate() runtime.GC() } // allocate allocates count byte slices and returns the first slice allocated. func allocate() []byte { var x [][]byte for i := 0; i \u0026lt; count; i++ { x = append(x, makeByteSlice()) } return x[0] } // makeByteSlice returns a byte slice of a random length in the range [0, 16384). func makeByteSlice() []byte { return make([]byte, rand.Intn(2^14)) } 该程序是带有 profile 包的注释，我们将内存配置文件速率设置为 1，也就是说，记录每次分配的堆栈跟踪。这会大大减慢该程序的速度，但是您很快就会知道原因。\n% go run main.go 2018/08/25 15:22:05 profile: memory profiling enabled (rate 1), /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile730812803/mem.pprof 2018/08/25 15:22:05 profile: memory profiling disabled, /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile730812803/mem.pprof 让我们看一下已分配对象的图，这是默认设置，并显示导致在概要文件期间分配每个对象的调用图。\n% go tool pprof -http=:8080 /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile891268605/mem.pprof 不足为奇的是，超过 99％ 的分配位于 makeByteSlice 内部。 现在让我们使用 -inuse_objects 查看相同的配置文件\n% go tool pprof -http=:8080 /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile891268605/mem.pprof 我们看到的不是在概要文件期间 分配 的对象，而是在获取概要文件时 仍在使用 的对象，这将忽略已由垃圾收集器回收的对象的堆栈跟踪。\n3.5.6. Block profiling # 我们要查看的最后一个配置文件类型是区块分析。 我们将使用来自 net/http 包的 ClientServer 基准测试\n% go test -run=XXX -bench=ClientServer$ -blockprofile=/tmp/block.p net/http % go tool pprof -http=:8080 /tmp/block.p 3.5.7. 线程创建分析 # Go 1.11 (?) 增加了对操作系统线程创建的分析支持。\n将线程创建分析添加到 `godoc` 中，并观察分析 `godoc -http =：8080 -index` 的结果。 3.5.8. Framepointers # Go 1.7 已发布，并且与用于 amd64 的新编译器一起，现在默认情况下编译器启用了帧指针。\n帧指针是一个始终指向当前堆栈帧顶部的寄存器。\n帧指针使诸如 gdb(1) 和 perf(1) 之类的工具能够理解 Go 调用堆栈。\n在本研讨会中，我们不会介绍这些工具，但是您可以阅读和观看我的演讲，该演讲以七种不同的方式介绍 Go 程序。\n分析 Go 程序的七种方法 (slides) 分析 Go 程序的七种方法 (video, 30 mins) 分析 Go 程序的七种方法 (webcast, 60 mins) 3.5.9. 练习 # 根据您熟悉的一段代码生成一个性能分析。如果您没有代码示例，请尝试对 godoc 进行分析。 % go get golang.org/x/tools/cmd/godoc % cd $GOPATH/src/golang.org/x/tools/cmd/godoc % vim main.go 如果要在一台计算机上生成性能分析，然后在另一台计算机上检查性能分析，您将如何处理？ 4. 编译优化 # 本节介绍了 Go 编译器执行的一些优化。\n例如;\n逃逸分析 内联 消除死代码 全部在编译器的前端处理，而代码仍为 AST 形式； 然后将代码传递给 SSA 编译器进行进一步优化。\n4.1. History of the Go compiler # Go 编译器大约在 2007 时作为 Plan9 编译器工具链的分支而开始的。当时的编译器与 Aho 和 Ullman 的 Dragon Book 非常相似。\n在 2015 年，当时的 Go 1.5 编译器为 C 转换为 Go。\n一年后，Go 1.7 引入了一种基于 SSA 技术的 new compiler backend 以前的 Plan9 样式代码生成。这个新的后端为通用和特定于架构的优化引入了许多机会。\n4.2. 逃逸优化 # 我们正在讨论的第一个优化是 逃逸优化。\n为了说明逃逸分析的作用，我们回想起 Go spec 并未提及堆或堆栈。它仅提及该语言是在引言中被垃圾收集的，并没有提示如何实现该语言。\nGo 规范的兼容 Go 实现 可以 将每个分配存储在堆上。这将对垃圾收集器施加很大压力，但是这绝不是不正确的。多年来，gccgo 对逃逸分析的支持非常有限，因此可以有效地认为它在这种模式下运行。\n但是，goroutine 的堆栈作为存储局部变量的廉价场所而存在。无需在堆栈上进行垃圾收集。因此，在安全的情况下，放置在堆栈上的分配将更有效。\n在某些语言中，例如 C 和 C++，选择在堆栈上还是在堆上分配是程序员的手动操作。堆分配是通过 malloc 和 free 进行的，栈分配是通过 alloca 进行的。使用这些机制的错误是导致内存损坏错误的常见原因。\n在 Go 中，如果值的寿命超出了函数调用的寿命，则编译器会自动将其移动到堆中。 说明该值 逃逸 到堆。\ntype Foo struct { a, b, c, d int } func NewFoo() *Foo { return \u0026amp;Foo{a: 3, b: 1, c: 4, d: 7} } 在这个例子中，在 NewFoo 中分配的 Foo 将被移到堆中，因此在 NewFoo 返回后其内容仍然有效。\n自 Go 成立以来，这种情况就一直存在。与其说是自动纠正功能，还不如说是一种优化。在 Go 中意外返回堆栈分配变量的地址是不可能的。\n但是编译器也可以做相反的事情。它可以找到假定在堆上分配的东西，并将它们移到堆栈中。\n让我们看一个例子\nfunc Sum() int { const count = 100 numbers := make([]int, count) for i := range numbers { numbers[i] = i + 1 } var sum int for _, i := range numbers { sum += i } return sum } func main() { answer := Sum() fmt.Println(answer) } sum 将 1 与 100 之间的 int 相加并返回结果。\n由于 numbers 切片仅在 Sum 内部引用，因此编译器将安排将该切片的 100 个整数存储在堆栈中，而不是堆中。无需垃圾回收 numbers，它会在 Sum 返回时自动释放。\n4.2.1. 证明它! # 要打印编译器的逃逸分析结果，请使用 -m 标志。\n% go build -gcflags=-m examples/esc/sum.go # command-line-arguments examples/esc/sum.go:22:13: inlining call to fmt.Println examples/esc/sum.go:8:17: Sum make([]int, count) does not escape examples/esc/sum.go:22:13: answer escapes to heap examples/esc/sum.go:22:13: io.Writer(os.Stdout) escapes to heap examples/esc/sum.go:22:13: main []interface {} literal does not escape \u0026lt;autogenerated\u0026gt;:1: os.(*File).close .this does not escape 第 8 行显示编译器已正确推断出 make([]int, 100) 的结果不会逸出到堆中。没有的原因\n第 22 行报告 answer 转储到堆中是 fmt.Println 是一个可变函数。可变参数函数的参数装在切片中，在本例中为 []interface {}，因此将 answer 放入接口值中，因为它是由对 fmt.Println 的调用引用的。由于 Go 1.6 的垃圾回收器要求通过接口传递的所有值都是指针，因此编译器优化后的代码的大致是：\nvar answer = Sum() fmt.Println([]interface{\u0026amp;answer}...) 我们可以使用 -gcflags=\u0026quot;-m -m\u0026quot; 标志来确认。在哪返回了\n% go build -gcflags=\u0026#39;-m -m\u0026#39; examples/esc/sum.go 2\u0026gt;\u0026amp;1 | grep sum.go:22 examples/esc/sum.go:22:13: inlining call to fmt.Println func(...interface {}) (int, error) { return fmt.Fprintln(io.Writer(os.Stdout), fmt.a...) } examples/esc/sum.go:22:13: answer escapes to heap examples/esc/sum.go:22:13: from ~arg0 (assign-pair) at examples/esc/sum.go:22:13 examples/esc/sum.go:22:13: io.Writer(os.Stdout) escapes to heap examples/esc/sum.go:22:13: from io.Writer(os.Stdout) (passed to call[argument escapes]) at examples/esc/sum.go:22:13 examples/esc/sum.go:22:13: main []interface {} literal does not escape 简而言之，不必担心第 22 行的逃逸，这对本次讨论并不重要。\n4.2.2. 练习 # 这种优化对所有 count 的值都适用吗？ 如果 count 是变量而不是常量，此优化是否成立？ 如果 count 是 Sum 的参数，此优化是否成立？ 4.2.3. Escape analysis (continued) # 这个例子是人为造的。它不旨在成为真实的代码，仅是示例。\ntype Point struct{ X, Y int } const Width = 640 const Height = 480 func Center(p *Point) { p.X = Width / 2 p.Y = Height / 2 } func NewPoint() { p := new(Point) Center(p) fmt.Println(p.X, p.Y) } NewPoint creates a new *Point value, p. We pass p to the Center function which moves the point to a position in the center of the screen. Finally we print the values of p.X and p.Y.\nNewPoint 创建一个新的 *Point 值 p。 我们将 p 传递给 Center 函数，该函数将点移动到屏幕中心的位置。 最后，我们输出 p.X 和 p.Y 的值。\n% go build -gcflags=-m examples/esc/center.go # command-line-arguments examples/esc/center.go:11:6: can inline Center examples/esc/center.go:18:8: inlining call to Center examples/esc/center.go:19:13: inlining call to fmt.Println examples/esc/center.go:11:13: Center p does not escape examples/esc/center.go:19:15: p.X escapes to heap examples/esc/center.go:19:20: p.Y escapes to heap examples/esc/center.go:19:13: io.Writer(os.Stdout) escapes to heap examples/esc/center.go:17:10: NewPoint new(Point) does not escape examples/esc/center.go:19:13: NewPoint []interface {} literal does not escape \u0026lt;autogenerated\u0026gt;:1: os.(*File).close .this does not escape 即使使用新函数分配了 p，也不会将其存储在堆中，因为没有引用 p 会逸出 Center 函数。\n问题: 那第 19 行，如果 p 不逃逸，那是什么逃逸到了堆呢？\n编写一个基准，以规定 Sum 不分配。\n4.3. 内联 # 在 Go 函数中，调用具有固定的开销；堆栈和抢占检查。\n硬件分支预测器可以改善其中的一些功能，但是就功能大小和时钟周期而言，这仍然是一个代价。\n内联是避免这些成本的经典优化方法。\n直到 Go 1.11 内联仅在 叶函数 上起作用，该函数不会调用另一个函数。这样做的理由是:\n如果您的函数做了很多工作，那么前导开销将可以忽略不计。这就是为什么功能要达到一定的大小（目前有一些指令，加上一些阻止全部内联的操作，例如，在 Go 1.7 之前进行切换） 另一方面，小的功能为执行的相对少量的有用工作支付固定的开销。这些是内联目标的功能，因为它们最大程度地受益。 另一个原因是过多的内联使堆栈跟踪更难遵循。\n4.3.1. Inlining (example) # func Max(a, b int) int { if a \u0026gt; b { return a } return b } func F() { const a, b = 100, 20 if Max(a, b) == b { panic(b) } } 同样，我们使用 -gcflags=-m 标志来查看编译器的优化决策。\n% go build -gcflags=-m examples/inl/max.go # command-line-arguments examples/inl/max.go:4:6: can inline Max examples/inl/max.go:11:6: can inline F examples/inl/max.go:13:8: inlining call to Max examples/inl/max.go:20:6: can inline main examples/inl/max.go:21:3: inlining call to F examples/inl/max.go:21:3: inlining call to Max 编译器打印了两行。\n第 3 行中的第一个是 Max 的声明，告诉我们可以内联。 第二个报告说，Max 的主体已在第 12 行内联到调用方中。 在不使用 //go:noinline comment 的情况下，重写 Max 使得它仍然返回正确的答案，但是编译器不再认为它是可内联的。\n4.3.2. 内联是什么样的？ # 编译 max.go，看看 F() 的优化版本是什么。\n% go build -gcflags=-S examples/inl/max.go 2\u0026gt;\u0026amp;1 | grep -A5 \u0026#39;\u0026#34;\u0026#34;.F STEXT\u0026#39; \u0026#34;\u0026#34;.F STEXT nosplit size=2 args=0x0 locals=0x0 0x0000 00000 (/Users/dfc/devel/high-performance-go-workshop/examples/inl/max.go:11) TEXT \u0026#34;\u0026#34;.F(SB), NOSPLIT|ABIInternal, $0-0 0x0000 00000 (/Users/dfc/devel/high-performance-go-workshop/examples/inl/max.go:11) FUNCDATA $0, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB) 0x0000 00000 (/Users/dfc/devel/high-performance-go-workshop/examples/inl/max.go:11) FUNCDATA $1, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB) 0x0000 00000 (/Users/dfc/devel/high-performance-go-workshop/examples/inl/max.go:11) FUNCDATA $3, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB) 0x0000 00000 (/Users/dfc/devel/high-performance-go-workshop/examples/inl/max.go:13) PCDATA \u0026lt;span class=\u0026#34;katex\u0026#34;\u0026gt;$1$\u0026lt;/span\u0026gt;0 一旦将 Max 内联到其中，它就是 F 的主体, 此功能没有任何反应。 我知道屏幕上有很多文本，但是什么也没说，但请您相信，唯一发生的是 RET。 实际上，F 变为：\nfunc F() { return } -S 的输出不是二进制文件中的最终机器代码。链接器在最终链接阶段进行一些处理。像 FUNCDATA 和 PCDATA 这样的行是垃圾收集器的元数据，它们在链接时会移到其他位置。如果您正在读取 -S 的输出，则只需忽略 FUNCDATA 和 PCDATA 行；它们不是最终二进制文件的一部分。\n4.3.3. 讨论 # 为什么在 F() 中声明 a 和 b 为常数？\n实验输出以下内容：如果将 a 和 b 声明为变量，会发生什么？ 如果 a 和 b 作为参数传递给 F() 会怎样？\n-gcflags=-S 不会阻止在您的工作目录中构建最终的二进制文件。如果发现随后的 go build … 运行没有输出，请删除工作目录中的 ./max 二进制文件。\n4.3.4. 调整内联级别 # 调整 内联级别 是通过 -gcflags = -l 标志执行的。有些令人困惑的传递单个 -l 将禁用内联，而两个或多个将启用更激进的设置的内联。\n-gcflags=-l, 禁用内联. 没有，正常内联。 -gcflags='-l -l' 内联级别 2，更具攻击性，可能更快，可能会生成更大的二进制文件。 -gcflags='-l -l -l' 内联 3 级，再次更具攻击性，二进制文件肯定更大，也许再次更快，但也可能有问题。 -gcflags=-l=4 Go 1.11 中的（四个 -l）将启用实验性 mid stack inlining optimisation。我相信从 Go 1.12 开始它没有任何作用。 4.3.5. 中栈内联 # 由于 Go 1.12 已启用所谓的 中栈 内联（以前在 Go 1.11 中的预览中带有 -gcflags ='-l -l -l -l' 标志）。\n我们可以在前面的示例中看到中栈内联的示例。在 Go 1.11 和更早的版本中，F 不会是叶子函数，它称为 max。但是由于内联的改进，现在将 F 内联到其调用方中。这有两个原因。当将 max 内联到 F 中时，F 不包含其他函数调用，因此，如果未超过其复杂性预算，它将成为潜在的 叶函数。由于 F 是一个简单的函数，内联和消除死代码消除了许多复杂性预算-它有资格进行 中栈 内联，而与调用max无关。\n中栈内联可用于内联函数的快速路径，从而消除了快速路径中的函数调用开销。 最近进入 Go 1.13 的 CL 显示了此技术应用于 sync.RWMutex.Unlock()。\n4.4. 消除无效代码 # 为什么 a 和 b 是常量很重要？\n要了解发生了什么，让我们看一下编译器将其 Max 内联到 F 中后看到的内容。我们很难从编译器中获得此信息，但是直接手动完成是很简单的。\nBefore:\nfunc Max(a, b int) int { if a \u0026gt; b { return a } return b } func F() { const a, b = 100, 20 if Max(a, b) == b { panic(b) } } After:\nfunc F() { const a, b = 100, 20 var result int if a \u0026gt; b { result = a } else { result = b } if result == b { panic(b) } } 因为 a 和 b 是常量，所以编译器可以在编译时证明该分支永远不会为假。100 始终大于 20。 因此，编译器可以进一步优化 F 以\nfunc F() { const a, b = 100, 20 var result int if true { result = a } else { result = b } if result == b { panic(b) } } 现在已经知道了分支的结果，那么 result 的内容也就知道了。这就是 消除分支.\nfunc F() { const a, b = 100, 20 const result = a if result == b { panic(b) } } 现在消除了分支，我们知道 result 总是等于 a，并且因为 a 是常数，所以我们知道 result 是常数。编译器将此证明应用于第二个分支\nfunc F() { const a, b = 100, 20 const result = a if false { panic(b) } } 再次使用分支消除，最终形式为 F。\nfunc F() { const a, b = 100, 20 const result = a } 最后就是\nfunc F() { } 4.4.1. 消除无效代码（续） # 分支消除是称为 无效代码消除 的优化类别之一。实际上，使用静态证明来显示，段代码是永远无法访问的，俗称 无效，因此不需要在最终二进制文件中对其进行编译，优化或发出。\n我们看到了无效代码消除如何与内联一起工作，以减少通过删除证明无法访问的循环和分支而减少的代码量。\n您可以利用此优势实施昂贵的调试，并将其隐藏\nconst debug = false 与构建标签结合使用，这可能非常有用。\n4.4.2. 进一步阅读 # Using // +build to switch between debug and release builds How to use conditional compilation with the go build tool 4.5. 编译器标识练习 # % go build -gcflags=$FLAGS 研究以下编译器功能的操作：\n-S 打印正在编译的程序包的 (Go 风格) 程序集。 -l 控制内联的行为； -l 禁用内联，-l -l 增加内联（更多 -l 增加编译器对内联代码的需求）。尝试编译时间，程序大小和运行时间的差异。 -m 控制诸如内联，转义分析之类的优化决策的打印。-m -m 打印有关编译器思想的更多详细信息。 -l -N 禁用所有优化。 如果发现随后的 go build …​ 运行没有输出，请删除工作目录中的 ./max 二进制文件。\n4.5.1 进一步阅读 # Codegen Inspection by Jaana Burcu Dogan 4.6. 边界检查消除 # Go 是一种边界检查语言。这意味着将检查数组和切片下标操作，以确保它们在相应类型的范围内。\n对于数组，这可以在编译时完成。对于切片，这必须在运行时完成。\nvar v = make([]int, 9) var A, B, C, D, E, F, G, H, I int func BenchmarkBoundsCheckInOrder(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { A = v[0] B = v[1] C = v[2] D = v[3] E = v[4] F = v[5] G = v[6] H = v[7] I = v[8] } } 使用 -gcflags=-S 来拆解 BenchmarkBoundsCheckInOrder。每个循环执行多少个边界检查操作？\nfunc BenchmarkBoundsCheckOutOfOrder(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { I = v[8] A = v[0] B = v[1] C = v[2] D = v[3] E = v[4] F = v[5] G = v[6] H = v[7] } } 重新安排我们通过 I 分配 A 的顺序是否会影响装配。分解 BenchmarkBoundsCheckOutOfOrder 并找出。\n4.6.1. 练习 # 重新排列下标操作的顺序是否会影响函数的大小？它会影响功能的速度吗？ 如果将 v 移入 基准 函数内部会怎样？ 如果 v 被声明为数组，var v [9] int 会发生什么？ 5. 执行追踪器 # 执行跟踪程序是由 Dmitry Vyukov 为 Go 1.5 开发的，并且仍处于记录和使用状态，已有好几年了。\n与基于样本的分析不同，执行跟踪器集成到 Go 运行时中，因此它只知道 Go 程 序在特定时间点正在做什么，但是 为什么。\n5.1. 什么是执行跟踪器，为什么需要它？ # 我认为，通过查看一段代码 go tool pprof 的效果不好，可以最容易地解释执行跟踪器的功能，以及为什么这样做很重要。\nexamples/mandelbrot 目录包含一个简单的 mandelbrot 生成器。该代码来自 Francesc Campoy’s mandelbrot package。\n% cd examples/mandelbrot % go build \u0026amp;\u0026amp; ./mandelbrot 如果我们构建它，然后运行它，它将生成如下内容\n5.1.1. 多久时间？ # 那么，此程序需要多长时间才能生成 1024 * 1024 像素的图像？\n我知道如何执行此操作的最简单方法是使用 time(1) 之类的东西。\n% time ./mandelbrot real 0m1.654s user 0m1.630s sys 0m0.015s 不要使用 time go run mandebrot.go，否则您将花费 编译 程序以及运行该程序所需的时间。\n5.1.2. 该程序在做什么？ # 在此示例中，程序花费了 1.6 秒来生成 mandelbrot 并写入 png。\n这样好吗？我们可以加快速度吗？\n回答该问题的一种方法是使用 Go 内置的 pprof 来分析程序。\n让我们尝试一下。\n5.2. 生成性能分析 # 要生成性能分析，我们需要\n直接使用 runtime/pprof 软件包。 使用类似 github.com/pkg/profile 的包装器自动执行此操作。 5.3. 使用 runtime/pprof 生成性能分析 # 为了告诉您我没有使用黑科技(魔法)，让我们修改程序以将 CPU 性能分析写入 os.Stdout。\nimport \u0026#34;runtime/pprof\u0026#34; func main() { pprof.StartCPUProfile(os.Stdout) defer pprof.StopCPUProfile() By adding this code to the top of the main function, this program will write a profile to os.Stdout.\n通过将此代码添加到 main 函数的顶部，该程序会将配置文件写入 os.Stdout。\n% cd examples/mandelbrot-runtime-pprof % go run mandelbrot.go \u0026gt; cpu.pprof 在这种情况下，我们可以使用 go run，因为 cpu 配置文件将仅包含 mandelbrot.go 的执行，而不包括其编译。\n5.3.1. 使用 github.com/pkg/profile 生成性能分析 # 上一张幻灯片显示了一种生成配置文件的超级简便的方法，但是存在一些问题。\n如果您忘记了将输出重定向到文件，则会破坏该终端会话。😞（提示：reset(1) 是您的朋友） 如果您向 os.Stdout 中写入其他内容 (例如，fmt.Println)，则会破坏跟踪。 推荐使用 runtime/pprof 的方法是 将跟踪信息写入文件。 但是，您必须确保跟踪已停止，并且在程序停止之前 (包括有人 ^C) 关闭了文件。\n因此，几年前，我写了一个 package 来处理它。\nimport \u0026#34;github.com/pkg/profile\u0026#34; func main() { defer profile.Start(profile.CPUProfile, profile.ProfilePath(\u0026#34;.\u0026#34;)).Stop() 如果运行此版本，则将配置文件写入当前工作目录\n% go run mandelbrot.go 2017/09/17 12:22:06 profile: cpu profiling enabled, cpu.pprof 2017/09/17 12:22:08 profile: cpu profiling disabled, cpu.pprof 使用 pkg/profile 不是强制性的，但是它会处理收集和记录跟踪信息的许多样板，因此我们将在本讲习班的其余部分中使用它。\n5.3.2. 分析性能 # 现在我们有了一个性能分析，我们可以使用 go tool pprof 对其进行分析。\n% go tool pprof -http=:8080 cpu.pprof 在此运行中，我们看到程序运行了 1.81 秒 (分析增加了少量开销)。我们还可以看到 pprof 仅捕获了 1.53 秒的数据，因为 pprof 基于示例，它依赖于操作系统的 SIGPROF 计时器。\n从 1.9 开始，pprof 跟踪包含分析跟踪所需的所有信息。您不再需要生成跟踪的匹配二进制文件。🎉\n我们可以使用 top pprof 函数对跟踪记录的函数进行排序\n% go tool pprof cpu.pprof Type: cpu Time: Mar 24, 2019 at 5:18pm (CET) Duration: 2.16s, Total samples = 1.91s (88.51%) Entering interactive mode (type \u0026#34;help\u0026#34; for commands, \u0026#34;o\u0026#34; for options) (pprof) top Showing nodes accounting for 1.90s, 99.48% of 1.91s total Showing top 10 nodes out of 35 flat flat% sum% cum cum% 0.82s 42.93% 42.93% 1.63s 85.34% main.fillPixel 0.81s 42.41% 85.34% 0.81s 42.41% main.paint 0.11s 5.76% 91.10% 0.12s 6.28% runtime.mallocgc 0.04s 2.09% 93.19% 0.04s 2.09% runtime.memmove 0.04s 2.09% 95.29% 0.04s 2.09% runtime.nanotime 0.03s 1.57% 96.86% 0.03s 1.57% runtime.pthread_cond_signal 0.02s 1.05% 97.91% 0.04s 2.09% compress/flate.(*compressor).deflate 0.01s 0.52% 98.43% 0.01s 0.52% compress/flate.(*compressor).findMatch 0.01s 0.52% 98.95% 0.01s 0.52% compress/flate.hash4 0.01s 0.52% 99.48% 0.01s 0.52% image/png.filter 我们看到，当 pprof 捕获堆栈时，main.fillPixel 函数在 CPU 上的数量最多。\n在堆栈上找到 main.paint 并不奇怪，这就是程序的作用。它绘制像素。但是，是什么导致 paint 花费大量时间呢？ 我们可以通过将 cummulative 标志设置为 top 来进行检查。\n(pprof) top --cum Showing nodes accounting for 1630ms, 85.34% of 1910ms total Showing top 10 nodes out of 35 flat flat% sum% cum cum% 0 0% 0% 1840ms 96.34% main.main 0 0% 0% 1840ms 96.34% runtime.main 820ms 42.93% 42.93% 1630ms 85.34% main.fillPixel 0 0% 42.93% 1630ms 85.34% main.seqFillImg 810ms 42.41% 85.34% 810ms 42.41% main.paint 0 0% 85.34% 210ms 10.99% image/png.(*Encoder).Encode 0 0% 85.34% 210ms 10.99% image/png.Encode 0 0% 85.34% 160ms 8.38% main.(*img).At 0 0% 85.34% 160ms 8.38% runtime.convT2Inoptr 0 0% 85.34% 150ms 7.85% image/png.(*encoder).writeIDATs 这暗示着 main.fillPixed 实际上正在完成大部分工作。\n您也可以使用 web 命令来形象化配置文件，如下所示：\n5.4. 跟踪和性能分析 # 希望此示例显示了分析的局限性。性能分析告诉我们探查器看到的内容；fillPixel 正在完成所有工作。似乎没有很多事情可以做。\n因此，现在是引入执行跟踪程序的好时机，该跟踪程序可以为同一程序提供不同的视图。\n5.4.1. 使用执行跟踪器 # 使用跟踪程序就只需要改变为 profile.TraceProfile 即可。\nimport \u0026#34;github.com/pkg/profile\u0026#34; func main() { defer profile.Start(profile.TraceProfile, profile.ProfilePath(\u0026#34;.\u0026#34;)).Stop() 当我们运行程序时，我们在当前工作目录中得到一个 trace.out 文件。\n% go build mandelbrot.go % time ./mandelbrot 2017/09/17 13:19:10 profile: trace enabled, trace.out 2017/09/17 13:19:12 profile: trace disabled, trace.out real 0m1.740s user 0m1.707s sys 0m0.020s 就像 pprof 一样，go 命令中有一个工具可以分析跟踪。\n% go tool trace trace.out 2017/09/17 12:41:39 Parsing trace... 2017/09/17 12:41:40 Serializing trace... 2017/09/17 12:41:40 Splitting trace... 2017/09/17 12:41:40 Opening browser. Trace viewer s listening on http://127.0.0.1:57842 这个工具和 go tool pprof 有点不同。执行跟踪器正在重用 Chrome 内置的许多配置文件可视化基础结构，因此 go tool trace 充当服务器将原始执行跟踪转换为 Chome 可以本地显示的数据。\n5.4.2. 分析追踪 # 从跟踪中我们可以看到该程序仅使用一个 cpu。\nfunc seqFillImg(m *img) { for i, row := range m.m { for j := range row { fillPixel(m, i, j) } } } 这并不奇怪，默认情况下，mandelbrot.go 会按顺序为每一行中的每个像素调用 fillPixel。\n绘制完图像后，查看执行切换为写入 .png 文件。这会在堆上生成垃圾，因此跟踪在那时发生了变化，我们可以看到垃圾收集堆的经典锯齿模式。\n跟踪性能分析可提供低至 毫秒 级别的时序分辨率。 这是您使用外部性能分析无法获得的。\n在继续之前，我们需要谈谈跟踪工具的用法。\n抱歉，该工具使用 Chrome 内置的 javascript 调试支持。跟踪配置文件只能在 Chrome 中查看，而不能在 Firefox, Safari, IE/Edge 中使用。 因为这是 Google 产品，所以它支持键盘快捷键。使用 WASD 导航，使用 ? 获取列表。 查看追踪可能会占用大量内存。 认真地说，4Gb 不会削减它，8Gb 可能是最小值，更多肯定更好。 如果您是从 Fedora 之类的 OS 发行版中安装 Go 的，则跟踪查看器的支持文件可能不是主 golang deb/rpm 的一部分，它们可能位于某些 -extra 软件包中。 5.5. 使用多个 CPU # 从前面的跟踪中我们可以看到程序正在按顺序运行，并且没有利用该计算机上的其他 CPU。\nMandelbrot 的生成称为 embarassingly_parallel 。每个像素彼此独立，都可以并行计算。所以，让我们尝试一下。\n% go build mandelbrot.go % time ./mandelbrot -mode px 2017/09/17 13:19:48 profile: trace enabled, trace.out 2017/09/17 13:19:50 profile: trace disabled, trace.out real 0m1.764s user 0m4.031s sys 0m0.865s 因此，运行时间基本上是相同的。我们使用了所有 CPU，因此有更多的用户时间，这是有道理的，但是实 际(挂钟) 时间大致相同。\n让我们看一下追踪。\n如您所见，此跟踪生成了 更多 的数据。\n似乎需要完成很多工作，但是如果您放大放大，就会发现差距。据信这是调度程序。 虽然我们使用所有四个内核，但是由于每个 fillPixel 的工作量相对较小，因此我们在调度开销方面花费了大量时间。 5.6. 整理工作 # 每个像素使用一个 goroutine 太细粒度。没有足够的工作来证明 goroutine 的成本合理。\n相反，让我们尝试为每个 goroutine 处理一行。\n% go build mandelbrot.go % time ./mandelbrot -mode row 2017/09/17 13:41:55 profile: trace enabled, trace.out 2017/09/17 13:41:55 profile: trace disabled, trace.out real 0m0.764s user 0m1.907s sys 0m0.025s 这看起来是一个不错的改进，我们几乎将程序的运行时间减少了一半。 让我们看一下痕迹。\n如您所见，轨迹现在更小，更易于使用。我们可以看到跨度的整个轨迹，这是一个不错的奖励。\n在程序开始时，我们看到 goroutine 的数量大约为 1000 这是对上一条跟踪中看到的 1 \u0026lt;\u0026lt; 20 的改进。 放大后，我们看到 onePerRowFillImg 运行时间更长，并且由于 goroutine 生产 工作尽早完成，调度程序有效地处理了其余可运行的 goroutine。 5.7. 使用 workers # mandelbrot.go 支持另一种模式，请尝试一下\n% go build mandelbrot.go % time ./mandelbrot -mode workers 2017/09/17 13:49:46 profile: trace enabled, trace.out 2017/09/17 13:49:50 profile: trace disabled, trace.out real 0m4.207s user 0m4.459s sys 0m1.284s 因此，运行时比以前任何时候都差。让我们看一下追踪，看看是否可以弄清楚发生了什么。\n观察痕迹，您会发现只有一个 worker 处理器，生产者和消费者往往会轮换，因为只有一个 worker 处理器和一个消费者。 让我们增加 worker 处理器数量\n% go build mandelbrot.go % time ./mandelbrot -mode workers -workers 4 2017/09/17 13:52:51 profile: trace enabled, trace.out 2017/09/17 13:52:57 profile: trace disabled, trace.out real 0m5.528s user 0m7.307s sys 0m4.311s 这样就更糟了！ 更多实时，更多 CPU 时间。让我们看一下追踪，看看发生了什么。\n那条痕迹是一团糟。 有更多的 worker 处理器可用，但是似乎所有的时间都花在处理器执行上。\n这是因为通道是 无缓存的。只有在有人准备接收之前，无缓冲的通道才能发送。\n在没有 worker 处理器准备接收之前，生产者无法发送作品。 worker 处理器要等到有人准备派遣后才能接受工作，因此他们在等待时会互相竞争。 发送者没有特权，它不能比已经运行的工作者享有优先权。 我们在这里看到的是无缓冲通道带来的大量延迟。调度程序内部有很多停止和启动，并且在等待工作时可能会锁定和互斥，这就是为什么我们看到 sys 时间更长的原因。\n5.8. 使用缓冲通道 # import \u0026#34;github.com/pkg/profile\u0026#34; func main() { defer profile.Start(profile.TraceProfile, profile.ProfilePath(\u0026#34;.\u0026#34;)).Stop() % go build mandelbrot.go % time ./mandelbrot -mode workers -workers 4 2017/09/17 14:23:56 profile: trace enabled, trace.out 2017/09/17 14:23:57 profile: trace disabled, trace.out real 0m0.905s user 0m2.150s sys 0m0.121s 这与上面的每行模式非常接近。\n使用缓冲的通道，跟踪显示出：\n生产者不必等待 worker 处理器的到来，它可以迅速填补渠道。 worker 处理器可以快速从通道中取出下一个物品，而无需休眠等待生产。 使用这种方法，我们使用通道进行每个像素的工作传递的速度几乎与之前在每行 goroutine 上进行调度的速度相同。\n修改 nWorkersFillImg 以每行工作。计时结果并分析轨迹。\n5.9. Mandelbrot 微服务 # 在 2019 年，除非您可以将 Internet 作为无服务器微服务提供，否则生成 Mandelbrots 毫无意义。 因此，我向您介绍 Mandelweb\n% go run examples/mandelweb/mandelweb.go 2017/09/17 15:29:21 listening on http://127.0.0.1:8080/ http://127.0.0.1:8080/mandelbrot\n5.9.1. 跟踪正在运行的应用程序 # 在前面的示例中，我们对整个程序进行了跟踪。\n如您所见，即使在很短的时间内，跟踪也可能非常大，因此，连续收集跟踪数据将产生太多的数据。同样，跟踪可能会影响程序的速度，特别是在活动很多的情况下。\n我们想要的是一种从正在运行的程序中收集简短跟踪的方法。\n幸运的是，net/http/pprof 软件包具有这样的功能。\n5.9.2. 通过 http 收集跟踪 # 希望每个人都知道 net/http/pprof 软件包。\nimport _ \u0026#34;net/http/pprof\u0026#34; 导入后，net/http/pprof 将向 http.DefaultServeMux 注册跟踪和分析路由。从 Go 1.5 开始，这包括跟踪分析器。\nnet/http/pprof 向 http.DefaultServeMux 注册。如果您隐式或显式地使用该 ServeMux，则可能会无意间将 pprof 端点公开到 Internet。这可能导致源代码泄露。您可能不想这样做。\n我们可以使用 curl（或wget）从 mandelweb 中获取五秒钟的跟踪记录\n% curl -o trace.out http://127.0.0.1:8080/debug/pprof/trace?seconds=5 5.9.3. 产生一些负载 # 前面的示例很有趣，但是根据定义，空闲的 Web 服务器没有性能问题。我们需要产生一些负载。为此，我使用的是 hey by JBD。\n% go get -u github.com/rakyll/hey 让我们从每秒一个请求开始。\n% hey -c 1 -n 1000 -q 1 http://127.0.0.1:8080/mandelbrot 然后运行，在另一个窗口中收集跟踪\n% curl -o trace.out http://127.0.0.1:8080/debug/pprof/trace?seconds=5 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 66169 0 66169 0 0 13233 0 --:--:-- 0:00:05 --:--:-- 17390 % go tool trace trace.out 2017/09/17 16:09:30 Parsing trace... 2017/09/17 16:09:30 Serializing trace... 2017/09/17 16:09:30 Splitting trace... 2017/09/17 16:09:30 Opening browser. Trace viewer is listening on http://127.0.0.1:60301 5.9.4. 模拟过载 # 让我们将速率提高到每秒 5 个请求。\n% hey -c 5 -n 1000 -q 5 http://127.0.0.1:8080/mandelbrot 然后运行，在另一个窗口中收集跟踪\n% curl -o trace.out http://127.0.0.1:8080/debug/pprof/trace?seconds=5 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 66169 0 66169 0 0 13233 0 --:--:-- 0:00:05 --:--:-- 17390 % go tool trace trace.out 2017/09/17 16:09:30 Parsing trace... 2017/09/17 16:09:30 Serializing trace... 2017/09/17 16:09:30 Splitting trace... 2017/09/17 16:09:30 Opening browser. Trace viewer is listening on http://127.0.0.1:60301 5.9.5. 额外的信誉，Eratosthenes 的筛子 # concurrent prime sieve 是最早编写的 Go 程序之一。\nIvan Daniluk 撰写了一篇关于可视化的很棒的文章。\n让我们看一下使用执行跟踪器的操作。\n5.9.6. 更多资源 # Rhys Hiltner, Go’s execution tracer (dotGo 2016) Rhys Hiltner, An Introduction to \u0026ldquo;go tool trace\u0026rdquo; (GopherCon 2017) Dave Cheney, Seven ways to profile Go programs (GolangUK 2016) Dave Cheney, High performance Go workshop] Ivan Daniluk, Visualizing Concurrency in Go (GopherCon 2016) Kavya Joshi, Understanding Channels (GopherCon 2017) Francesc Campoy, Using the Go execution tracer 6. 内存和垃圾收集器 # Go 是一种 gc 语言。这是设计原则，不会改变。\n作为 gc 语言，Go 程序的性能通常取决于它们与 gc 的交互。\n除了选择算法之外，内存消耗是决定应用程序性能和可伸缩性的最重要因素。\n本节讨论垃圾收集器的操作，如何测量程序的内存使用情况以及在垃圾收集器性能成为瓶颈的情况下降低内存使用量的策略。\n6.1. gc 的世界观 # 任何垃圾收集器的目的都是为了给程序一种幻想，即存在无限数量的可用内存。\n您可能不同意此声明，但这是垃圾收集器设计者如何工作的基本假设。\n令人震惊的是，就总运行时间而言，标记扫描 GC 是最有效的；很好，适用于批处理，模拟等。但是，随着时间的流逝，Go GC 已从纯粹的停止世界收集器转变为并发的非压缩收集器。这是因为 Go GC 专为低延迟服务器和交互式应用程序而设计。\nGo GC 的设计倾向于在 最大吞吐量 上 降低延迟。它将一些分配成本移到了 mutator 上，以减少以后的清理成本。\n6.2. 垃圾收集器设计 # 多年来，Go GC 的设计发生了变化\nGo 1.0, 停止大量基于 tcmalloc 的世界标记清除收集器。 Go 1.3, 完全精确的收集器，不会将堆上的大数字误认为是指针，从而不会浪费内存。 Go 1.5, 新的 GC 设计，着重于 吞吐量 延迟。 Go 1.6, GC 的改进，以较低的延迟处理较大的堆。 Go 1.7, 较小的 GC 改进，主要是重构。 Go 1.8, 进一步工作以减少 STW 时间，现在已降至 100 微秒范围。 Go 1.10+, 摆脱纯粹的合作 Goroutine 调度 以降低触发整个GC周期时的延迟。 6.3. 垃圾收集器监控 # 一种获得垃圾收集器工作量的总体思路的简单方法是启用 GC 日志记录的输出。\n这些统计信息始终会收集，但通常会被禁止显示，您可以通过设置环境变量 GODEBUG 启用它们。\n% env GODEBUG=gctrace=1 godoc -http=:8080 gc 1 @0.012s 2%: 0.026+0.39+0.10 ms clock, 0.21+0.88/0.52/0+0.84 ms cpu, 4-\u0026gt;4-\u0026gt;0 MB, 5 MB goal, 8 P gc 2 @0.016s 3%: 0.038+0.41+0.042 ms clock, 0.30+1.2/0.59/0+0.33 ms cpu, 4-\u0026gt;4-\u0026gt;1 MB, 5 MB goal, 8 P gc 3 @0.020s 4%: 0.054+0.56+0.054 ms clock, 0.43+1.0/0.59/0+0.43 ms cpu, 4-\u0026gt;4-\u0026gt;1 MB, 5 MB goal, 8 P gc 4 @0.025s 4%: 0.043+0.52+0.058 ms clock, 0.34+1.3/0.64/0+0.46 ms cpu, 4-\u0026gt;4-\u0026gt;1 MB, 5 MB goal, 8 P gc 5 @0.029s 5%: 0.058+0.64+0.053 ms clock, 0.46+1.3/0.89/0+0.42 ms cpu, 4-\u0026gt;4-\u0026gt;1 MB, 5 MB goal, 8 P gc 6 @0.034s 5%: 0.062+0.42+0.050 ms clock, 0.50+1.2/0.63/0+0.40 ms cpu, 4-\u0026gt;4-\u0026gt;1 MB, 5 MB goal, 8 P gc 7 @0.038s 6%: 0.057+0.47+0.046 ms clock, 0.46+1.2/0.67/0+0.37 ms cpu, 4-\u0026gt;4-\u0026gt;1 MB, 5 MB goal, 8 P gc 8 @0.041s 6%: 0.049+0.42+0.057 ms clock, 0.39+1.1/0.57/0+0.46 ms cpu, 4-\u0026gt;4-\u0026gt;1 MB, 5 MB goal, 8 P gc 9 @0.045s 6%: 0.047+0.38+0.042 ms clock, 0.37+0.94/0.61/0+0.33 ms cpu, 4-\u0026gt;4-\u0026gt;1 MB, 5 MB goal, 8 P 跟踪输出给出了GC活性的一般度量。 the runtime package documentation 中描述了 gctrace=1 的输出格式。\nDEMO: 显示启用了 GODEBUG=gctrace=1 的 godoc\n在生产环境中使用此环境变量，不会对性能产生影响。\n当您知道有问题时，使用 GODEBUG=gctrace=1 很好，但是对于Go应用程序上的常规遥测，我建议使用 net/http/pprof 接口。\nimport _ \u0026#34;net/http/pprof\u0026#34; 导入 net/http/pprof 软件包将在 /debug/pprof 注册一个具有各种运行时指标的处理程序，包括：\nA list of all the running goroutines, /debug/pprof/heap?debug=1. A report on the memory allocation statistics, /debug/pprof/heap?debug=1. net/http/pprof 将使用默认的 http.ServeMux 注册自己。\n请小心，因为如果您使用 http.ListenAndServe(address, nil)，这将是可见的。\nDEMO: godoc -http=:8080, 会显示 /debug/pprof。\n6.3.1. 垃圾收集器调整 # Go 运行时提供了一个用于调整 GC 的环境变量 GOGC。\nGOGC 的公式是\n$$ goal = reachabl\\e * (1 + (GOGC)/100) $$ 例如，如果我们当前有一个256MB的堆，并且 GOGC=100 (默认值)，当堆填满时，它将增长到\n$$ 512MB = 256MB * (1 + 100/100) $$ GOGC 的值大于100会使堆增长更快，从而减轻了 GC 的压力。 小于 100 的 GOGC 值会导致堆缓慢增长，从而增加了 GC 的压力。 默认值 100 仅作为参考。 在使用生产负载对应用程序进行性能分析 之后，您应该选择自己的值。\n6.4. 减少分配 # 确保您的 API 允许调用方减少生成的垃圾量。\n考虑这两种读取方法\nfunc (r *Reader) Read() ([]byte, error) func (r *Reader) Read(buf []byte) (int, error) 第一个 Read 方法不带任何参数，并以 []byte 的形式返回一些数据。第二个接收一个 []byte 缓冲区，并返回读取的字节数。\n第一个 Read 方法将 始终 分配缓冲区，从而给GC带来压力。第二个填充它给定的缓冲区。\n您可以在标准库中命名遵循此模式的示例吗？\n6.5. strings 和 []bytes # 在 Go 中，string 的值是不可变的，[]byte 是可变的。\n大多数程序都喜欢使用 string，但是大多数 IO 是使用 []byte 来完成的。\n尽可能避免将 []byte 转换为字符串，这通常意味着选择一种表示形式，即 string 或 []byte 作为值。如果您从网络或磁盘读取数据，通常为 []byte。\nbytes 包包含许多与 strings 软件包相同的操作 - Split, Compare, HasPrefix，Trim 等。\n在底层，strings 与 bytes包使用相同的汇编原语。\n6.6. 使用 []byte 作为 map 的 key # 使用 string 作为映射键是很常见的，但是通常您会使用 []byte。\n编译器针对这种情况实现了特定的优化\nvar m map[string]string v, ok := m[string(bytes)] 这将避免将字节切片转换为用于映射查找的字符串。这是非常具体的操作，如果您执行以下操作将无法正常工作\nkey := string(bytes) val, ok := m[key] 让我们看看这是否仍然正确。编写一个基准，比较使用 []byte 作为 string 映射键的这两种方法。\n6.7. 避免字符串连接 # 转到字符串是不可变的。连接两个字符串会生成第三个字符串。 以下哪项是最快的？\ns := request.ID s += \u0026#34; \u0026#34; + client.Addr().String() s += \u0026#34; \u0026#34; + time.Now().String() r = s var b bytes.Buffer fmt.Fprintf(\u0026amp;b, \u0026#34;%s %v %v\u0026#34;, request.ID, client.Addr(), time.Now()) r = b.String() r = fmt.Sprintf(\u0026#34;%s %v %v\u0026#34;, request.ID, client.Addr(), time.Now()) b := make([]byte, 0, 40) b = append(b, request.ID...) b = append(b, \u0026#39; \u0026#39;) b = append(b, client.Addr().String()...) b = append(b, \u0026#39; \u0026#39;) b = time.Now().AppendFormat(b, \u0026#34;2006-01-02 15:04:05.999999999 -0700 MST\u0026#34;) r = string(b) var b strings.Builder b.WriteString(request.ID) b.WriteString(\u0026#34; \u0026#34;) b.WriteString(client.Addr().String()) b.WriteString(\u0026#34; \u0026#34;) b.WriteString(time.Now().String()) r = b.String() DEMO: go test -bench=. ./examples/concat\n6.8. 如果长度已知，则预分配片 # 追加很方便，但是很浪费。\n切片通过将多达 1024 个元素加倍而增长，然后增加约 25％。我们再追加一项后，b 的容量是多少？\nfunc main() { b := make([]int, 1024) b = append(b, 99) fmt.Println(\u0026#34;len:\u0026#34;, len(b), \u0026#34;cap:\u0026#34;, cap(b)) } 如果使用 append 模式，则可能会复制大量数据并创建大量垃圾。\n如果知道事先知道切片的长度，则可以预先分配目标，以避免复制并确保目标大小正确。\nBefore\nvar s []string for _, v := range fn() { s = append(s, v) } return s After\nvals := fn() s := make([]string, len(vals)) for i, v := range vals { s[i] = v } return s 6.9. 使用 sync.Pool # sync 软件包带有 sync.Pool 类型，用于重用公共对象。\nsync.Pool 没有固定大小或最大容量。您添加到它并从中取出直到发生 GC，然后将其无条件清空。这是 by design:\n如果在垃圾回收之前为时过早而在垃圾回收之后为时过晚，则排空池的正确时间必须在垃圾回收期间。也就是说，池类型的语义必须是它在每个垃圾回收时都消耗掉。— Russ Cox\n使用 sync.Pool\nvar pool = sync.Pool{ New: func() interface{} { return make([]byte, 4096) }, } func fn() { buf := pool.Get().([]byte) // takes from pool or calls New // do work pool.Put(buf) // returns buf to the pool } sync.Pool 不是缓存。它可以并且将在任何时间清空。\n不要将重要项目放在 sync.Pool 中，它们将被丢弃。\nGo 1.13 中可能会更改在每个 GC 上清空的 sync.Pool 的设计，这将有助于提高其实用性。\n此 CL 通过引入受害者缓存机制来解决此问题。代替清除池，将删除受害缓存，并将主缓存移至受害缓存。 结果，在稳定状态下，（几乎）没有新分配，但是如果 Pool 使用率下降，对象仍将在两个 GC（而不是一个）中收集。— Austin Clements\nhttps://go-review.googlesource.com/c/go/+/166961/\n6.10. 练习 # 使用 godoc（或其他程序）观察使用 GODEBUG=gctrace=1 改变 GOGC 的结果。 使用 bytes, string 作为 map key 并检查基准。 不同字符串连接策略的基准分配。 7. 提示和旅行 # 随机获取提示和建议\n最后一部分包含一些微优化 Go 代码的技巧。\n7.1. Goroutines # Goroutines 是使其非常适合现代硬件的关键功能。\nGoroutines 易于使用，而且创建成本低廉，您可以将它们视为 几乎 免费。\nGo 运行时已针对具有成千上万个 goroutine 作为标准的程序而编写，数十万个并不意外。\n但是，每个 goroutine 确实消耗了 goroutine 堆栈的最小内存量，目前至少为2k。\n2048 * 1,000,000 个 goroutines == 2GB 的内存，他们还没有做任何事情。\n可能很多，但未提供应用程序的其他用法。\n7.1.1. 知道何时停止 goroutine # Goroutine 的启动方便，运行也很方便，但是在内存占用方面确实有一定的成本。您不能创建无限数量的它们。\n每次您在程序中使用go关键字启动 goroutine 时，您都必须 知道 该 goroutine 如何以及何时退出。\n在您的设计中，某些 goroutine 可能会运行直到程序退出。这些 goroutine 非常罕见，不会成为规则的例外。\n如果您不知道答案，那将是潜在的内存泄漏，因为 goroutine 会将其堆栈的内存以及可从堆栈访问的所有堆分配变量固定在堆栈上。\n切勿在不知道如何停止 goroutine 的情况下启动它。\n7.1.2. 进一步阅读 # Concurrency Made Easy (video) Concurrency Made Easy (slides) Never start a goroutine without knowning when it will stop (Practical Go, QCon Shanghai 2018) 7.2. Go 对某些请求使用有效的网络轮询 # Go 运行时使用有效的操作系统轮询机制（kqueue，epoll，windows IOCP等）处理网络IO。一个单一的操作系统线程将为许多等待的 goroutine 提供服务。\n但是，对于本地文件 IO，Go 不会实现任何 IO 轮询。 *os.File 上的每个操作在进行中都会消耗一个操作系统线程。\n大量使用本地文件 IO 可能会导致您的程序产生数百或数千个线程。可能超出您的操作系统所允许的范围。\n您的磁盘子系统不希望能够处理成百上千的并发 IO 请求。\n要限制并发阻塞 IO 的数量，请使用工作程序 goroutine 池或缓冲通道作为信号灯。\nvar semaphore = make(chan struct{}, 10) func processRequest(work *Work) { semaphore \u0026lt;- struct{}{} // acquire semaphore // process request \u0026lt;-semaphore // release semaphore } 7.3. 注意您的应用程序中的 IO multipliers # 如果您正在编写服务器进程，那么它的主要工作是多路复用通过网络连接的客户端和存储在应用程序中的数据。\n大多数服务器程序接受请求，进行一些处理，然后返回结果。这听起来很简单，但是根据结果，它可能会使客户端消耗服务器上大量（可能是无限制的）资源。 这里有一些注意事项:\n每个传入请求的 IO 请求数量；单个客户端请求生成多少个IO事件？它可能平均为 1，或者如果从缓存中提供了许多请求，则可能小于一个。 服务查询所需的读取量；它是固定的，N + 1还是线性的（读取整个表以生成结果的最后一页）。 相对而言，如果内存很慢，那么IO太慢了，您应该不惜一切代价避免这样做。最重要的是，避免在请求的上下文中进行IO，不要让用户等待您的磁盘子系统写入磁盘甚至读取磁盘。\n7.4. 使用流式 IO 接口 # 尽可能避免将数据读取到 []byte 中并将其传递。\n根据请求，您可能最终将兆字节（或更多！）的数据读取到内存中。这给 GC 带来了巨大压力，这将增加应用程序的平均延迟。\n而是使用 io.Reader 和 io.Writer 来构造处理管道，以限制每个请求使用的内存量。\n为了提高效率，如果您使用大量的 io.Copy，请考虑实现 io.ReaderFrom/io.WriterTo。 这些接口效率更高，并且避免将内存复制到临时缓冲区中。\n7.5. 超时, 超时, 超时 # 在不知道所需的最长时间之前，切勿启动 IO 操作。\n您需要使用 SetDeadline, SetReadDeadline, SetWriteDeadline 对每个网络请求设置超时。\n7.6. defer 消耗很大，还是？ # defer 消耗很高，因为它必须记录下 defer 的论点。\ndefer mu.Unlock() 相当于\ndefer func() { mu.Unlock() }() 如果完成的工作量很小，则 defer 会很昂贵，经典的例子是 defer 围绕结构变量或映射查找进行互斥解锁。在这种情况下，您可以选择避免 defer。\n在这种情况下，为了获得性能而牺牲了可读性和维护性。\n始终重新审视这些决定。\n7.7. 避免 Finalisers # Finalisation 是一种将行为附加到即将被垃圾回收的对象的技术。\n因此，最终确定是不确定的。\n要运行 finalizer，该对象不得通过任何物体到达。 如果您不小心在地图上保留了对该对象的引用，则该对象不会被最终确定。\nfinalizer 是 gc 周期的一部分，这意味着它们何时运行将是不可预测的，并且与减少 gc 操作的目标相矛盾。\n如果堆很大并且已调整应用程序以创建最少的垃圾，则 finalizer 可能不会运行很长时间。\n7.8. 减少 cgo # cgo 允许 Go 程序调用 C 库。\nC 代码和 Go 代码生活在两个不同的世界中，cgo 穿越了它们之间的边界。\n这种转换不是免费的，并且取决于它在代码中的位置，其成本可能很高。\ncgo 调用类似于阻塞 IO，它们在操作期间消耗线程。\n不要在紧密循环中调用 C 代码。\n7.8.1. 其实，也许避免 cgo # cgo 的开销很高。\n为了获得最佳性能，我建议您在应用程序中避免使用 cgo。\n如果C代码花费很长时间，则 cgo 开销并不重要。 如果您使用 cgo 调用非常短的 C 函数（其开销最明显），请在 Go 中重写该代码 - 根据定义，这很短。 如果您在紧密的循环中使用了大量昂贵的 C 代码，那么为什么要使用 Go？ 是否有人使用 cgo 频繁调用昂贵的 C 代码？\n进一步阅读 # cgo is not Go 7.9. 始终使用最新发布的Go版本 # 旧版本的 Go 永远不会变得更好。他们将永远不会得到错误修复或优化。\nGo 1.4 不应该使用。 Go 1.5 和 1.6 编译器速度较慢，但生成的代码更快，GC更快。 Go 1.7 与 1.6 相比，编译速度提高了约 30％，链接速度提高了 2 倍（比任何以前的Go版本都要好）。 Go 1.8 （此时）将提供较小的编译速度改进，但非英特尔架构的代码质量将得到显着改进。 Go 1.9-1.12 继续提高所生成代码的性能，修复错误，并改善内联和改进调试。 旧版本的Go没有更新。请勿使用。使用最新版本，您将获得最佳性能。\n7.9.1. 进一步阅读 # Go 1.7 toolchain improvements Go 1.8 performance improvements 7.9.2. 将热点字段移动到 struct 顶部 # 7.10. 讨论 # Any questions?\n8. 最后的问题和结论 # 可读意味着可靠 — Rob Pike\n从最简单的代码开始。\n测量。 分析您的代码以识别瓶颈，请不要猜测。\n如果性能良好，请 停止。您无需优化所有内容，只需优化代码中最热的部分。\n随着应用程序的增长或流量模式的发展，性能热点将发生变化。\n不要留下对性能不重要的复杂代码，如果瓶颈转移到其他地方，请使用更简单的操作将其重写。\n始终编写最简单的代码，编译器针对 正规 代码进行了优化。\n较短的代码是较快的代码； Go 不是 C++，不要指望编译器能够解开复杂的抽象。\n代码越短，代码 越小；这对于CPU的缓存很重要。\n非常注意分配，尽可能避免不必要的分配。\n如果事情不一定正确，我可以将事情做得很快。— Russ Cox\n性能和可靠性同样重要。\n我认为制作一个非常快速的服务器但是却定期 panics，死锁或 OOM 毫无价值。\n不要为了可靠性而牺牲性能。\n1. Hennessy et al: 40 年的年度绩效提高了 1.4 倍。\nref # https://github.com/davecheney/gophercon2018-performance-tuning-workshop https://dave.cheney.net/high-performance-go-workshop/dotgo-paris.html#avoid_finalisers https://github.com/davecheney/high-performance-go-workshop\n"},{"id":10,"href":"/posts/backend/report/","title":"报告实现","section":"Blog","content":"文章简介：报告系统设计\n需求是无外网环境下，汇总当前系统的运行状态，需要绘制大量图表到 word/pdf。\n实现语言: golang\n如何更加方便的管理报告模版 # 主要思路：\n报告模版使用 markdown+golang/text/template 渲染，图表使用 datauri 直接内嵌入 markdown。\u0008\u0008markdown 经过 ast 解析后翻译成对应的元素，渲染成 report.Report 中间接口。report.Report 接口作为中间层，下边实现 pdf/word 渲染实现。\n最终达成的目标：样式由 report.Report 的实现决定，上层 markdown 关注内容。\ngithub.com/yuin/goldmark\n报表中大量图表如何渲染 # 使用前端技术绘制图表，可在浏览器中直接获得结果\n渲染服务：控制浏览器打开网页，并指定截图某区域\u0008.\n有一个关键设计: chromedp 控制浏览器，会没有基础的 post请求方式，需要通过控制浏览器执行 js, 对网页进行控制（创建 dom ，并画出图）\n渲染服务无状态，需要管理 chrome headless。对外提供 openapi\ngolang\nhttps://github.com/browserless/chrome/issues/52 github.com/chromedp/chromedp\n如何保存最终生成的报告 # markdown 直接存储到数据库，优点不需要做大量小文件优化\n"},{"id":11,"href":"/posts/Go/go-issue/","title":"Go Issue","section":"Blog","content":"记录 golang 相关很难调试的 bug\nbugs # SIGSEGV during C.getaddrinfo # https://github.com/golang/go/issues/30310\nglibc 的 bug 导致 go 中 net 模块静态编译后的可执行文件 getaddrinfo 的时候 pinic，无法启动服务 解决办法: 临时使用 go 自己的 net 实现 -tags netgo, 或者 已经 build 的程序 GODEBUG=netdns=go binery 加环境变量使用 go 的 net 实现\n"},{"id":12,"href":"/posts/Architecture/opentracing/jaeger-agent/","title":"Opentracing - jaeger","section":"Blog","content":"文章简介：介绍 Opentracing - jaeger\nagent # Agent 处于 jaeger-client 和 collector 之间，属于代理的作用，主要是把 client 发送过来的数据从 thrift 转为 Batch，并通过 RPC 批量提交到 collector\njaegertracing/jaeger/cmd/agent/app/flags.go#L62\nvar defaultProcessors = []struct { model Model protocol Protocol port int }{ {model: \u0026#34;zipkin\u0026#34;, protocol: \u0026#34;compact\u0026#34;, port: 5775}, {model: \u0026#34;jaeger\u0026#34;, protocol: \u0026#34;compact\u0026#34;, port: 6831}, {model: \u0026#34;jaeger\u0026#34;, protocol: \u0026#34;binary\u0026#34;, port: 6832}, } jaegertracing/jaeger/cmd/agent/app/servers/tbuffered_server.go#L82\n// Serve initiates the readers and starts serving traffic func (s *TBufferedServer) Serve() { atomic.StoreUint32(\u0026amp;s.serving, 1) for s.IsServing() { readBuf := s.readBufPool.Get().(*ReadBuf) n, err := s.transport.Read(readBuf.bytes) if err == nil { readBuf.n = n s.metrics.PacketSize.Update(int64(n)) select { case s.dataChan \u0026lt;- readBuf: s.metrics.PacketsProcessed.Inc(1) s.updateQueueSize(1) default: s.metrics.PacketsDropped.Inc(1) } } else { s.metrics.ReadError.Inc(1) } } } jaegertracing/jaeger/blob/master/cmd/agent/app/processors/thrift_processor.go#L114\n// processBuffer reads data off the channel and puts it into a custom transport for // the processor to process func (s *ThriftProcessor) processBuffer() { for readBuf := range s.server.DataChan() { protocol := s.protocolPool.Get().(thrift.TProtocol) payload := readBuf.GetBytes() protocol.Transport().Write(payload) s.logger.Debug(\u0026#34;Span(s) received by the agent\u0026#34;, zap.Int(\u0026#34;bytes-received\u0026#34;, len(payload))) if ok, err := s.handler.Process(protocol, protocol); !ok { s.logger.Error(\u0026#34;Processor failed\u0026#34;, zap.Error(err)) s.metrics.HandlerProcessError.Inc(1) } s.protocolPool.Put(protocol) s.server.DataRecd(readBuf) // acknowledge receipt and release the buffer } } jaegertracing/jaeger/thrift-gen/agent/agent.go#L187\nfunc (p *agentProcessorEmitBatch) Process(seqId int32, iprot, oprot thrift.TProtocol) (success bool, err thrift.TException) { args := AgentEmitBatchArgs{} if err = args.Read(iprot); err != nil { iprot.ReadMessageEnd() return false, err } iprot.ReadMessageEnd() var err2 error if err2 = p.handler.EmitBatch(args.Batch); err2 != nil { return true, err2 } return true, nil } jaegertracing/jaeger/thrift-gen/jaeger/tchan-jaeger.go#L39\nCollector # 接收 Agent 的数据 # jaegertracing/jaeger/cmd/collector/app/handler/thrift_span_handler.go#L60\n比较舒服的维护metrics的场景\nreferences # jaeger "},{"id":13,"href":"/posts/Architecture/opentracing/overview/","title":"Opentracing Overview","section":"Blog","content":"文章简介：简单介绍 opentracing，以及技术选型。\n基础名词理解 # spec-zh\nspec-office\n语义惯例\nreferences # jaeger opentracing-contrib "},{"id":14,"href":"/posts/db/elastic/","title":"ElasticSearch","section":"Blog","content":"ElasticSearch 整理一下知识点\nElasticSearch 是一个分布式搜索引擎，底层使用 Lucene 来实现其核心搜索功能.其核心是全文检索.\n全文检索 # 倒排索引 + TF-IDF为全文搜索的基石。\nElasticSearch 诞生的背景 # 大规模数据如何检索 # 当系统数据量上了 10 亿、100 亿条的时候，我们在做系统架构的时候通常会从以下角度去考虑问题：\n用什么数据库好？(mysql、postgres、sybase、oracle、达梦、神通、mongodb、hbase…) 如何解决单点故障；(lvs、F5、A10、Zookeep、MQ) 如何保证数据安全性；(热备、冷备、异地多活) 如何解决检索难题；(数据库代理中间件：mysql-proxy、Cobar、MaxScale 等;) 如何解决统计分析问题；(离线、近实时) 传统数据库的应对解决方案 # 对于关系型数据，我们通常采用以下或类似架构去解决查询瓶颈和写入瓶颈：\n通过主从备份解决数据安全性问题； 通过数据库代理中间件心跳监测，解决单点故障问题； 通过代理中间件将查询语句分发到各个 slave 节点进行查询，并汇总结果 非关系型数据库的解决方案 # 对于 Nosql 数据库，基本原理类似：\n通过副本备份保证数据安全性； 通过节点竞选机制解决单点问题； 先从配置库检索分片信息，然后将请求分发到各个节点，最后由路由节点合并汇总结果 Elastic 理论知识 # Elasticsearch vs mysql # Mysql -\u0026gt; database -\u0026gt; table -\u0026gt; rows -\u0026gt; columns Elasticsearch -\u0026gt; index -\u0026gt; type -\u0026gt; documents -\u0026gt; fields\nelastic 在 7.x 之后将去掉 types, 替代方案：index per document type / custom type field\nES 的 CRUD # 资料\n索引新文档 # 当用户向一个节点提交了一个索引新文档的请求，节点会计算新文档应该加入到哪个分片（shard）中。每个节点都存储有每个分片存储在哪个节点的信息，因此协调节点会将请求发送给对应的节点。注意这个请求会发送给主分片，等主分片完成索引，会并行将请求发送到其所有副本分片，保证每个分片都持有最新数据。\n每次写入新文档时，都会先写入内存中，并将这一操作写入一个 translog 文件（transaction log）中，此时如果执行搜索操作，这个新文档还不能被索引到。\nES 会每隔 1 秒时间（这个时间可以修改）进行一次刷新操作（refresh），此时在这 1 秒时间内写入内存的新文档都会被写入一个文件系统缓存（filesystem cache）中，并构成一个分段（segment）。此时这个 segment 里的文档可以被搜索到，但是尚未写入硬盘，即如果此时发生断电，则这些文档可能会丢失。\n不断有新的文档写入，则这一过程将不断重复执行。每隔一秒将生成一个新的 segment，而 translog 文件将越来越大。每隔 30 分钟或者 translog 文件变得很大，则执行一次 fsync 操作。此时所有在文件系统缓存中的 segment 将被写入磁盘，而 translog 将被删除（此后会生成新的 translog）。\n由上面的流程可以看出，在两次 fsync 操作之间，存储在内存和文件系统缓存中的文档是不安全的，一旦出现断电这些文档就会丢失。所以 ES 引入了 translog 来记录两次 fsync 之间所有的操作，这样机器从故障中恢复或者重新启动，ES 便可以根据 translog 进行还原。\n此外，由于每一秒就会生成一个新的 segment，很快将会有大量的 segment。对于一个分片进行查询请求，将会轮流查询分片中的所有 segment，这将降低搜索的效率。因此 ES 会自动启动合并 segment 的工作，将一部分相似大小的 segment 合并成一个新的大 segment。合并的过程实际上是创建了一个新的 segment，当新 segment 被写入磁盘，所有被合并的旧 segment 被清除。\n更新（Update）和删除（Delete）文档 # ES 的索引是不能修改的，因此更新和删除操作并不是直接在原索引上直接执行。每一个磁盘上的 segment 都会维护一个 del 文件，用来记录被删除的文件。每当用户提出一个删除请求，文档并没有被真正删除，索引也没有发生改变，而是在 del 文件中标记该文档已被删除。因此，被删除的文档依然可以被检索到，只是在返回检索结果时被过滤掉了。每次在启动 segment 合并工作时，那些被标记为删除的文档才会被真正删除。\n更新文档会首先查找原文档，得到该文档的版本号。然后将修改后的文档写入内存，此过程与写入一个新文档相同。同时，旧版本文档被标记为删除，同理，该文档可以被搜索到，只是最终被过滤掉。\n读操作（Read）：查询过程 # 查询阶段 # 当一个节点接收到一个搜索请求，则这个节点就变成了协调节点。第一步是广播请求到索引中每一个节点的分片拷贝。 查询请求可以被某个主分片或某个副本分片处理，协调节点将在之后的请求中轮询所有的分片拷贝来分摊负载。\n每个分片将会在本地构建一个优先级队列。如果客户端要求返回结果排序中从第 from 名开始的数量为 size 的结果集，则每个节点都需要生成一个 from+size 大小的结果集，因此优先级队列的大小也是 from+size。分片仅会返回一个轻量级的结果给协调节点，包含结果集中的每一个文档的 ID 和进行排序所需要的信息。\n协调节点会将所有分片的结果汇总，并进行全局排序，得到最终的查询排序结果。此时查询阶段结束。\n取回阶段 # 查询过程得到的是一个排序结果，标记出哪些文档是符合搜索要求的，此时仍然需要获取这些文档返回客户端。\n协调节点会确定实际需要返回的文档，并向含有该文档的分片发送 get 请求；分片获取文档返回给协调节点；协调节点将结果返回给客户端。\n分布式一致性原理 # 资源，有源码分析\nES 集群构成 # node.master node.data 两两组合成不同的节点\n节点发现 # Master 选举 # 多数派原则 选主\nreferences # elasticsearch cheatsheet Elasticsearch Reference Elasticsearch－基础介绍及索引原理分析 Choosing a fast unique identifier (UUID) for Lucene Elasticsearch架构原理 Elasticsearch性能优化 ElasticSearch读写底层原理及性能调优 "},{"id":15,"href":"/posts/high_performance/linux_epoll/","title":"Linux epoll","section":"Blog","content":"epoll 高性能 network io 模型\n由于太多 blog 已经讲 epoll，再此引用看的blog，以及项目。并根据自己的理解，实现出一个 epoll 实现的 server。\nTODO: 需要跟一步设计实现跨平台的序列化反序列化方案，进一步实现应用层需求。\nepoll # 高性能 异步io 模型，在 Linux 是 epoll，mac kqueue，windows IOCP.\nApp: nginx, redis\nreferences # EPOLL的LINUX内核工作机制 https://blog.lucode.net/linux/epoll-tutorial.html https://github.com/millken/c-example/blob/master/epoll-example.c https://www.suchprogramming.com/epoll-in-3-easy-steps/ https://medium.com/@copyconstruct/the-method-to-epolls-madness-d9d2d6378642 https://github.com/angrave/SystemProgramming/wiki/Networking,-Part-7:-Nonblocking-I-O,-select(),-and-epoll https://github.com/angrave/SystemProgramming/wiki https://jvns.ca/blog/2017/06/03/async-io-on-linux--select--poll--and-epoll simple http server base on epoll https://github.com/cloudwu/cstring camel # 使用 linux epoll 实现的 socket server.\n"},{"id":16,"href":"/posts/db/how-fast-the-read-write/","title":"读写操作到底有多慢, 操作系统相关一些参数","section":"Blog","content":"文章简介：Latency numbers every programmer should know\nL1 cache reference ......................... 0.5 ns Branch mispredict ............................ 5 ns L2 cache reference ........................... 7 ns Mutex lock/unlock ........................... 25 ns Main memory reference ...................... 100 ns Compress 1K bytes with Zippy ............. 3,000 ns = 3 µs Send 2K bytes over 1 Gbps network ....... 20,000 ns = 20 µs SSD random read ........................ 150,000 ns = 150 µs Read 1 MB sequentially from memory ..... 250,000 ns = 250 µs Round trip within same datacenter ...... 500,000 ns = 0.5 ms Read 1 MB sequentially from SSD* ..... 1,000,000 ns = 1 ms Disk seek ........................... 10,000,000 ns = 10 ms Read 1 MB sequentially from disk .... 20,000,000 ns = 20 ms Send packet CA-\u0026gt;Netherlands-\u0026gt;CA .... 150,000,000 ns = 150 ms src\n"},{"id":17,"href":"/posts/tools/dotfiles/","title":"整理了一套 dotfiles 自用","section":"Blog","content":"文章简介：自己的 dotfiles 供自己使用，几乎一键换机\n简介 # 使用方法\ngit clone https://github.com/exfly/dotfiles.git ~/.dotfiles cd ~/.dotfiles make preinstall-arch make bootstrap make install 备注 # 对于所有的 *.zsh 都会加载到 .zshrc 中，所有的 *.symlink 都会对应的在$HOME生成一个软连接 .*, 文件夹也可以直接使用这种方式进行加载。\n比如 zsh/zshrc.symlink 会生成软连接 $HOME/.zshrc,具体如何工作可以看一下 script/bootstrap\nreferences # my dotfiles "},{"id":18,"href":"/posts/docker/comtainerd-cgroup-namespace/","title":"Comtainerd Cgroup Namespace","section":"Blog","content":"文章简介：Docker Cgroup , Namespace , union fs 运行机制\nNamespaces # 命名空间 (namespaces) 是 Linux 为我们提供的用于分离进程树、网络接口、挂载点以及进程间通信等资源的方法.\nLinux 的命名空间机制提供了以下七种不同的命名空间，包括 CLONE_NEWCGROUP、CLONE_NEWIPC、CLONE_NEWNET、CLONE_NEWNS、CLONE_NEWPID、CLONE_NEWUSER 和 CLONE_NEWUTS(分别对应 Cgroup, IPC, Network, Mount, PID, User, UTS)，通过这七个选项我们能在创建新的进程时设置新进程应该在哪些资源上与宿主机器进行隔离。\nthe Network namespace encapsulates system resources related to networking such as network interfaces (e.g wlan0, eth0), route tables etc, the Mount namespace encapsulates files and directories in the system, PID contains process IDs and so on. So two instances of a Network namespace A and B (corresponding to two boxes of the same type in our analogy) can contain different resources - maybe A contains wlan0 while B contains eth0 and a different route table copy \u0026ndash; related\n$ ls -l /proc/$$/ns total 0 lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 cgroup -\u0026gt; \u0026#39;cgroup:[4026531835]\u0026#39; lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 ipc -\u0026gt; \u0026#39;ipc:[4026531839]\u0026#39; lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 mnt -\u0026gt; \u0026#39;mnt:[4026531840]\u0026#39; lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 net -\u0026gt; \u0026#39;net:[4026531992]\u0026#39; lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 pid -\u0026gt; \u0026#39;pid:[4026531836]\u0026#39; lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 pid_for_children -\u0026gt; \u0026#39;pid:[4026531836]\u0026#39; lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 user -\u0026gt; \u0026#39;user:[4026531837]\u0026#39; lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 uts -\u0026gt; \u0026#39;uts:[4026531838]\u0026#39; $ sudo unshare -u bash $ ls -l /proc/$$/ns total 0 lrwxrwxrwx 1 root root 0 Sep 19 01:00 cgroup -\u0026gt; \u0026#39;cgroup:[4026531835]\u0026#39; lrwxrwxrwx 1 root root 0 Sep 19 01:00 ipc -\u0026gt; \u0026#39;ipc:[4026531839]\u0026#39; lrwxrwxrwx 1 root root 0 Sep 19 01:00 mnt -\u0026gt; \u0026#39;mnt:[4026531840]\u0026#39; lrwxrwxrwx 1 root root 0 Sep 19 01:00 net -\u0026gt; \u0026#39;net:[4026531992]\u0026#39; lrwxrwxrwx 1 root root 0 Sep 19 01:00 pid -\u0026gt; \u0026#39;pid:[4026531836]\u0026#39; lrwxrwxrwx 1 root root 0 Sep 19 01:00 pid_for_children -\u0026gt; \u0026#39;pid:[4026531836]\u0026#39; lrwxrwxrwx 1 root root 0 Sep 19 01:00 user -\u0026gt; \u0026#39;user:[4026531837]\u0026#39; lrwxrwxrwx 1 root root 0 Sep 19 01:00 uts -\u0026gt; \u0026#39;uts:[4026532239]\u0026#39; unshare 会在一个新的 namespace 执行新的程序，flag -u 指定新的 UTS namespace, sudo unshare -u bash即是在新的UTS命名空间执行 bash\nlinux 以默认的 namespace 启动新的程序，除非明确的指定\nuser namespace # P$ Q$ 不同的字母代表不同的 user namespace\nP$ whoami vagrant P$ id uid=1000(vagrant) gid=1000(vagrant) groups=1000(vagrant),977(docker) P$ unshare -U bash # Enter a new shell that runs within a nested user namespace, the shell is bash which is your cmd shell C$ id uid=65534(nobody) gid=65534(nobody) groups=65534(nobody) C$ ls -l /proc/$pid/uid_map # the map file /proc/$pid/uid_map returns a mapping from UIDs in the user namespace to which the process pid belongs， user_namespace linux man page\n每一行的格式为 $fromID $toID $length，\nC$ echo $$ 8683 # 新的 PID P$ echo $$ 7898 # 原始 PID C$ cat /proc/8683/uid_map C$ cat /proc/7898/uid_map 0 0 4294967295 P$ id uid=1000(vagrant) gid=1000(vagrant) groups=1000(vagrant),977(docker) P$ ip link add type veth RTNETLINK answers: Operation not permitted # 在一个不同的username 和 net namespace中尝试 P$ unshare -nU bash C$ ip link add type veth RTNETLINK answers: Operation not permitted C$ echo $$ 9078 P$ echo \u0026#34;0 1000 1\u0026#34; \u0026gt; /proc/9078/uid_map C$ id uid=0(root) gid=65534(nobody) groups=65534(nobody) C$ ip link add type veth # Success! P$ echo deny \u0026gt; /proc/9078/setgroups P$ echo \u0026#34;0 1000 1\u0026#34; \u0026gt; /proc/9078/gid_map C$ id uid=0(root) gid=0(root) groups=0(root),65534(nobody) mount namespace # cat /proc/$$/mounts proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0 sys /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0 dev /dev devtmpfs rw,nosuid,relatime,size=496792k,nr_inodes=124198,mode=755 0 0 run /run tmpfs rw,nosuid,nodev,relatime,mode=755 0 0 /dev/sda2 / btrfs rw,relatime,space_cache,subvolid=5,subvol=/ 0 0 securityfs /sys/kernel/security securityfs rw,nosuid,nodev,noexec,relatime 0 0 tmpfs /dev/shm tmpfs rw,nosuid,nodev 0 0 devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0 tmpfs /sys/fs/cgroup tmpfs ro,nosuid,nodev,noexec,mode=755 0 0 cgroup2 /sys/fs/cgroup/unified cgroup2 rw,nosuid,nodev,noexec,relatime,nsdelegate 0 0 cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,name=systemd 0 0 pstore /sys/fs/pstore pstore rw,nosuid,nodev,noexec,relatime 0 0 bpf /sys/fs/bpf bpf rw,nosuid,nodev,noexec,relatime,mode=700 0 0 cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0 cgroup /sys/fs/cgroup/pids cgroup rw,nosuid,nodev,noexec,relatime,pids 0 0 cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0 cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0 cgroup /sys/fs/cgroup/rdma cgroup rw,nosuid,nodev,noexec,relatime,rdma 0 0 cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0 cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0 cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0 cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0 cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0 cgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0 hugetlbfs /dev/hugepages hugetlbfs rw,relatime,pagesize=2M 0 0 configfs /sys/kernel/config configfs rw,nosuid,nodev,noexec,relatime 0 0 systemd-1 /proc/sys/fs/binfmt_misc autofs rw,relatime,fd=46,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=10711 0 0 mqueue /dev/mqueue mqueue rw,nosuid,nodev,noexec,relatime 0 0 debugfs /sys/kernel/debug debugfs rw,nosuid,nodev,noexec,relatime 0 0 tmpfs /tmp tmpfs rw,nosuid,nodev 0 0 tmpfs /run/user/1000 tmpfs rw,nosuid,nodev,relatime,size=100836k,mode=700,uid=1000,gid=1000 0 0 vagrant /vagrant vboxsf rw,relatime 0 0 mount point\n$ unshare -m bash $ cat /proc/$$/mounts /dev/sda2 / btrfs rw,relatime,space_cache,subvolid=5,subvol=/ 0 0 dev /dev devtmpfs rw,nosuid,relatime,size=496792k,nr_inodes=124198,mode=755 0 0 tmpfs /dev/shm tmpfs rw,nosuid,nodev 0 0 devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0 hugetlbfs /dev/hugepages hugetlbfs rw,relatime,pagesize=2M 0 0 mqueue /dev/mqueue mqueue rw,nosuid,nodev,noexec,relatime 0 0 proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0 systemd-1 /proc/sys/fs/binfmt_misc autofs rw,relatime,fd=46,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=10711 0 0 sys /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0 securityfs /sys/kernel/security securityfs rw,nosuid,nodev,noexec,relatime 0 0 tmpfs /sys/fs/cgroup tmpfs ro,nosuid,nodev,noexec,mode=755 0 0 cgroup2 /sys/fs/cgroup/unified cgroup2 rw,nosuid,nodev,noexec,relatime,nsdelegate 0 0 cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,name=systemd 0 0 cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0 cgroup /sys/fs/cgroup/pids cgroup rw,nosuid,nodev,noexec,relatime,pids 0 0 cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0 cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0 cgroup /sys/fs/cgroup/rdma cgroup rw,nosuid,nodev,noexec,relatime,rdma 0 0 cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0 cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0 cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0 cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0 cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0 cgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0 pstore /sys/fs/pstore pstore rw,nosuid,nodev,noexec,relatime 0 0 bpf /sys/fs/bpf bpf rw,nosuid,nodev,noexec,relatime,mode=700 0 0 configfs /sys/kernel/config configfs rw,nosuid,nodev,noexec,relatime 0 0 debugfs /sys/kernel/debug debugfs rw,nosuid,nodev,noexec,relatime 0 0 tracefs /sys/kernel/debug/tracing tracefs rw,nosuid,nodev,noexec,relatime 0 0 run /run tmpfs rw,nosuid,nodev,relatime,mode=755 0 0 tmpfs /run/user/1000 tmpfs rw,nosuid,nodev,relatime,size=100836k,mode=700,uid=1000,gid=1000 0 0 tmpfs /tmp tmpfs rw,nosuid,nodev 0 0 vagrant /vagrant vboxsf rw,relatime 0 0 说明 unshare -m bash 并没有按照预期在新的 mount namespace 中运行\n所以为了验证mount namespace, 我们需要做如下一些事情:\n创建命令所需的依赖项和系统文件的副本 创建一个新的 mount namespace 将新挂载命名空间中的根文件系统替换为由我们的系统文件副本组成的根文件系统。 在新的安装名称空间内执行程序 进行实验 # $ wget http://dl-cdn.alpinelinux.org/alpine/v3.10/releases/x86_64/alpine-minirootfs-3.10.1-x86_64.tar.gz $ mkdir rootfs $ tar -xzf alpine-minirootfs-3.10.1-x86_64.tar.gz -C rootfs $ ls rootfs $ ls rootfs/{mnt,dev,proc,home,sys} Pivot root # $ unshare -m bash $ mount --bind rootfs rootfs $ cd rootfs $ mkdir put_old $ pivot_root . put_old $ cd / # We should now have our new root. e.g if we: $ ls proc # proc is empty # And the old root is now in put_old $ ls put_old bin dev home lib lost+found mnt proc run srv tmp var boot etc initrd.img lib64 media opt root sbin sys usr vmlinuz $ /bin/busybox umount -l put_old # 卸载旧的文件系统 $ mount -t proc proc proc/ 到这里，已经创建了一个隔离的 rootfs\n进程 # $ ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 01:17 ? 00:00:00 /sbin/init root 2 0 0 01:17 ? 00:00:00 [kthreadd] root 3 2 0 01:17 ? 00:00:00 [rcu_gp] root 4 2 0 01:17 ? 00:00:00 [rcu_par_gp] root 5 2 0 01:17 ? 00:00:00 [kworker/0:0-events] root 6 2 0 01:17 ? 00:00:00 [kworker/0:0H-kblockd] root 7 2 0 01:17 ? 00:00:00 [kworker/u4:0-btrfs-endio-write] root 8 2 0 01:17 ? 00:00:00 [mm_percpu_wq] 有两个进程很特殊，pid={1,2}, 这两个进程都是被 Linux 中的上帝进程 idle 创建出来的\npid=1 的 /sbin/init, 执行内核的一部分初始化工作和系统配置，也会创建一些类似 getty 的注册进程 pid=2 的kthreadd, 管理和调度其他的内核进程 网络 # 在默认情况下，每一个容器在创建时都会创建一对虚拟网卡，两个虚拟网卡组成了数据的通道，其中一个会放在创建的容器中，会加入到名为 docker0 网桥中。我们可以使用如下的命令来查看当前网桥的接口：\n$ brctl show bridge name bridge id STP enabled interfaces br-43ee5ed53f84 8000.024297630322 no docker0 8000.0242bf0282f8 no docker0 会为每一个容器分配一个新的 IP 地址并将 docker0 的 IP 地址设置为默认的网关。网桥 docker0 通过 iptables 中的配置与宿主机器上的网卡相连，所有符合条件的请求都会通过 iptables 转发到 docker0 并由网桥分发给对应的机器。\n$ iptables -t nat -L Chain PREROUTING (policy ACCEPT) target prot opt source destination DOCKER all -- anywhere anywhere ADDRTYPE match dst-type LOCAL Chain DOCKER (2 references) target prot opt source destination RETURN all -- anywhere anywhere ip # $ ip link list 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether 08:00:27:d4:b6:c8 brd ff:ff:ff:ff:ff:ff 3: eth1: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc fq_codel state DOWN mode DEFAULT group default qlen 1000 link/ether 08:00:27:b7:f5:14 brd ff:ff:ff:ff:ff:ff $ ip netns add coke $ ip netns list coke ip netns list 只显示命名的 netns，初始 netns 是非命名 netns。并且每一个命名 netns 都会在/var/run/netns创建同名文件，这个文件可以让进程切换到此 netns\nC$ 代表在一个子命名空间中执行\n$ ip netns exec coke bash C$ ip link list 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 C$ ping 127.0.0.1 connect: Network is unreachable C$ ip link set dev lo up C$ ip link list 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 C$ ping 127.0.0.1 # OK 现在在此 netns 中只可以与同 netns 的进程进行沟通(localhost),我们可以尝试与 init netns 的程序进行沟通\nVeth Devices # $ ip link add veth0 type veth peer name veth1 # # Create a veth pair (veth0 \u0026lt;=\u0026gt; veth1) $ ip link set veth1 netns coke # # Move the veth1 end to the new namespace C$ ip link list 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 7: veth1@if5: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether ee:16:0c:23:f3:af brd ff:ff:ff:ff:ff:ff link-netnsid 0 $ ip addr add 10.1.1.1/24 dev veth0 现在 veth1 设备已经可以在两个 netns 中看到，为了是他们都可以工作，我们需要给它们两个ip addresses 和让interface up\n$ ip addr add 10.1.1.1/24 dev veth0 # In the initial namespace $ ip link set dev veth0 up C$ ip addr add 10.1.1.2/24 dev veth1 # # In the coke namespace C$ ip link set dev veth1 up C$ ip addr show veth1 4: veth1@if5: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 1a:e7:9f:4e:d4:db brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.1.1.2/24 scope global veth1 valid_lft forever preferred_lft forever inet6 fe80::18e7:9fff:fe4e:d4db/64 scope link valid_lft forever preferred_lft forever 现在 veth0 和 veth1 已经 up 和赋予了 ip address 10.1.1.1 10.1.1.2\n$ ping -I veth0 10.1.1.2 C$ ping 10.1.1.1 netlink # libnetwork # chroot # https://www.ibm.com/developerworks/cn/linux/l-cn-chroot/index.html\nCGroups # https://www.ibm.com/developerworks/cn/linux/1506_cgroup/index.html\nUnionFS # docker export $(docker create busybox) | tar -C rootfs -xvf - AUFS overlay2\n查看系统是否支持相应的文件系统\n$ uname -a Linux archlinux 5.1.15-arch1-1-ARCH #1 SMP PREEMPT Tue Jun 25 04:49:39 UTC 2019 x86_64 GNU/Linux $ grep btrfs /proc/filesystems btrfs 有一些系统这种方式找不到对应的系统，但是同样支持此文件系统，比如 archlinux 发行版， Overlay_filesystem, overlay-docker-doc\n$ mkdir b0 b1 b2 upper work merged $ mount -t overlay overlay -o lowerdir=./b0:./b1:./b2,upperdir=./upper,workdir=./work ./merged overlay2 将 lowerdir、upperdir、workdir 联合挂载，形成最终的 merged 挂载点，其中 lowerdir 是镜像只读层，upperdir 是容器可读可写层，workdir 是执行涉及修改 lowerdir 执行 copy_up 操作的中转层（例如，upperdir 中不存在，需要从 lowerdir 中进行复制，该过程暂未详细了解，遇到了再分析）\n$ tree . ├── b0 ├── b1 ├── b2 ├── merged ├── README.md ├── upper └── work ├── index └── work 获得的文件系统是有层次的，当前的层次关系为：\n/upper /b0 /b1 /b2 $ echo \u0026#39;192.168.0.1\u0026#39; \u0026gt; b0/ip.txt $ tree . ├── b0 │ └── ip.txt ├── b1 ├── b2 ├── merged │ └── ip.txt ├── README.md ├── upper └── work ├── index └── work $ cat merged/ip.txt 192.168.0.1 $ echo \u0026#39;192.168.0.2\u0026#39; \u0026gt; merged/ip.txt $ tree . ├── b0 │ └── ip.txt ├── b1 ├── b2 ├── merged │ └── ip.txt ├── README.md ├── upper │ └── ip.txt └── work ├── index └── work $ cat upper/ip.txt 192.168.0.2 $ echo \u0026#39;192.168.0.3\u0026#39; \u0026gt; upper/ip.txt $ cat merged/ip.txt 192.168.0.3 $ rm -rf merged/ip.txt $ tree . ├── b0 │ └── ip.txt ├── b1 │ └── ip.txt ├── b2 ├── merged ├── README.md ├── upper │ └── ip.txt └── work ├── index └── work $ ls upper/ip.txt c--------- 1 root root 0, 0 Sep 18 04:35 upper/ip.txt references # container-lab\nhttps://draveness.me/docker\n调试网络 docker run -it --net container:vibrant_blackburn nicolaka/netshoot\nhttps://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt\noverlayfs的一些限制/兼容性问题\nlinux-namespace-isolate-programing 有一个例子程序\nNamespaces in operation\ncgroup/linux-man-page\nuser-namespace/linum-man-page\nmount namespaces\nproc special filesystem\nmanagement ip netns\nip command cheatsheet\nveth\nNetlink-doc\nman netlink\nA deep dive into Linux namespaces\n100个容器周边项目，点亮你的容器集群技能树\n"},{"id":19,"href":"/posts/Architecture/log/","title":"Log","section":"Blog","content":"文章简介：\u0008log\n在多个系统相互协作的分布式系统中，为了能够保证异步系统的协作是否工作，需要一套日志系统保证系统正常的工作。\n工具 # 应用内日志，比如开一个新的 log table，其中记录关键元数据，以及一些返回结果。应用内日志记录的内容未来可以作为数据 migration 的有效工具，保证没有成功的操作可以在未来重试，保证系统的数据的完整 opentracing 跟踪请求链路，方便 debug 分布式系统。（具体如何做，之后补充 TODO: flag） elk 系列 nginx 日志 "},{"id":20,"href":"/posts/tools/git/","title":"Git","section":"Blog","content":"文章简介：git 简单使用， 分享一些资料\n\u0008references # 图解 Git Git 交互式学习环境-演示 Git 交互式学习环境-学习中使用 Google Engineering Practices Documentation Conventional Commits 有工具可以生成 changelog "},{"id":21,"href":"/posts/cs/linker/","title":"Linker","section":"Blog","content":"文章简介：链接器 学习资源\n链接器 学习资源\nreferences # Beginner\u0026rsquo;s Guide to Linkers-cn\nBeginner\u0026rsquo;s Guide to Linkers\nCppCon 2017: Nir Friedman “What C++ developers should know about globals (and the linker)”\n"},{"id":22,"href":"/posts/Architecture/mq/","title":"消息队列","section":"Blog","content":"文章简介：消息队列中需要解决的问题\n网络通信可能会包含，成功、失败以及超时三种情况\n消息投递语义 # 最多一次（At-Most Once）、最少一次（At-Least Once）以及恰好一次（Exactly Once）\n最多一次在 TCP/UDP 传输层协议就是保证最多一次消息投递，消息的发送者只会尝试发送该消息一次，并不会关心该消息是否得到了远程节点的响应。 最少一次引入超时重试的机制。同时引入新的问题，消息重复。 恰好一次从理论上来说，在分布式系统中想要解决消息重复的问题是不可能的，很多消息服务提供了正好一次的 QoS 其实是在接收端进行了去重。\n投递顺序 # 由于一些网络的问题，消息在投递时可能会出现顺序不一致性的情况，在网络条件非常不稳定时，我们就可能会遇到接收方处理消息的顺序和生产者投递的不一致；消费者就需要对顺序不一致的消息进行处理，常见的两种方式就是使用序列号或者状态机。\n序列号 # 用阻塞的方式保证序列号的递增或者忽略部分『过期』的消息。\n状态机 # 虽然消息投递的顺序是乱序的，但是资源最终还是通过状态机达到了我们想要的正确状态，不会出现不一致的问题。\n协议 # AMQP： StormMQ、RabbitMQ， 支持最多一次和最少一次的投递语义，当我们选择最少一次时，需要幂等或者重入机制保证消息重复不会出现问题。\nMQTT\nreference # 分布式事务的实现原理 分布式系统与消息的投递 浅谈数据库并发控制 - 锁和 MVCC 消息队列设计的精髓基本都藏在本文里了 消息队列设计精要 消息队列设计精要 "},{"id":23,"href":"/posts/Architecture/MicroServiceDesign/","title":"高可用系统设计问题、微服务设计笔记 脑图","section":"Blog","content":"高可用系统设计，有哪些问题需要解决\n什么是高可用系统 # 关于高可用的系统\n高可用系统需要解决的问题 # 挑战 # 成倍的 API 数量 引入网络延迟 CAP 理论，处理跨多个服务的事务复杂 调试分布式系统十分复杂 服务雪崩 大量请求堆积，故障恢复慢 微服务技术选型 API 版本管理混乱 TCC、事务消息队列 需要做的事 # 可扩展：水平扩展、垂直扩展。 通过冗余部署，避免单点故障。 隔离：避免单一业务占用全部资源。避免业务之间的相互影响 2. 机房隔离避免单点故障。 解耦：降低维护成本，降低耦合风险。减少依赖，减少相互间的影响。 限流：滑动窗口计数法、漏桶算法、令牌桶算法等算法。遇到突发流量时，保证系统稳定。 降级：紧急情况下释放非核心功能的资源。牺牲非核心业务，保证核心业务的高可用。 熔断：异常情况超出阈值进入熔断状态，快速失败。减少不稳定的外部依赖对核心服务的影响。 系统监控：对 CPU 利用率，load，内存，带宽，系统调用量，应用错误量，PV，UV 和业务量进行监控 自动化测试：通过完善的测试，减少发布引起的故障。 灰度发布(+回滚)：灰度发布是速度与安全性作为妥协，能够有效减少发布故障。 异步消息系统 需要深入的技术栈 # 分布式系统、DevOps、基础架构即代码(IaC)、不同类型的数据库、前端组件化和复合化、单元测试、全自动发布、迭代、小版本发布计划、测试工具、多版本管理\n分布式一致性与共识算法 # Consensus from wiki, 总结下来一致性就是，在分布式系统中，在给定的一系列操作，即使系统内部出错，最终整个系统对外提供的数据都是可靠的。在协调一致性的过程中，对于一个 Proposal 整个系统达成共识，共识算法起着很重要的作用。\nCAP # 加州伯克利大学的教授 Eric Brewer 论文 Brewer’s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services 阐述了 CAP 理论 CAP from wiki, 论文中提出的观点：在异步的网络模型中，所有的节点由于没有时钟仅仅能根据接收到的消息作出判断，这时完全不能同时保证一致性(Consistency)、可用性(Availability)和分区容错性(Partition tolerance)，每一个系统只能在这三种特性中选择两种。 由于网络有一定的延迟，并不能做到强一致性，所以大部分时候采用最终一致性的方式，容忍一定时间内数据不一致，在一定的时间内系统内部各节点可以在有限的时间内解决冲突，使数据恢复准确的状态。\n拜占庭将军问题 # 拜占庭将军问题论文\n拜占庭将军问题是对分布式系统容错的最高要求，然而这不是日常工作中使用的大多数分布式系统中会面对的问题，我们遇到更多的还是节点故障宕机或者不响应等情况，这就大大简化了系统对容错的要求\nFLP # FLP 不可能定理是分布式系统领域最重要的定理之一，它给出了一个非常重要的结论：在网络可靠并且存在节点失效的异步模型系统中，不存在一个可以解决一致性问题的确定性算法。\nIn this paper, we show the surprising result that no completely asynchronous consensus protocol can tolerate even a single unannounced process death. We do not consider Byzantine failures, and we assume that the message system is reliable it delivers all messages correctly and exactly once.\n相关论文\n共识算法 # paxos, raft # 包括 Paxos、Raft\nThe Raft Consensus Algorithm raft 工作原理动图\nPaxos 算法难以理解、难以实现，难道什么程度呢？在 raft 的论文中有提及。比如 zookeeper 的 zab 就是在 paxos 的基础上进行设计的。每一种 Paxos 的实现，都需要重新设计实现一套算法。而 raft 相对难度降低很多。 基于 raft 的 etcd，consul 等。\nPOW(Proof-of-Work) # 无论是 Paxos 还是 Raft 其实都只能解决非拜占庭将军容错的一致性问题，不能够应对分布式网络中出现的极端情况，但是这在传统的分布式系统都不是什么问题，无论是分布式数据库还是消息队列集群，它们内部的节点并不会故意的发送错误信息，在类似系统中，最常见的问题就是节点失去响应或者失效，所以它们在这种前提下是有效可行的，也是充分的。\n工作量证明是一个用于阻止拒绝服务攻击和类似垃圾邮件等服务错误问题的协议\nPOS(Proof-of-Stake) # 权益证明是区块链网络中的使用的另一种共识算法，在基于权益证明的密码货币中，下一个区块的选择是根据不同节点的股份和时间进行随机选择的。\nDPOS(Delegated Proof-of-Stake) # 《微服务设计》脑图 # 脑图如下： 脑图地址 references # 后端架构师技术图谱\nhystrix\nbilibili/kratos\nGo Microservices blog series\nmicroservices-in-golang\nshippy-demo\n20 个好用的 Go 语言微服务开发框架\nmicro/micro\nmicro/go-micro\nCAP theorem\ngrpc-example\n分布式一致性与共识算法\n"},{"id":24,"href":"/posts/db/log/","title":"The Log: What every software engineer should know about real-time data's unifying abstraction 翻译","section":"Blog","content":"文章简介：翻译 The Log: What every software engineer should know about real-time data\u0026rsquo;s unifying abstraction\n正文 # 六年前我在一个特别有趣的时间加入了 LinkedIn。 我们刚刚开始遇到单体集中式数据库的极限，需要开始向分布式系统过渡。 这是一个有趣的经历：我们构建，部署并运行分布式图形数据库，分布式搜索后端，Hadoop 安装以及第一代和第二代键值存储一直到今天。\n我在这一切中学到的最有用的东西之一就是我们正在构建的许多东西都有一个非常简单的概念：日志。 有时称为预写日志或提交日志或事务日志，日志几乎与计算机一样长，并且是许多分布式数据系统和实时应用程序体系结构的核心。\n在不了解日志的情况下，您无法完全理解数据库，NoSQL 存储，键值存储，复制，paxos，hadoop，版本控制或几乎任何软件系统; 然而，大多数软件工程师并不熟悉它们。 我想改变这一点。 在这篇文章中，我将向您介绍有关日志的所有信息，包括日志以及如何使用日志进行数据集成，实时处理和系统构建。\nPart One: What is Log # A log is perhaps the simplest possible storage abstraction. It is an append-only, totally-ordered sequence of records ordered by time.\n记录附加到日志的末尾，读取从左到右进行。 每个条目都分配有唯一的顺序日志条目号。\n记录的顺序定义了“时间”的概念，因为左边的条目被定义为比右边的条目更旧。 日志条目号可以被认为是条目的“时间戳”。 将这种排序描述为时间概念起初看起来有点奇怪，但它具有与任何特定物理时钟分离的便利特性。 当我们进入分布式系统时，此属性将变得至关重要。\n出于本讨论的目的，记录的内容和格式并不重要。 此外，我们不能只是继续向日志添加记录，因为我们最终将耗尽空间。 我会稍微回过头来看看。\n因此，日志与文件或表格完全不同。 文件是一个字节数组，一个表是一个记录数组，一个日志实际上只是一种表或文件，其中记录按时间排序。\n在这一点上，您可能想知道为什么值得谈论这么简单的事情？ 如何以仅附加的记录序列与数据系统相关联？ 答案是日志有一个特定的目的：它们记录发生的事情和时间。 对于分布式数据系统，这在许多方面都是问题的核心。\n但在我们走得太远之前，让我澄清一些有点令人困惑的事情。 每个程序员都熟悉另一个日志记录定义 - 应用程序可能使用 syslog 或 log4j 写入本地文件的非结构化错误消息或跟踪信息。 为清楚起见，我将其称为“应用程序日志记录”。 应用程序日志是我描述的日志概念的退化形式。 最大的区别是文本日志主要是供人阅读，而我所描述的“日志”或“数据日志”是为编程访问而构建的。\n（实际上，如果你对它进行深入的思考，那么人们读取某个机器上的日志这种理念有些不顺应时代。当涉及到许多服务和服务器的时候，这种方法很快就变成一个难于管理的方式，而且为了认识多个机器的行为，日志的目标很快就变成查询和图形化这些行为的输入了-对多个机器的某些行为而言，文件里的英文形式的文本同这儿所描述的这种结构化的日志相比几乎就不适合了。）\nLogs in databases # 我不知道日志概念的起源 - 可能是二元搜索这样的事情之一，对于发明者来说太简单了，不能发现它是一项发明。 它早在 IBM 的 System R 就已存在。数据库中的使用与在出现崩溃时保持同步的各种数据结构和索引有关。 为了使这种原子性和持久性，数据库使用日志来写出有关他们将要修改的记录的信息，然后将更改应用于它维护的所有各种数据结构。 日志是发生的事件的记录，每个表或索引是将该历史记录投影到一些有用的数据结构或索引中。 由于日志会立即保留，因此在崩溃时将其用作恢复所有其他持久性结构的权威来源。\n随着时间的推移，日志的使用从 ACID 的实现细节发展到在数据库之间复制数据的方法。 事实证明，数据库上发生的更改顺序正是保持远程副本数据库同步所需的。 Oracle，MySQL 和 PostgreSQL 包括日志传送协议，用于将部分日志传输到充当从属服务器的副本数据库。 Oracle 已将日志产品化为非 oracle 数据订阅者的通用数据订阅机制，其 XStream和 GoldenGate以及 MySQL 和 PostgreSQL 中的类似工具是许多数据架构的关键组件。\n由于这个起源，机器可读日志的概念主要局限于数据库内部。 使用日志作为数据订阅的机制似乎几乎是偶然出现的。 但这种抽象是支持各种消息传递，数据流和实时数据处理的理想选择。\nLogs in distributed systems # 日志解决的两个问题 - 排序更改(ordering changes)和分发数据(distributing data) - 在分布式数据系统中更为重要。 同意订购更新（或同意不同意和应对副作用）是这些系统的核心设计问题。\n分布式系统的以日志为中心的方法源于一个简单的讨论，我称之为状态机复制原则：\nIf two identical, deterministic processes begin in the same state and get the same inputs in the same order, they will produce the same output and end in the same state.\n如果两个相同的确定性过程以相同的状态开始并以相同的顺序获得相同的输入，则它们将产生相同的输出并以相同的状态结束。\n这可能看起来有点难懂的(obtuse)，所以让我们深入了解它的含义。\nDeterministic(确定性)意味着处理不依赖于时间，并且不允许任何其他“外部的”输入影响其结果。 例如，一个程序的输出受到线程执行的特定顺序的影响，或者通过调用 gettimeofday 或其他一些不可重复的东西，通常被认为是非确定性的。\n进程的 state 状态是处理结束时机器上的任何数据，无论是在内存中还是在磁盘上。\n以相同的顺序获得相同输入的地方应当引起注意-这就是引入日志的地方。这儿有一个重要的常识：如果给两段确定性代码相同的日志输入，那么它们就会生成相同的输出。\n分布式计算这方面的应用就格外明显。你可以把用多台机器一起执行同一件事情的问题缩减为实现分布式一致性日志为这些进程输入的问题。这日志的目的是把所有非确定性的东西排除在输入流之外，来确保每个复制进程能够同步地处理输入。\n当你理解了这个以后，状态机复制原理就不再复杂或者说不再深奥了：这或多或少的意味着\u0026quot;deterministic processing is deterministic\u0026quot;(确定性的处理过程就是确定性的)。不管怎样，我都认为它是分布式系统设计里较常用的工具之一。\n这种方式的一个美妙之处就在于索引日志的时间戳就像时钟状态的一个副本——你可以用一个单独的数字描述每一个副本，这就是经过处理的日志的时间戳。时间戳与日志一一对应着整个副本的状态。\n由于写进日志的内容的不同，也就有许多在系统中应用这个原则的不同方式。举个例子，我们记录一个服务的请求，或者服务从请求到响应的状态变化，或者它执行命令的转换。理论上来说，我们甚至可以为每一个副本记录一系列要执行的机器指令或者调用的方法名和参数。只要两个进程用相同的方式处理这些输入，这些进程就会保持副本的一致性。\n一千个人眼中有一千种日志的用法。数据库工作者通常区分物理日志和逻辑日志。物理日志就是记录每一行被改变的内容。逻辑日志记录的不是改变的行而是那些引起行的内容被改变的 SQL 语句（insert，update 和 delete 语句）。\n分布式系统通常可以宽泛分为两种方法来处理数据和完成响应。“状态机器模型”通常引用一个主动-主动的模型——也就是我们为之记录请求和响应的对象。对此进行一个细微的更改，称之为“主备模型”，就是选出一个副本做为 leader，并允许它按照请求到达的时间来进行处理并从处理过程中输出记录其状态改变的日志。其他的副本按照 leader 状态改变的顺序而应用那些改变，这样他们之间达到同步，并能够在 leader 失败的时候接替 leader 的工作。\n为了理解两种方式的不同，我们来看一个不太严谨的例子。假定有一个算法服务的副本，保持一个独立的数字作为它的状态（初始值为 0），并对这个值进行加法和乘法运算。主动-主动方式应该会输出所进行的变换，比如“+1”，“*2”等。每一个副本都会应用这些变换，从而得到同样的解集。主动-被动方式将会有一个独立的主体执行这些变换并输出结果日志，比如“1”，“3”，“6”等。这个例子也清楚的展示了为什么说顺序是保证各副本间一致性的关键：一次加法和乘法的顺序的改变将会导致不同的结果。\n分布式日志可以理解为一致性问题模型的数据结构。因为日志代表了后续追加值的一系列决策。你需要重新审视 Paxos 算法簇，尽管日志模块是他们最常见的应用。 在 Paxos 算法中，它通常通过使用称之为多 paxos 的协议，这种协议将日志建模为一系列的问题，在日志中每个问题都有对应的部分。在 ZAB， Raft等其它的协议中，日志的作用尤为突出，它直接对维护分布式的、一致性的日志的问题建模。\n我怀疑的是，我们就历史发展的观点是有偏差的，可能是由于过去的几十年中，分布式计算的理论远超过了其实际应用。在现实中，共识的问题是有点太简单了。计算机系统很少需要决定单个值，他们几乎总是处理成序列的请求。这样的记录，而不是一个简单的单值寄存器，自然是更加抽象。\n此外，专注于算法掩盖了 抽象系统需要的底层的日志。我怀疑，我们最终会把日志中更注重作为一个商品化的基石，不论其是否以同样的方式 实施的，我们经常谈论一个哈希表而不是纠结我们 得到是不是具体某个细节的哈希表，例如线性或者带有什么什么其它变体哈希表。日志将成为一种大众化的接口，为大多数算法和其实现提升提供最好的保证和最佳的性能。\nChangelog 101: Tables and Events are Dual # 让我们继续聊数据库。数据库中存在着大量变更日志和表之间的二相性。这些日志有点类似借贷清单和银行的流程，数据库表就是当前的盈余表。如果你有大量的变更日志，你就可以使用这些变更用以创建捕获当前状态的表。这张表将记录每个关键点（日志中一个特别的时间点）的状态信息。这就是为什么日志是非常基本的数据结构的意义所在：日志可用来创建基本表，也可以用来创建各类衍生表。同时意味着可以存储非关系型的对象。\n这个流程也是可逆的：如果你正在对一张表进行更新，你可以记录这些变更，并把所有更新的日志发布到表的状态信息中。这些变更日志就是你所需要的支持准实时的克隆。基于此，你就可以清楚的理解表与事件的二相性： 表支持了静态数据而日志捕获变更。日志的魅力就在于它是变更的完整记录，它不仅仅捕获了表的最终版本的内容，它还记录了曾经存在过的其它版本的信息。日志实质上是表历史状态的一系列备份。\n这可能会引起你对源代码的版本管理。源代码管理和数据库之间有密切关系。版本管理解决了一个大家非常熟悉的问题，那就是什么是分布式数据系统需要解决的\u0026mdash; 时时刻刻在变化着的分布式管理。版本管理系统通常以补丁的发布为基础，这实际上可能是一个日志。您可以直接对当前 类似于表中的代码做出“快照”互动。你会注意到， 与其他分布式状态化系统类似，版本控制系统 当你更新时会复制日志，你希望的只是更新补丁并将它们应用到你的当前快照中。\n最近，有些人从 Datomic \u0026ndash; 一家销售日志数据库的公司得到了一些想法。 这个演讲回顾了他们是如何讲这个想法应用到他们的系统中的。这些想法使他们对如何 在他们的系统应用这些想法有了开阔的认识。 当然这些想法不是只针对这个系统，他们会成为 十多年分布式系统和数据库文献的一部分。\n这可能似乎有点过于理想化。但是不要悲观！我们会很快把它实现。\nWhat\u0026rsquo;s next # 在这篇文章的其余部分，我将试图说明日志除了可用在分布式计算或者抽象分布式计算模型内部之外，还可用在哪些方面。其中包括：\n数据集成(Data Integration)-让机构的全部存储和处理系统里的所有数据很容易地得到访问。 实时数据处理(Real-time data processing)-计算生成的数据流。 分布式系统设计(Distributed system design)-实际应用的系统是如何通过使用集中式日志来简化设计的。 所有这些用法都是通过把日志用做单独服务来实现的。\n在上面任何一种用法里，日志的用途开始都是使用了日志所能提供的某个简单功能：生成永久的、可重现的历史记录。令人意外的是，问题的核心是可以让多少台机器以特定的方式，按照自身的速度重现历史记录的能力。\nPart Two: Data Integration # 请让我首先解释 一下“数据集成”是什么意思，还有为什么我觉得它很重要，之后我们再来看看它和日志有什么关系。\nData integration is making all the data an organization has available in all its services and systems.\n数据集成就是将数据组织起来，使得在与其有关的服务和系统中可以访问它们\n而更常见的术语 ETL 通常只是覆盖了数据集成的一个有限子集(译注：ETL，Extraction-Transformation-Loading 的缩写，即数据提取、转换和加载)——相对于关系型数据仓库。但我描述的东西很大程度上可以理解为，将 ETL 推广至实时系统和处理流程。\n对数据的高效使用遵循一种 马斯洛的需要层次理论 。金字塔的基础部分包括捕获所有相关数据，能够将它们全部放到适当的处理环境（那个环境应该是一个奇妙的实时查询系统，或者仅仅是文本文件和 python 脚本）。这些数据需要以统一的方式建模，这样就可以方便读取和数据处理。如果这种以统一的方式捕获数据的基本需求得到满足，那么就可以在基础设施上以若干种方法处理这些数据——映射化简（MapReduce），实时查询系统，等等。\n很明显，有一点值得注意：如果没有可靠的、完整的数据流，Hadoop 集群除了象昂贵的且难于安装的空间取暖器哪样外不会做更多事情了。一旦数据和处理可用，人们就会关心良好数据模型和一致地易于理解的语法哪些更细致的问题。最后，人们才会关注更加高级的处理-更好的可视化、报表以及处理和预测算法。 以我的经验，大多数机构在数据金字塔的底部存在巨大的漏洞-它们缺乏可靠的、完整的数据流-而是打算直接跳到高级数据模型技术上。这样做完全是反着来做的。\n因此，问题是我们如何构建通过机构内所有数据系统的可靠的数据流。\nData Integration: Two complications # 两种趋势使数据集成变得更困难。\n第一个趋势是增长的事件数据(event data)。事件数据记录的是发生的事情，而不是存在的东西。在 web 系统中，这就意味着用户活动日志，还有为了可靠的操作以及监控数据中心的机器的目的，所需要记录的机器级别的事件和统计数字。人们倾向称它们为“日志数据”，因为它们经常被写到应用的日志中，但是这混淆了形式与功能。这种数据位于现代 web 的中心：归根结底，Google 的资产是由这样一些建立在点击和映像基础之上的相关管道所生成的——那也就是事件。\n这些东西并不是仅限于网络公司，只是网络公司已经完全数字化，所以它们更容易用设备记录。财务数据一直是面向事件的。 RFID(无线射频识别)将这种跟踪能力赋予物理对象。我认为这种趋势仍将继续，伴随着这个过程的是传统商务活动的 数字化。\n这种类型的事件数据记录下发生的事情，而且往往比传统数据库应用要大好几个数量级。这对于处理提出了重大挑战。\nTThe explosion of specialized data systems # 第二个趋势来自于专门的数据系统的爆发，通常这些数据系统在最近的五年中开始变得流行，并且可以免费获得。专门的数据系统是为 OLAP, 搜索, 简单 在线 存储, 批处理, 图像分析, 等 等 而存在的。\n更多的不同类型数据的组合，以及将这些数据存放到更多的系统中的愿望，导致了一个巨大的数据集成问题。\nLog-structured data flow # 为了处理系统之间的数据流，日志是最自然的数据结构。其中的秘诀很简单：\n将所有组织的数据提取出来，并将它们放到一个中心日志，以便实时查阅。\n每个逻辑数据源都可以建模为它自己的日志。一个数据源可以是一个应用程序的事件日志（如点击量或者页面浏览量），或者是一个接受修改的数据库表。每个订阅消息的系统都尽可能快的从日志读取信息，将每条新的记录保存到自己的存储，并且提升其在日志中的地位。订阅方可以是任意一种数据系统 —— 一个缓存，Hadoop，另一个网站中的另一个数据库，一个搜索系统，等等。\n例如，日志针对每个更改给出了逻辑时钟的概念，这样所有的订阅方都可以被测量。推导不同的订阅系统的状态也因此变得相对简单的多，因为每个系统都有一个读取动作的“时间点”。\n为了让这个显得更具体，我们考虑一个简单的案例，有一个数据库和一组缓存服务器集群。日志提供了一种同步更新所有这些系统，并推导出每一个系统的接触时间点的方法。我们假设写了一条日志 X，然后需要从缓存做一次读取。如果我们想保证看到的不是陈旧的数据，我们只需保证没有从任何尚未复制 X 的缓存中读取即可。\n日志也起到缓存的作用，使数据生产与数据消费相同步。由于许多原因这个功能很重要，特别是在多个订阅方消费数据的速度各不相同的时候。这意味着一个订阅数据系统可以宕机，或者下线维护，之后重新上线以后再赶上来：订阅方按照自己控制的节拍来消费数据。批处理系统，如 Hadoop 或者是一个数据仓库，或许只是每小时或者每天消费一次数据，而实时查询系统可能需要及时到秒。由于无论是原始数据源还是日志，都没有各种目标数据系统的相关知识，因此消费方系统可以被添加和删除，而无需传输管道的变化。\nEache working data pipeline is designed like a log; each broken data pipeline is broken is its own way \u0026ndash; Anna Karenina principle\n特别重要的是：目标系统只知道日志而不知道原始系统的任何细节。 消费者系统不需要关心数据是来自 RDBMS，新的键值存储，还是没有任何类型的实时查询系统。 这似乎是一个小问题，但实际上是需要仔细鉴别的(critical)。\n这里我使用术语“日志”取代了“消息系统”或者“发布-订阅”，因为它在语义上更明确，并且对支持数据复制的实际实现这样的需求，有着更接近的描述。我发现“发布订阅”并不比间接寻址的消息具有更多的含义——如果你比较任何两个发布-订阅的消息传递系统的话，你会发现他们承诺的是完全不同的东西，而且大多数模型在这一领域都不是有用的。你可以认为日志是一种消息系统，它具有持久性保证和强大的订阅语义。在分布式系统中，这个通信模型有时有个(有些可怕的)名字叫做 atomic broadcast。\n值得强调的是，日志仍然只是基础设施。这并不是管理数据流这个故事的结束：故事的其余部分围绕着元数据，模式，兼容性，以及处理数据结构的所有细节及其演化。除非有一种可靠的，一般的方法来处理数据流运作，语义在其中总是次要的细节。\nAt LinkedIn # 在 LinkedIn 从集中式关系数据库向分布式系统集合转化的过程中，我看到这个数据集成问题迅速演变。\n主要的数据系统包括：\nSearch Social Graph Voldemort (key-value store) Espresso (document store) Recommendation engine OLAP query engine Hadoop Terradata Ingraphs (monitoring graphs and metrics services) 这种使用日志作为数据流的思想，甚至在我到这里之前就已经与 LinkedIn 相伴了。我们开发的一个最早的基础设施之一，是一种称为 databus 的服务，它在我们早期的 Oracle 表上提供了一种日志缓存抽象，可伸缩订阅数据库修改,这样我们就可以很好支持我们的社交网络和搜索索引。\n我会给出一些历史并交代一下上下文。我首次参与到这些大约是在 2008 年左右，在我们转移键值存储之后。我的下一个项目是让一个工作中的 Hadoop 配置演进，并给其增加一些我们的推荐流程。由于缺乏这方面的经验，我们自然而然的安排了数周计划在数据的导入导出方面，剩下的时间则用来实现奇妙的预测算法。这样我们就开始了长途跋涉。\n我们本来计划是仅仅将数据从现存的 Oracle 数据仓库中剖离。但是我们首先发现将数据从 Oracle 中迅速取出是一种黑暗艺术。更糟的是，数据仓库的处理过程与我们为 Hadoop 而计划的批处理生产过程不适合——其大部分处理都是不可逆转的，并且与即将生成的报告具体相关。最终我们采取的办法是，避免使用数据仓库，直接访问源数据库和日志文件。最后，我们为了加载 数据到键值存储 并生成结果，实现了另外一种管道。\n这种普通的数据复制最终成为原始开发项目的主要内容之一。糟糕的是，在任何时间任意管道都有一个问题，Hadoop 系统很大程度上是无用的——在错误的数据基础上运行奇特的(fancy)算法，只会产生更多的错误数据。\n虽然我们已经以一种通用的方式创建事物，但是每个数据源都需要自定义配置安装。这也被证明是巨量错误与失败的根源。我们在 Hadoop 上实现的网站功能已经开始流行起来，同时我们发现我们有一长串感兴趣的工程师。每个用户都有他们想要集成的一系列系统，他们想要的一系列新数据源。\nETL in Ancient Greece. Not much has changed.\n有些东西在我面前开始渐渐清晰起来。\n首先，我们已建成的通道虽然有一些杂乱，但实质上它们是很有价值的。在采用诸如 Hadoop 的新的处理系统生成可用数据的过程，它解锁了大量的可能性。 基于这些数据过去很难实现的计算，如今变为可能。 许多新的产品和分析技术都来源于把分片的数据放在一起，这些数据被锁定在特定的系统中。\n第二， 众所周知，可靠的数据加载需要数据通道的深度支持。如果我们可以捕获所有我们需要的结构，我就可以使得 Hadoop 数据全自动的加载，这样就不需要额外的操作来增加新的数据源或者处理模式变更\u0026ndash;数据就会自动的出现在 HDFS，Hive 表就会自动的生成对应于新数据源的恰当的列。\n第三，我们的数据覆盖率仍然非常低。如果你查看存储于 Hadoop 中的可用的 Linked 数据的全部百分比，它仍然是不完整的。花费大量的努力去使得各个新的数据源运转起来，使得数据覆盖度完整不是一件容易的事情。\n我们正在推行的，为每个数据源和目标增建客户化数据加载，这种方式很显然是不可行的。我们有大量的数据系统和数据仓库。把这些系统和仓库联系起来，就会导致任意一对系统会产生如下所示的客户化通道。\n需要注意的是：数据是双向流动的：例如许多系统诸如数据库和 Hadoop 既是数据转化的来源又是数据转化的目的地。这就意味着我们我们不必为每个系统建立两个通道：一个用于数据输入，一个用于数据输出。\n这显然需要一大群人，而且也不具有可操作性。随着我们接近完全连接，最终我们将有差不多 O(N2)条管道。\nInstead，我们需要像这样通用的东西：\n我们需要尽可能的将每个消费者与数据源隔离。理想情形下，它们应该只与一个单独的数据仓库集成，并由此让他们能访问到所有东西。\n这个思想是增加一个新的数据系统——或者它是一个数据源或者它是一个数据目的地——让集成工作只需连接到一个单独的管道，而无需连接到每个数据消费方。\n在相当长的时间内，Kafka 是独一无二的底层产品，它既不是数据库，也不是日志文件收集系统，更不是传统的消息系统。但是最近 Amazon 提供了非常类似 Kafka 的服务，称之为 Kinesis.相似度包括了分片处理的方式，数据的保持，甚至包括在 Kafka API 中，有点特别的高端和低端消费者分类。我很开心看到这些，这表明了你已经创建了很好的底层协议，AWS 已经把它作为服务提供。他们对此的期待与我所描述的吻合：通道联通了所有的分布式系统，诸如 DynamoDB, RedShift, S3 等，它同时作为使用 EC2 进行分布式流处理的基础。\nRelationship to ETL and the Data Warehouse # 我们再来聊聊数据仓库。数据仓库是清洗和归一数据结构用于支撑数据分析的仓库。这是一个伟大的理念。对不熟悉数据仓库概念的人来说，数据仓库方法论包括了：周期性的从数据源抽取数据，把它们转化为可理解的形式，然后把它导入中心数据仓库。对于数据集中分析和处理，拥有高度集中的位置存放全部数据的原始副本是非常宝贵的资产。在高层级上，也许你抽取和加载数据的顺序略微调整，这个方法论不会有太多变化,无论你使用传统的数据仓库 Oracle 还是 Teradata 或者 Hadoop。\n数据仓库是极其重要的资产，它包含了原始的和规整的数据，但是实现此目标的机制有点过时了。\n对以数据为中心的组织关键问题是把原始的归一数据联结到数据仓库。数据仓库是批处理的基础查询：它们适用于各类报表和临时性分析，特别是当查询包含了简单的计数、聚合和过滤。但是如果一个批处理系统仅仅包含了原始的完整的数据的数据仓库，这就意味着这些数据对于实时数据处理、搜索索引和系统监控等实时的查询是不可用的。\n依我之见，ETL 包括两件事：首先，它是抽取和数据清洗过程\u0026ndash;特别是释放被锁在组织的各类系统中的数据，移除系统专有的无用物。第二，依照数据仓库的查询重构数据。例如使其符合关系数据库类型系统，强制使用星号、雪花型模式，或者分解为高性能的柱状格式等。合并这两者是有困难的。这些规整的数据集应当可以在实时或低时延处理中可用，也可以在其它实施存储系统索引。\n在我看来，正是因为这个原因有了额外好处：使得数据仓库 ETL 更具了组织级的规模。数据仓库的精典问题是数据仓库负责收集和清洗组织中各个组所生成的全部数据。各组织的动机是不同的，数据的生产者并不知晓在数据仓库中数据的使用情况，最终产生的数据很难抽取，或者需要花费规模化的转化才可以转化为可用的形式。当然， 中心团队不可能恰到好处的掌握规模，使得这规模刚好与组织中其它团队相匹配，因此数据的覆盖率常常差别很大，数据流是脆弱的同时变更是缓慢的。\n较好的方法是有一个中心通道，日志和用于增加数据的定义良好的 API。与通道集成的且提供良好的结构化的数据文件的职责依赖于数据的生产者所生成的数据文件。这意味着在设计和实施其它系统时应当考虑数据的输出以及输出的数据如何转化为结构良好的形式并传递给中心通道。增加新的存储系统倒是不必因为数据仓库团队有一个中心结点需要集成而关注数据仓库团队。数据仓库团队仅需处理简单的问题，例如从中心日志中加载结构化的数据，向其它周边系统实施个性化的数据转化等。\n如图所示：当考虑在传统的数据仓库之外增加额外的数据系统时，组织结构的可扩展性显得尤为重要。例如，可以考虑为组织的完整的数据集提供搜索功能。或者提供二级的数据流监控实时数据趋势和告警。无论是这两者中的哪一个，传统的数据仓库架构甚至于 Hadoop 聚簇都不再适用。更糟的是，ETL 的流程通道的目的就是支持数据加载，然而 ETL 似乎无法输出到其它的各个系统，也无法通过引导程序，使得这些外围的系统的各个架构成为适用于数据仓库的重要资产。这就不难解释为什么组织很难轻松的使用它的全部数据。反之，如果组织已建立起了一套标准的、结构良好的数据，那么任何新的系统要使用这些数据仅仅需要与通道进行简单的集成就可以实现。\n这种架构引出了数据清理和转化在哪个阶段进行的不同观点：\n由数据的生产者在把数据增加到公司全局日志之前。 在日志的实时转化阶段进行，这将会产生一个新的转化日志。 在向目标系统加载数据时，做为加载过程的一部分进行。 理想的模形是：由数据的生产者在把数据发布到日志之前对数据进行清理。这样可以确保数据的权威性，不需要维护其它的遗留物例如为数据产生的特殊处理代码或者维护这些数据的其它的存储系统。这些细节应当由产生数据的团队来处理，因为他们最了解他们自己的数据。这个阶段所使用的任何逻辑都应该是无损的和可逆的。\n任何可以实时完成的增值转化类型都应当基于原始日志进行后期处理。这一过程包括了事件数据的会话流程，或者增加大众感兴趣的衍生字段。原始的日志仍然是可用的，但是这种实时处理产生的衍生日志包含了参数数据。\n最终，只有针对目标系统的聚合需要做了加载流程的一部分。它包括了把数据转化成特定的星型或者雪花状模式，从而用于数据仓库的分析和报表。因为在这个阶段，大部分自然的映射到传统的 ETL 流程中，而现在它是在一个更加干净和规整的数据流集在进行的，它将会更加的简单。\nLog Files and Events # 我们再来聊聊这种架构的优势：它支持解耦和事件驱动的系统。\n在网络行业取得活动数据的典型方法是把它记为文本形式的日志，这些文本文件是可分解进入数据仓库或者 Hadoop，用于聚合和查询处理的。由此产生的问题与所有批处理的 ETL 的问题是相同的：它耦合了数据流进入数据仓库系统的能力和流程的调度。\n在 LinkedIn 中，我们已经以中心日志的方式构建了事件数据处理。我们正在使用 Kafka 做为中心的、多订阅者事件日志。我们已经定义了数百种事件类型，每种类型都会捕获用于特定类型动作的独特的属性。这将会覆盖包括页面视图、表达式、搜索以及服务调用、应用异常等方方面面。\n为了进一步理解这一优势：设想一个简单的事务\u0026ndash;在日志页面显示已发布的日志。这个日志页面应当只包括显示日志所需要的逻辑。然而，在相当多的动态站点中，日志页面常常变的添加了很多与显示日志无关的逻辑。例如，我们将对如下的系统进行集成：\n需要把数据传送到 Hadoop 和数据仓库中用于离线数据处理。 需要对视图进行统计，确保视图订阅者不会破坏一些内容片段。 需要聚合这些视图，视图将用于作业发布者的分析页面显示。 需要记录视图以确保我们为作业推荐的使用者提供了恰当的印象覆盖，我们不想一次次的重复同样的事情。 推荐系统需要记录日志用于正确的跟踪作业的普及度。 等等。 不久，简单的作业显示变得相当的复杂。我们增加了作业显示的其它终端\u0026ndash;移动终端应用等\u0026ndash;这些逻辑必须继续存在，复杂度不断的增加。更糟的是我们需要与之做接口交互的系统现在是错综复杂的\u0026ndash;在为显示日作业而工作的工程师们需要知晓多个其它系统和它们的特征，才可以确保它们被正确的集成了。这仅仅是问题的简单版本，真实的的应用系统只会更加的复杂。\n“事件驱动”的模式提供了一种简化这类问题的机制。作业显示页面现在只显示作业并记录与正在显示的作业，作业订阅者相关的其它属性，和其它与作业显示相关的其它有价值的属性。每个与此相关的其它系统诸如推荐系统、安全系统、作业推送分析系统和数据仓库，所有这些只是订阅种子文件，并进行它们的操作。显示代码并不需要关注其它的系统，也不需要因为增加了数据的消费者而相应的进行变更。\nBuilding a Scalable Log # 当然，把发布者与订阅者分离不再是什么新鲜事了。但是如果你想要确保提交日志的行为就像多个订阅者实时的分类日志那样记录网站发生的每件事时，可扩展性就会成为你所面临的首要挑战。如果我们不能创建快速、高性价比和可扩展性灵活的日志以满足实际的可扩展需求，把日志做为统一的集成机制不再是美好的想像，\n人们普遍认为分布式日志是缓慢的、重量经的概念（并且通常会把它仅仅与“原数据”类型的使用联系起来，对于这类使用 Zookeeper 可以适用）。但是深入实现并重点关注分类记录大规模的数据流，这种需求是不切实际的。在 LinkedIn, 我们现在每天通过 Kafka 运行着超过 600 亿个不同的消息写入点(如果 统计镜相与数据中心之间的写入，那么这个数字会是数千亿。)\n我们在 Kafk 中使用了一些小技巧来支持这种可扩展性：\n日志分片 通过批处理读出和写入优化吞吐力 规避无用的数据复制 为了确保水平可扩展性，我们把日志进行切片：\n每个切片都是一篇有序的日志，但是各片之间没有全局的次序（这个有别于你可能包含在消息中的挂钟时间）。把消息分配到特定的日志片段这是由写入者控制的，大部分使用者会通过用户 ID 等键值来进行分片。分片可以把日志追加到不存在协作的片段之间，也可以使系统的吞吐量与 Kafka 聚簇大小成线性比例关系。\n每个分片都是通过可配置数量的复制品复制的，每个复制品都有分片的一份完全一致的拷贝。无论何时，它们中的任一个都可以做为主分片，如果主分片出错了，任何一个复制品都可以接管并做为主分片。\n缺少跨分片的全局顺序是这个机制的局限性，但是我们不认为它有多重要。事实上，与日志的交互主要来源于成百上千个不同的流程，以致于对于它们的行为排一个总体的顺序是没什么意义的。相反，我们可以确保的是我们提供的每个分片都是按顺序保留的。Kafka 保证了追加到由单一发送者送出的特定分片会按照发送的顺序依次处理。\n日志，就像文件系统一样，是容易优化成线性可读可写的样式的。日志可以把小的读入和写出组合成大的、高吞吐量的操作。Kafka 一直至立于实现这一优化目标。批处理可以发生在由客户端向服务器端发送数据、写入磁盘;在服务器各端之间复制；数据传递给消费者和确认提交数据等诸多环节。\n最终，Kafka 使用简单的二进制形式维护内存日志，磁盘日志和网络数据传送。这使得我们可以使用包括 “0 数据复制传送”在内的大量的优化机制。\n这些优化的积累效应是你常常进行的写出和读入数据的操作可以在磁盘和网络上得到支持，甚至于维护内存以外的大量数据集。 这些详细记述并不意味着这是关于 Kafka 的主要内容，那么我就不需要了解细节了。你可在 这个链接阅读到更多的关于 LinkedIn 的方法，和 这个链接是关于 Kafka 的设计总述。\nPart Three: Logs \u0026amp; Real-time Stream Processing # 到此为止，我只是描述从端到端数据复制的理想机制。但是在存储系统中搬运字节不是所要讲述内容的全部。最终我们发现日志是流的另一种说法，日志是 流处理的核心。\n但是，等等，什么是流处理呢？\n如果你是 90 年代晚期或者 21 世纪初 数据库文化或者 数据基础架构产品的爱好者，那么你就可能会把流处理与建创 SQL 引擎或者创建“箱子和箭头”接口用于事件驱动的处理等联系起来。\n如果你关注开源数据库系统的大量出现，你就可能把流处理和一些开源数据库系统关联起来，这些系统包括了： Storm,Akka,S4和 Samza.但是大部分人会把这些系统作为异步消息处理系统，这些系统与支持群集的远程过程调用层的应用没什么差别（而事实上在开源数据库系统领域某些方面确实如此）。\n这些视图都有一些局限性。流处理与 SQL 是无关的。它也局限于实时流处理。不存在内在的原因限制你不能处理昨天的或者一个月之前的流数据，且使用多种不同的语言表达计算。\n我把流处理视为更广泛的概念：持续数据流处理的基础架构。我认为计算模型可以像 MapReduce 或者分布式处理架构一样普遍，但是有能力处理低时延的结果。\n处理模型的实时驱动是数据收集方法。成批收集的数据是分批处理的。数据是不断收集的，它也是按顺序不断处理的。\n美国的统计调查就是成批收集数据的良好典范。统计调查周期性的开展，通过挨门挨户的走访，使用蛮力发现和统计美国的公民信息。1790 年统计调查刚刚开始时这种方式是奏效的。那时的数据收集是批处理的，它包括了骑着马悠闲的行进，把信息写在纸上，然后把成批的记录传送到人们统计数据的中心站点。现在，在描述这个统计过程时，人们立即会想到为什么我们不保留出生和死亡的记录，这样就可以产生人口统计信息这些信息或是持续的或者是其它维度的。\n这是一个极端的例子，但是大量的数据传送处理仍然依赖于周期性的转储，批量转化和集成。处理大容量转储的唯一方法就是批量的处理。但是随着这些批处理被持续的供给所取代，人们自然而然的开始不间断的处理以平滑的处理所需资源并且消除延迟。\n例如 LinkedIn 几乎没有批量数据收集。大部分的数据或者是活动数据或者是数据库变更，这两者都是不间断发生的。事实上，你可以想到的任何商业，正如：Jack Bauer 告诉我们的，低层的机制都是实时发生的不间断的流程事件。数据是成批收集的，它总是会依赖于一些人为的步骤，或者缺少数字化或者是一些自动化的非数字化流程处理的遗留信息。当传送和处理这些数据的机制是邮件或者人工的处理时，这一过程是非常缓慢的。首轮自动化总是保持着最初的处理形式，它常常会持续相当长的时间。\n每天运行的批量处理作业常常是模拟了一种一天的窗口大小的不间断计算。当然，低层的数据也经常变化。在 LinkedIn,这些是司空见贯的，并且使得它们在 Hadoop 运转的机制是有技巧的，所以我们实施了一整套管理增量的 Hadoop 工作流的架构。\n由此看来，对于流处理可以有不同的观点。流处理包括了在底层数据处理的时间概念，它不需要数据的静态快照，它可以产生用户可控频率的输出，而不用等待数据集的全部到达。从这个角度上讲，流处理就是广义上的批处理，随着实时数据的流行，会儿更加普遍。\n这就是为什么从传统的视角看来流处理是利基应用。我个人认为最大的原因是缺少实时数据收集使得不间断的处理成为了学术性的概念。\n我想缺少实时数据收集就像是商用流处理系统注定的命运。他们的客户仍然需要处理面向文件的、每日批量处理 ETL 和数据集成。公司建设流处理系统关注的是提供附着在实时数据流的处理引擎，但是最终当时极少数人真正使用了实时数据流。事实上，在我在 LinkedIn 工作的初期，有一家公司试图把一个非常棒的流处理系统销售给我们，但是因为当时我们的全部数据都按小时收集在的文件里，当时我们提出的最好的应用就是在每小时的最后把这些文件输入到流处理系统中。他们注意到这是一个普遍性的问题。这些异常证明了如下规则：流处理系统要满足的重要商业目标之一是：财务， 它是实时数据流已具备的基准，并且流处理已经成为了瓶颈。\n甚至于在一个健康的批处理系统中，流处理作为一种基础架构的实际应用能力是相当广泛的。它跨越了实时数据请求-应答服务和离线批量处理之间的鸿沟。现在的互联网公司，大约 25%的代码可以划分到这个类型中。\n最终这些日志解决了流处理中绝大部分关键的技术问题。在我看来，它所解决的最大的问题是它使得多订阅者可以获得实时数据。对这些技术细节感兴趣的朋友，我们可以用开源的 Samza,它是基于这些理念建设的一个流处理系统。这些应用的更多技术细节我们在 此文档中有详细的描述。\nData flow graphs # 流处理最有趣的角度是它与流处理系统内部无关，但是与之密切相关的是如何扩展了我们谈到的早期数据集成的数据获取的理念。我们主要讨论了基础数据的获取或日志\u0026ndash;事件和各类系统执行中产生的数据等。但是流处理允许我们包括了计算其它数据的数据。这些衍生的数据在消费者看来与他们计算的原始数据没什么差别。这些衍生的数据可以按任意的复杂度进行压缩。\n让我们再深入一步。我们的目标是：流处理作业可以读取任意的日志并把日志写入到日志或者其它的系统中。他们用于输入输出的日志把这些处理关联到一组处理过程中。事实上，使用这种样式的集中日志，你可以把组织全部的数据抓取、转化和工作流看成是一系列的日志和写入它们的处理过程。\n流处理器根本不需要理想的框架：它可能是读写日志的任何处理器或者处理器集合，但是额外的基础设施和辅助可以提供帮助管理处理代码。\n日志集成的目标是双重的：\n首先，它确保每个数据集都有多个订阅者和有序的。让我们回顾一下状态复制原则来记住顺序的重要性。为了使这个更加具体，设想一下从数据库中更新数据流\u0026ndash;如果在处理过程中我们把对同一记录的两次更新重新排序，可能会产生错误的输出。 TCP 之类的链接仅仅局限于单一的点对点链接，这一顺序的持久性要优于 TCP 之类的链接，它可以在流程处理失败和重连时仍然存在。\n第二，日志提供了流程的缓冲。这是非常基础的。如果处理流程是非同步的，那么上行生成流数据的作业比下行消费流数据的作业运行的更快。这将会导致处理流程阻塞，或者缓冲数据，或者丢弃数据。丢弃数据并不是可行的方法，阻塞将会导致整个流程图立即停止。 日志实际上是一个非常大的缓冲，它允许流程重启或者停止但不会影响流程图其它部分的处理速度。如果要把数据流扩展到更大规模的组织，如果处理作业是由多个不同的团队提供的，这种隔离性是极其重的。我们不能容忍一个错误的作业引发后台的压力，这种压力会使得整个处理流程停止。\nStorm和 Sama这两者都是按非同步方式设计的，可以使用 Kafka 或者其它类似的系统作为它们的日志。\nStateful Real-Time Processing # 一些实时流处理在转化时是无状态的记录。在流处理中大部分的应用会是相当复杂的统计、聚合、不同窗口之间的关联。例如有时人们想扩大包含用户操作信息的事件流（一系列的单击动作）\u0026ndash;实际上关联了用户的单击动作流与用户的账户信息数据库。不变的是这类流程最终会需要由处理器维护的一些状态信息。例如数据统计时，你需要统计到目前为止需要维护的计数器。如果处理器本身失败了，如何正确的维护这些状态信息呢？\n最简单的替换方案是把这些状态信息保存在内存中。但是如果流程崩溃，它就会丢失中间状态。如果状态是按窗口维护的，流程就会回退到日志中窗口开始的时间点上。但是，如果统计是按小时进行的，那么这种方式就会变得不可行。\n另一个替换方案是简单的存储所有的状态信息到远程的存储系统，通过网络与这些存储关联起来。这种机制的问题是没有本地数据和大量的网络间通信。\n我们如何支持处理过程可以像表一样分区的数据呢?\n回顾一下关于表和日志二相性的讨论。这一机制提供了工具把数据流转化为与处理过程协同定位的表，同时也提供了这些表的容错处理的机制。\n流处理器可以把它的状态保存在本地的表或索引\u0026ndash; bdb,或者 leveldb,甚至于类似于 Lucene 或 fastbit 一样不常见的索引。这些内容存储在它的输入流中（或许是使用任意的转化）。生成的变更日志记录了本地的索引，它允许存储事件崩溃、重启等的状态信息。流处理提供了通用的机制用于在本地输入流数据的随机索引中保存共同分片的状态。\n当流程运行失败时，它会从变更日志中恢复它的索引。每次备份时，日志把本地状态转化成一系列的增量记录。\n这种状态管理的方法有一个优势是把处理器的状态也做为日志进行维护。我们可以把这些日志看成与数据库表相对应的变更日志。事实上，这些处理器同时维护着像共同分片表一样的表。因为这些状态它本身就是日志，其它的处理器可以订阅它。如果流程处理的目标是更新结点的最后状态，这种状态又是流程的输出，那么这种方法就显得尤为重要。\n为了数据集成，与来自数据库的日志关联，日志和数据库表的二象性就更加清晰了。变更日志可以从数据库中抽取出来，日志可以由不同的流处理器（流处理器用于关联不同的事件流）按不同的方式进行索引。\n我们可以列举在 Samza 中有状态流处理管理的更多细节和 大量实用的例子。\nLog Compaction # 当然，我们不能奢望保存全部变更的完整日志。除非想要使用无限空间，日志不可能完全清除。为了澄清它，我们再来聊聊 Kafka 的实现。在 Kafka 中,清理有两种选择，这取决于数据是否包括关键更新和事件数据。对于事件数据，Kafka 支持仅维护一个窗口的数据。通常，配置需要一些时间，窗口可以按时间或空间定义。虽然对于关键数据而言，完整日志的重要特征是你可以重现源系统的状态信息，或者在其它的系统重现。\n随着时间的推移，保持完整的日志会使用越来越多的空间，重现所耗费的时间越来越长。因些在 Kafka 中,我们支持不同类型的保留。我们移除了废弃的记录(这些记录的主键最近更新过)而不是简单的丢弃旧日志。我们仍然保证日志包含了源系统的完整备份，但是现在我们不再重现原系统的全部状态，而是仅仅重现最近的状态。我们把这一特征称为 日志压缩。\nPart Four: System Building # 我们最后要讨论的是在线数据系统设计中日志的角色。\n在分布式数据库数据流中日志的角色和在大型组织机构数据完整中日志的角色是相似的。在这两个应用场景中，日志是对于数据源是可靠的，一致的和可恢复的。组织如果不是一个复杂的分布式数据系统呢，它究竟是什么？\n分类计价吗？(Unbundling) # 如果换个角度，你可以看到把整个组织系统和数据流看做是单一的分布式数据系统。你可以把所有的子查询系统（诸如 Redis, SOLR,Hive 表等）看成是数据的特定索引。你可以把 Storm 或 Samza 一样的流处理系统看成是发展良好的触发器和视图具体化机制。我已经注意到，传统的数据库管理人员非常喜欢这样的视图，因为它最终解释了这些不同的数据系统到底是做什么用的\u0026ndash;它们只是不同的索引类型而已。\n不可否认这类数据库系统现在大量的出现，但是事实上，这种复杂性一直都存在。即使是在关系数据库系统的鼎盛时期，组织中有大量的关系数据库系统。或许自大型机时代开始，所有的数据都存储在相同的位置，真正的集成是根本不存在的。存在多种外在需求，需要把数据分解成多个系统，这些外在需求包括：规模、地理因素、安全性，性能隔离是最常见的因素。这些需求都可以由一个优质的系统实现：例如，组织可以使用单一的 Hadoop 聚簇，它包括了全部的数据，可以服务于大型的和多样性的客户。\n因此在向分布式系统变迁的过程中，已经存在一种处理数据的简便的方法：把大量的不同系统的小的实例聚合成为大的聚簇。许多的系统还不足以支持这一方法：因为它们不够安全，或者性能隔离性得不到保证，或者规模不符合要求。不过这些问题都是可以解决的。\n依我之见，不同系统大量出现的原因是建设分布式数据库系统很困难。通过削减到单一的查询或者用例，每个系统都可以把规模控制到易于实现的程度。但是运行这些系统产生的复杂度依然很高。\n未来这类问题可能的发展趋势有三种：\n第一种可能是保持现状：孤立的系统还会或长或短的持续一段时间。这是因为建设分布式系统的困难很难克服，或者因为孤立系统的独特性和便捷性很难达到。基于这些原因，数据集成的核心问题仍然是如何恰当的使用数据。因此，集成数据的外部日志非常的重要。\n第二种可能是重构：具备通用性的单一的系统逐步融合多个功能形成超极系统。这个超级系统表面看起来类似关系数据库系统，但是在组织中你使用时最大的不同是你只需要一个大的系统而不是无数个小系统。在这个世界里，除了在系统内已解决的这个问题不存在什么真正的数据集成问题。我想这是因为建设这样的系统的实际困难。\n虽然另一种可能的结果对于工程师来说是很有吸引力的。新一代数据库系统的特征之一是它们是完全开源的。开源提供了一种可能性：数据基础架构不必打包成服务集或者面向应用的系统接口。在 Java 栈中，你可以看到在一定程度上，这种状况已经发生了。\nZookeeper用于处理多个系统之间的协调，或许会从诸如 Helix 或者 Curator等高级别的抽象中得到一些帮助。 Mesos和 YARN用于处理流程可视化和资源管理。 Lucene和 LevelDB等嵌入式类库做为索引。 Netty, Jetty和 Finagle, rest.li等封装成高级别的用于处理远程通信。 Avro, Protocol Buffers, Thrift和 umpteen zillion等其它类库用于处理序列化。 Kafka和 Bookeeper提供支持日志。 如果你把这些堆放在一起，换个角度看，它有点像是简化版的分布式数据库系统工程。你可以把这些拼装在一起，创建大量的可能的系统。显而易见，现在探讨的不是最终用户所关心的 API 或者如何实现，而是在不断多样化和模块化的过程中如何设计实现单一系统的途径。因为随着可靠的、灵活的模块的出现，实施分布式系统的时间周期由年缩减为周，聚合形成大型整体系统的压力逐步消失。\nThe place of the log in system architecture # 那些提供外部日志的系统如今已允许个人电脑抛弃他们自身复杂的日志系统转而使用共享日志。在我看来，日志可以做到以下事情：\n通过对节点的并发更新的排序处理数据的一致性（无论在及时还是最终情况下） 提供节点之间的数据复制 提供”commit“语法（只有当写入器确保数据不会丢失时才会写入） 位系统提供外部的数据订阅资源 提供存储失败的复制操作和引导新的复制操作的能力 处理节点间的数据平衡 这实际上是一个数据分发系统最重要的部分，剩下的大部分内容与终端调用的 API 和索引策略相关。这正是不同系统间的差异所在，例如：一个全文本查询语句需要查询所有的分区，而一个主键查询只需要查询负责键数据的单个节点就可以了。\n下面我们来看下该系统是如何工作的。系统被分为两个逻辑区域：日志和服务层。日志按顺序捕获状态变化，服务节点存储索引提供查询服务需要的所有信息（键-值的存储可能以 B-tree 或 SSTable 的方式进行，而搜索系统可能存在与之相反的索引）。写入器可以直接访问日志，尽管需要通过服务层代理。在写入日志的时候会产生逻辑时间戳（即 log 中的索引），如果系统是分段式的，那么就会产生与段数目相同数量的日志文件和服务节点，这里的数量和机器数量可能会有较大差距。\n服务节点订阅日志信息并将写入器按照日志存储的顺序尽快应用到它的本地索引上。\n客户端只要在查询语句中提供对应的写入器的时间戳，它就可以从任何节点中获取”读写“语义。服务节点收到该查询语句后会将其中的时间戳与自身的索引比较，如果必要，服务节点会延迟请求直到对应时间的索引建立完毕，以免提供旧数据。\n服务节点或许根本无需知道”控制“或”lerder 选举（leader election）“的概念，对很多简单的操作，服务节点可以爱完全脱离领导的情况下提供服务，日志即是信息的来源。\n分发系统所需要做的其中一个比较复杂的工作，就是修复失败节点并移除几点之间的隔离。保留修复的数据并结合上各区域内的数据快照是一种较为典型的做法，它与保留完整的数据备份并从 垃圾箱内回收日志的做法几乎等价。这就使得服务层简单了很多，日志系统也更有针对性。\n有了这个日志系统，你可以订阅到API，这个API提供了把ETL提供给其它系统的数据内容。事实上，许多系统都可以共享相同的日志同时提供不同的索引，如下所示：\n这个系统的视图可以清晰的分解到日志和查询API,因为它允许你从系统的可用性和一致性角度分解查询的特征。这可以帮助我们对系统进行分解，并理解那些并没按这种方式设计实施的系统。\n虽然Kafka和Bookeeper都是一致性日志，但这不是必须的，也没什么意义。你可以轻松的把Dynamo之类的数据构分解为一致性的AP日志和键值对服务层。这样的日志使用起来灵活，因为它重传了旧消息，像Dynamo一样，这样的处理取决于消息的订阅者。\n在很多人看来，在日志中另外保存一份数据的完整复本是一种浪费。事实上，虽然有很多因素使得这件事并不困难。首先，日志可以是一种有效的存储机制。我们在Kafka生产环境的服务器上存储了5 TB的数据。同时有许多的服务系统需要更多的内存来提供有效的数据服务，例如文本搜索，它通常是在内存中的。服务系统同样也需样硬盘的优化。例如，我们的实时数据系统或者在内存外提供服务或者使用固态硬盘。相反，日志系统只需要线性的读写，因此，它很乐于使用TB量级的硬盘。最终，如上图所示，由多个系统提供的数据，日志的成本分摊到多个索引上，这种聚合使得外部日志的成本降到了最低点。\nLinkedIn就是使用了这种方式实现它的多个实时查询系统的。这些系统提供了一个数据库（使用数据总线做为日志摘要，或者从Kafka去掉专用的日志），这些系统在顶层数据流上还提供了特殊的分片、索引和查询功能。这也是我们实施搜索、社交网络和OLAP查询系统的方式。事实上这种方式是相当普遍的：为多个用于实时服务的服务系统提供单一的数据（这些来自Hadoop的数据或是实时的或是衍生的）。这种方式已被证实是相当简洁的。这些系统根本不需要外部可写入的API，Kafka和数据库被用做系统的记录和变更流，通过日志你可以查询系统。持有特定分片的结点在本地完成写操作。这些结点盲目的把日志提供的数据转录到它们自己的存储空间中。通过回放上行流日志可以恢复转录失败的结点。\n这些系统的程度则取决于日志的多样性。一个完全可靠的系统可以用日志来对数据分片、存储结点、均衡负载，以及用于数据一致性和数据复制等多方面。在这一过程中，服务层实际上只不过是一种缓存机制，这种缓存机制允许直接写入日志的流处理。\n结束语 # 如果你对于本文中所谈到的关于日志的大部内容，如下内容是您可以参考的其它资料。对于同一事务人们会用不同的术语，这会让人有一些困惑，从数据库系统到分布式系统，从各类企业级应用软件到广阔的开源世界。无论如何，在大方向上还是有一些共同之处。\nreferences # 参考翻译 部分自己翻译，大段的抄这里的翻译～ 参考文献见此 "},{"id":25,"href":"/posts/db/newdb/","title":"Newdb","section":"Blog","content":"文章简介：简单介绍最近自己最近设计实现 newdb 的工作\n项目地址 anydemo/newdb\n最近一段时间花了很大的力气学习数据库的知识，设计实现了简单的 RDBMS, 收获比较丰富\n开始先从数据库底层的数据持久化开始。再此设计的数据库主要是固定类型大小的关系型数据库。一个 table 中有固定大小的一定数量的数据列，每一个 tuple 的大小是固定不变的，也就是每一个 page 的可以容纳的 tuple 是一定的，这样很容易在记录某一个 tuple 的位置、以及其在文件中的位置。在这过程中收获最大的部分就是对于 Marshal 于 Unmarshal 的理解更进一步，知道数据如何在数据库中是如何存储的，以及事务的本质。\n最后通过一个 Sequences Scan 的例子，手写一个物理执行计划，执行一次全表扫描。\n未来还有很多的内容可以做，暂时告一段落，最近抽时间沉淀一下，思考一下未来，回头有时间了再回头继续加新的内容。毕竟 issues积攒了太多。\n"},{"id":26,"href":"/posts/Go/rpc-ipc/","title":"Go 启动多个程序，及 IPC 和 RPC 交互例子,以及 Gracefully Shutdown","section":"Blog","content":"文章简介：Go 程序启动 RPC 子进程，通过 pipe 进行交互，以及通过 RPC 交互\n代码见 exfly/go-ipc\n引言 # 为什么要写这篇文章。最近看了一些 Docker 源码。dockerd 的架构长 这个样子。 一条 docker 命令的执行，比如docker run，是先由 containerd 执行，containerd 也同样不是真正运行容器，他会将执行请求发给 runc，有 runc 真正去执行。dockerd、containderd、runc 分别是三个可执行文件，他们是通过 管道（IPC）以及 rest、RPC 进行交互的。\n为了展示三者交互方式是如何进行的，这里写一个简单的 demo 来解释。\n正文 # RPC # 首先说一下 RPC。Go 标准库便有net/rpc，写一个 Go 的 rpc 很简单：\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;net/rpc\u0026#34; ) type Task []string type Todo {ID string} func (t Task) Get(id string, reply *Todo) error { *reply=Todo{ID:id} return nil } func main() { task := new(Task) // Publish the receivers methods err := rpc.Register(task) if err != nil { log.Fatal(\u0026#34;Format of service Task isn\u0026#39;t correct. \u0026#34;, err) } // Register a HTTP handler rpc.HandleHTTP() listener, e := net.Listen(\u0026#34;unix\u0026#34;, \u0026#34;rpc.sock\u0026#34;) if e != nil { log.Fatal(\u0026#34;Listen error: \u0026#34;, e) } err = http.Serve(listener, nil) if err != nil { log.Fatal(\u0026#34;Error serving: \u0026#34;, err) } } // client client, err := rpc.DialHTTP(\u0026#34;unix\u0026#34;, \u0026#34;rpc.sock\u0026#34;) reply := TODO{} err = client.Call(\u0026#34;Task.Get\u0026#34;, \u0026#34;new_id\u0026#34;, \u0026amp;reply) 如何进行进程间通信 IPC 呢 # 首先 linux 下的 FIFO（有名管道）是什么样子的\nmkfifo tpipe ll # total 8 # drwx------ 2 vagrant vagrant 4096 May 18 17:28 ./ # drwxrwxrwt 10 root root 4096 May 18 17:28 ../ # prw-rw-r-- 1 vagrant vagrant 0 May 18 17:28 tpipe| # 现在一个terminal cat tpipe # 另一个terminal echo tttttttttt \u0026gt; tpipe 此时第一个 terminal 会输出 tttttttttt，另一个命令行会返回\n在 Go 中应该如何使用\nrpcSvrCmd := exec.Command(conf.RPCSvrBinPath) rpcSvrStdinPipe, err := rpcSvrCmd.StdinPipe() rpcSvrStdoutPipe, err := rpcSvrCmd.StdoutPipe() rpcSvrStderrPipe, err := rpcSvrCmd.StderrPipe() rpcSvrCmd.Start() 如此即可获得子进程的各种 FIFO\n完整例子 # 代码见 exfly/go-ipc\nmake task rpc \u0026amp;\u0026amp; ./bin/task # ▶ running gofmt… # ▶ running golint… # ▶ building executable… # ▶ building executable… # 2019/05/19 01:33:12 sleep 1s to wait rpc server startup # 2019/05/19 01:33:13 2019/05/19 01:33:12 Serving RPC server on {unix rpc.sock} # 2019/05/19 01:33:13 Finish App: {Finish App Started} # 2019/05/19 01:33:13 2019/05/19 01:33:13 stop the server # 2019/05/19 01:33:13 2019/05/19 01:33:13 deleted socket file: rpc.sock # 2019/05/19 01:33:13 pipe rpc_srv_stderr has Closed # 2019/05/19 01:33:13 pipe rpc_srv_stdout has Closed # 2019/05/19 01:33:13 stop subproc ./bin/task-rpc success 当前更新主要以发布代码为主，没有详细解释，详细见代码 exfly/go-ipc\n"},{"id":27,"href":"/posts/docker/docker-volume/","title":"Docker 源码阅读: Docker Volume","section":"Blog","content":"文章简介：描述 docker volume 的原理\ndocker 存储基于 UnionFS 实现，所有容器存储是将多个 layout 层通过类似 aufs 或者 overlay2 将多个层联合到一起，mount 成新的目录，在通过 namespaces 将新的文件目录 chroot 进入新启动的进程，达到文件隔离的目的。\n对于读请求，由于写实复制技术，会直接读取底层文件 对于写请求，会先将文件复制到读写层，然后进行修改文件 对于删除，没有真正删除文件，只是讲文件标记删除，没有真正删除下层文件。 命令 # docker volume create volume-for-test可以创建新的 volume\ndocker volume ls 查看新创建的 volume\ndocker volume inspect volume-for-test\ndocker run -d --name devtest --mount source=volume-for-test,target=/app alpine /bin/sh\ndocker inspect \u0026lt;container_id\u0026gt;| grap Mounts 可以看到刚刚 mount 进去的 volume /var/lib/docker/volumes/volume-for-test/_data\nrelated links # Manage data in Docker Use volumes docker 存储基础 "},{"id":28,"href":"/posts/docker/docker-build/","title":"Docker 源码阅读: Docker Build","section":"Blog","content":"文章简介：docker build 源码阅读\n阅读源码顺序 # api/server/router/build/build_routes.go#postBuild api/server/backend/build/backend.go#Build builder/builder-next/builder.go#Build builder/dockerfile/builder.go#Build builder/dockerfile/builder.go#dispatchDockerfileWithCancellation builder/dockerfile/evaluator.go#dispatch，在这里，有所有命令的执行方式，可以仔细研究一下 在dispatch中有 dockerfile 支持的指令，如RUN等。以 RUN 为例，dockerd 会读取 CLI 发来的 dockerfile，解析后。如果为 RUN，则会启动一个新的 container,然后在容器中执行，执行结束后将当前层 commit，继续执行下一个指令.\n其他 # 从 v18.09开始，docker build 依赖于 buildkit "},{"id":29,"href":"/posts/docker/docker-run/","title":"Docker 源码阅读: Docker Run","section":"Blog","content":"文章简介：通过分析 docker run 命令，理解 Docker 的工作原理\ndocker run执行流程分析 # 整体执行流程: 从本地镜像中寻找是否存在命令指定镜像，如果存在则正常返回，执行接下来流程。如果不存在镜像，deamon 返回错误，由 cli 重新发起 pull 请求，之后重试。\n总体描述 # createContainer cli 通过 rest 接口请求 dockerd 创建容器，dockerd 设置基础配置后记录创建的容器 ContainerStart cli 通过 rest 接口请求 dockerd 运行容器，dockerd 使用 rpc 链接 containnerd 进行运行容器 代码 lanything/code-read forked moby/moby, lanything/cli forked docker/cli\ncreateContainer # docker/cli cli/command/container/run.go#NewRunCommand docker/cli cli/command/container/run.go#runContainer // cli/command/container/create.go#createContainer func createContainer(ctx context.Context, dockerCli command.Cli, containerConfig *containerConfig, opts *createOptions) (*container.ContainerCreateCreatedBody, error) { config := containerConfig.Config hostConfig := containerConfig.HostConfig networkingConfig := containerConfig.NetworkingConfig stderr := dockerCli.Err() warnOnOomKillDisable(*hostConfig, stderr) warnOnLocalhostDNS(*hostConfig, stderr) var ( trustedRef reference.Canonical namedRef reference.Named ) containerIDFile, err := newCIDFile(hostConfig.ContainerIDFile) if err != nil { return nil, err } defer containerIDFile.Close() ref, err := reference.ParseAnyReference(config.Image) if err != nil { return nil, err } if named, ok := ref.(reference.Named); ok { namedRef = reference.TagNameOnly(named) if taggedRef, ok := namedRef.(reference.NamedTagged); ok \u0026amp;\u0026amp; !opts.untrusted { var err error trustedRef, err = image.TrustedReference(ctx, dockerCli, taggedRef, nil) if err != nil { return nil, err } config.Image = reference.FamiliarString(trustedRef) } } //create the container response, err := dockerCli.Client().ContainerCreate(ctx, config, hostConfig, networkingConfig, opts.name) //if image not found try to pull it if err != nil { if apiclient.IsErrNotFound(err) \u0026amp;\u0026amp; namedRef != nil { fmt.Fprintf(stderr, \u0026#34;Unable to find image \u0026#39;%s\u0026#39; locally\\n\u0026#34;, reference.FamiliarString(namedRef)) // we don\u0026#39;t want to write to stdout anything apart from container.ID if err := pullImage(ctx, dockerCli, config.Image, opts.platform, stderr); err != nil { return nil, err } if taggedRef, ok := namedRef.(reference.NamedTagged); ok \u0026amp;\u0026amp; trustedRef != nil { if err := image.TagTrusted(ctx, dockerCli, trustedRef, taggedRef); err != nil { return nil, err } } // Retry var retryErr error response, retryErr = dockerCli.Client().ContainerCreate(ctx, config, hostConfig, networkingConfig, opts.name) if retryErr != nil { return nil, retryErr } } else { return nil, err } } for _, warning := range response.Warnings { fmt.Fprintf(stderr, \u0026#34;WARNING: %s\\n\u0026#34;, warning) } err = containerIDFile.Write(response.ID) return \u0026amp;response, err } docker/cli cli/command/container/create.go#createContainer 调用 ContainerCreate moby/moby api/server/router/container/container.go#initRoutes.postContainersCreate moby/moby api/server/router/container/container_routes.go#postContainersCreate moby/moby daemon/create.go#ContainerCreate containerCreate # // `moby/moby` daemon/create.go#containerCreate func (daemon *Daemon) containerCreate(opts createOpts) (containertypes.ContainerCreateCreatedBody, error) { start := time.Now() if opts.params.Config == nil { return containertypes.ContainerCreateCreatedBody{}, errdefs.InvalidParameter(errors.New(\u0026#34;Config cannot be empty in order to create a container\u0026#34;)) } os := runtime.GOOS if opts.params.Config.Image != \u0026#34;\u0026#34; { img, err := daemon.imageService.GetImage(opts.params.Config.Image) if err == nil { os = img.OS } } else { // This mean scratch. On Windows, we can safely assume that this is a linux // container. On other platforms, it\u0026#39;s the host OS (which it already is) if runtime.GOOS == \u0026#34;windows\u0026#34; \u0026amp;\u0026amp; system.LCOWSupported() { os = \u0026#34;linux\u0026#34; } } warnings, err := daemon.verifyContainerSettings(os, opts.params.HostConfig, opts.params.Config, false) if err != nil { return containertypes.ContainerCreateCreatedBody{Warnings: warnings}, errdefs.InvalidParameter(err) } err = verifyNetworkingConfig(opts.params.NetworkingConfig) if err != nil { return containertypes.ContainerCreateCreatedBody{Warnings: warnings}, errdefs.InvalidParameter(err) } if opts.params.HostConfig == nil { opts.params.HostConfig = \u0026amp;containertypes.HostConfig{} } err = daemon.adaptContainerSettings(opts.params.HostConfig, opts.params.AdjustCPUShares) if err != nil { return containertypes.ContainerCreateCreatedBody{Warnings: warnings}, errdefs.InvalidParameter(err) } container, err := daemon.create(opts) if err != nil { return containertypes.ContainerCreateCreatedBody{Warnings: warnings}, err } containerActions.WithValues(\u0026#34;create\u0026#34;).UpdateSince(start) if warnings == nil { warnings = make([]string, 0) // Create an empty slice to avoid https://github.com/moby/moby/issues/38222 } return containertypes.ContainerCreateCreatedBody{ID: container.ID, Warnings: warnings}, nil } GetImages # // GetImage returns an image corresponding to the image referred to by refOrID. func (i *ImageService) GetImage(refOrID string) (*image.Image, error) { ref, err := reference.ParseAnyReference(refOrID) if err != nil { return nil, errdefs.InvalidParameter(err) } namedRef, ok := ref.(reference.Named) if !ok { digested, ok := ref.(reference.Digested) if !ok { return nil, ErrImageDoesNotExist{ref} } id := image.IDFromDigest(digested.Digest()) if img, err := i.imageStore.Get(id); err == nil { return img, nil } return nil, ErrImageDoesNotExist{ref} } if digest, err := i.referenceStore.Get(namedRef); err == nil { // Search the image stores to get the operating system, defaulting to host OS. id := image.IDFromDigest(digest) if img, err := i.imageStore.Get(id); err == nil { return img, nil } } // Search based on ID if id, err := i.imageStore.Search(refOrID); err == nil { img, err := i.imageStore.Get(id) if err != nil { return nil, ErrImageDoesNotExist{ref} } return img, nil } return nil, ErrImageDoesNotExist{ref} } ImageService.imageStore 在mobydaemon/daemon.go#NewDaemon 中有设置 ImageService.imageStore 在mobyimage/store.go#NewImageStore 有初始化\nContainerStart # moby daemon/start.go#ContainerStart\nfunc (daemon *Daemon) ContainerStart(name string, hostConfig *containertypes.HostConfig, checkpoint string, checkpointDir string) error { if checkpoint != \u0026#34;\u0026#34; \u0026amp;\u0026amp; !daemon.HasExperimental() { return errdefs.InvalidParameter(errors.New(\u0026#34;checkpoint is only supported in experimental mode\u0026#34;)) } container, err := daemon.GetContainer(name) if err != nil { return err } validateState := func() error { container.Lock() defer container.Unlock() if container.Paused { return errdefs.Conflict(errors.New(\u0026#34;cannot start a paused container, try unpause instead\u0026#34;)) } if container.Running { return containerNotModifiedError{running: true} } if container.RemovalInProgress || container.Dead { return errdefs.Conflict(errors.New(\u0026#34;container is marked for removal and cannot be started\u0026#34;)) } return nil } if err := validateState(); err != nil { return err } // Windows does not have the backwards compatibility issue here. if runtime.GOOS != \u0026#34;windows\u0026#34; { // This is kept for backward compatibility - hostconfig should be passed when // creating a container, not during start. if hostConfig != nil { logrus.Warn(\u0026#34;DEPRECATED: Setting host configuration options when the container starts is deprecated and has been removed in Docker 1.12\u0026#34;) oldNetworkMode := container.HostConfig.NetworkMode if err := daemon.setSecurityOptions(container, hostConfig); err != nil { return errdefs.InvalidParameter(err) } if err := daemon.mergeAndVerifyLogConfig(\u0026amp;hostConfig.LogConfig); err != nil { return errdefs.InvalidParameter(err) } if err := daemon.setHostConfig(container, hostConfig); err != nil { return errdefs.InvalidParameter(err) } newNetworkMode := container.HostConfig.NetworkMode if string(oldNetworkMode) != string(newNetworkMode) { // if user has change the network mode on starting, clean up the // old networks. It is a deprecated feature and has been removed in Docker 1.12 container.NetworkSettings.Networks = nil if err := container.CheckpointTo(daemon.containersReplica); err != nil { return errdefs.System(err) } } container.InitDNSHostConfig() } } else { if hostConfig != nil { return errdefs.InvalidParameter(errors.New(\u0026#34;Supplying a hostconfig on start is not supported. It should be supplied on create\u0026#34;)) } } // check if hostConfig is in line with the current system settings. // It may happen cgroups are umounted or the like. if _, err = daemon.verifyContainerSettings(container.OS, container.HostConfig, nil, false); err != nil { return errdefs.InvalidParameter(err) } // Adapt for old containers in case we have updates in this function and // old containers never have chance to call the new function in create stage. if hostConfig != nil { if err := daemon.adaptContainerSettings(container.HostConfig, false); err != nil { return errdefs.InvalidParameter(err) } } return daemon.containerStart(container, checkpoint, checkpointDir, true) } moby daemon/start.go#containerStart\n// containerStart prepares the container to run by setting up everything the // container needs, such as storage and networking, as well as links // between containers. The container is left waiting for a signal to // begin running. func (daemon *Daemon) containerStart(container *container.Container, checkpoint string, checkpointDir string, resetRestartManager bool) (err error) { start := time.Now() container.Lock() defer container.Unlock() if resetRestartManager \u0026amp;\u0026amp; container.Running { // skip this check if already in restarting step and resetRestartManager==false return nil } if container.RemovalInProgress || container.Dead { return errdefs.Conflict(errors.New(\u0026#34;container is marked for removal and cannot be started\u0026#34;)) } if checkpointDir != \u0026#34;\u0026#34; { // TODO(mlaventure): how would we support that? return errdefs.Forbidden(errors.New(\u0026#34;custom checkpointdir is not supported\u0026#34;)) } // if we encounter an error during start we need to ensure that any other // setup has been cleaned up properly defer func() { if err != nil { container.SetError(err) // if no one else has set it, make sure we don\u0026#39;t leave it at zero if container.ExitCode() == 0 { container.SetExitCode(128) } if err := container.CheckpointTo(daemon.containersReplica); err != nil { logrus.Errorf(\u0026#34;%s: failed saving state on start failure: %v\u0026#34;, container.ID, err) } container.Reset(false) daemon.Cleanup(container) // if containers AutoRemove flag is set, remove it after clean up if container.HostConfig.AutoRemove { container.Unlock() if err := daemon.ContainerRm(container.ID, \u0026amp;types.ContainerRmConfig{ForceRemove: true, RemoveVolume: true}); err != nil { logrus.Errorf(\u0026#34;can\u0026#39;t remove container %s: %v\u0026#34;, container.ID, err) } container.Lock() } } }() if err := daemon.conditionalMountOnStart(container); err != nil { return err } if err := daemon.initializeNetworking(container); err != nil { return err } spec, err := daemon.createSpec(container) if err != nil { return errdefs.System(err) } if resetRestartManager { container.ResetRestartManager(true) container.HasBeenManuallyStopped = false } if daemon.saveApparmorConfig(container); err != nil { return err } if checkpoint != \u0026#34;\u0026#34; { checkpointDir, err = getCheckpointDir(checkpointDir, checkpoint, container.Name, container.ID, container.CheckpointDir(), false) if err != nil { return err } } createOptions, err := daemon.getLibcontainerdCreateOptions(container) if err != nil { return err } ctx := context.TODO() err = daemon.containerd.Create(ctx, container.ID, spec, createOptions) // daemon/daemon.go#NewDaemon#L1043 libcontainerd.NewClient if err != nil { if errdefs.IsConflict(err) { logrus.WithError(err).WithField(\u0026#34;container\u0026#34;, container.ID).Error(\u0026#34;Container not cleaned up from containerd from previous run\u0026#34;) // best effort to clean up old container object daemon.containerd.DeleteTask(ctx, container.ID) if err := daemon.containerd.Delete(ctx, container.ID); err != nil \u0026amp;\u0026amp; !errdefs.IsNotFound(err) { logrus.WithError(err).WithField(\u0026#34;container\u0026#34;, container.ID).Error(\u0026#34;Error cleaning up stale containerd container object\u0026#34;) } err = daemon.containerd.Create(ctx, container.ID, spec, createOptions) } if err != nil { return translateContainerdStartErr(container.Path, container.SetExitCode, err) } } // TODO(mlaventure): we need to specify checkpoint options here pid, err := daemon.containerd.Start(context.Background(), container.ID, checkpointDir, container.StreamConfig.Stdin() != nil || container.Config.Tty, container.InitializeStdio) if err != nil { if err := daemon.containerd.Delete(context.Background(), container.ID); err != nil { logrus.WithError(err).WithField(\u0026#34;container\u0026#34;, container.ID). Error(\u0026#34;failed to delete failed start container\u0026#34;) } return translateContainerdStartErr(container.Path, container.SetExitCode, err) } container.SetRunning(pid, true) container.HasBeenStartedBefore = true daemon.setStateCounter(container) daemon.initHealthMonitor(container) if err := container.CheckpointTo(daemon.containersReplica); err != nil { logrus.WithError(err).WithField(\u0026#34;container\u0026#34;, container.ID). Errorf(\u0026#34;failed to store container\u0026#34;) } daemon.LogContainerEvent(container, \u0026#34;start\u0026#34;) containerActions.WithValues(\u0026#34;start\u0026#34;).UpdateSince(start) return nil } 上边这段代码到 err = daemon.containerd.Create(ctx, container.ID, spec, createOptions) // daemon/daemon.go#NewDaemon#L1043 libcontainerd.NewClient 这里会比较麻烦，需要到libcontainerd/libcontainerd_linux.go#NewClient寻找源码\n// libcontainerd/remote/client.go#Creat( func (c *client) Create(ctx context.Context, id string, ociSpec *specs.Spec, runtimeOptions interface{}) error { bdir := c.bundleDir(id) c.logger.WithField(\u0026#34;bundle\u0026#34;, bdir).WithField(\u0026#34;root\u0026#34;, ociSpec.Root.Path).Debug(\u0026#34;bundle dir created\u0026#34;) _, err := c.client.NewContainer(ctx, id, containerd.WithSpec(ociSpec), containerd.WithRuntime(runtimeName, runtimeOptions), WithBundle(bdir, ociSpec), ) if err != nil { if containerderrors.IsAlreadyExists(err) { return errors.WithStack(errdefs.Conflict(errors.New(\u0026#34;id already in use\u0026#34;))) } return wrapError(err) } return nil } 技巧 # 在这过程中，比较方便的寻找代码实现方法是 比如 ContainerExecStart 是一个接口中的方法，希望找到她的实现可以通过比较简单的搜索注释寻找到方法：比如搜索 //ContainerExecStart\n"},{"id":30,"href":"/posts/docker/docker-source-dev-install/","title":"Docker 源码阅读: 开发环境搭建","section":"Blog","content":"文章简介：介绍 docker 开发环境搭建\ndocker 官方的贡献引导: moby/docs/contributing/README.md\n步骤 # moby/docs/contributing/set-up-dev-env.md完整的描述了开发环境如何搭建\n开发环境搭建 # 因为网络问题，需要尽快的提速安装 dev 环境，需要配置 Dockerfile 中的配置 APT_MIRROR=mirrors.163.com\n运行make --just-print BIND_DIR=. shell简单的看一下 make 都做了那些事情 运行make BIND_DIR=. shell开始安装开发环境\n编译 # 上一节已经进入 docker 中，可以开始编译 dockerd\nhack/make.sh binary make installcopy binary to container\u0026rsquo;s /usr/local/bin/ dockerd -D \u0026amp; 日常工作流 # 修改代码 hack/make.sh binary install-binary 常用目录/命令 # docker inspect 查看镜像或容器运行配置信息，GraphDriver 为容器挂载信息 /var/lib/docker/overlay2/\u0026lt;image_id\u0026gt;/{merged, diff, work} /var/lib/docker/containers当前运行中的容器配置信息 /var/lib/docker/image镜像库 /var/lib/docker/volumesvolumes 储存位置 调试 # 调试 makefile # make --just-print BIND_DIR=. shell\n调试 shell # bash -x hack/make.sh binary\n"},{"id":31,"href":"/posts/docker/docker-architecture/","title":"Docker 源码阅读: Docker架构介绍","section":"Blog","content":"文章简介：docker基本架构，以及在 moby 开源项目相关的一些组件之间如何协同工作的\nDocker? # 我们可以使用 Docker 做什么 # 快速，一致地交付您的应用程序\nDocker 允许开发人员使用提供应用程序和服务的本地容器在标准化环境中工作，从而简化了开发生命周期。 容器非常适合持续集成和持续交付（CI / CD）工作流程。\ndocker 全局架构 # CLI 使用 Docker REST API 通过脚本或直接 CLI 命令控制 Docker 守护程序或与 Docker 守护程序交互。 许多其他 Docker 应用程序使用底层 API 和 CLI。\nDocker daemon 创建和管理 Docker 对象，例如 images，containers，networks 和 volumes。\nThe Docker daemon # Docker daemon（dockerd）监听 Docker API 请求并管理 Docker 对象，例如 images，containers，networks 和 volumes。 Docker daemon 还可以与其他守护程序通信以管理 Docker 服务。\nNote: 例如 dockerd 通过 rest 接口与 containnerd 进行交互，containerd 与 runc 通过 grpc 进行通信\nThe Docker client # Docker 客户端（docker）是许多 Docker 用户与 Docker 交互的主要方式。 当您使用诸如 docker run 之类的命令时，客户端会将这些命令发送到 dockerd，dockerd 将其执行。 docker (CLI) 命令调用 REST 风格的 Docker API。 Docker 客户端可以与多个守护进程通信。\nNote: docker 可以通过配置环境变量 DOCKER_HOST或者修改配置变量，或者命令行参数的方式连接 dockerd\nDocker registries # Docker registry 存储 Docker 镜像。 Docker Hub 是任何人都可以使用的公共注册中心，Docker 配置为默认在 Docker Hub 上查找 images。 您甚至可以运行自己的私人 registry。 如果您使用 Docker Datacenter（DDC），它包含 Docker Trusted Registry（DTR）。\nNote: 可以通过配置不同的 Docker registry 作为 images 下载源，比如公司内部搭建 registry 私服，在平时 CI/CD 中提高工作速度。\nDocker objects # 使用 Docker 时，您正在创建和使用 images，containers，networks 和 volumes，plugins 和其他对象。 本节简要介绍其中一些对象。\nIMAGES # images 是一个只读模板，其中包含有关创建 Docker 容器的说明。 通常，images 基于另一个 images，并带有一些额外的自定义。 例如，您可以构建基于 ubuntu images 的 images，但安装 Apache Web 服务器和应用程序，以及运行应用程序所需的配置详细信息。\n您可以创建自己的 images，也可以只使用其他人创建的 images 并在 registries 中发布。 要构建自己的 images，可以使用简单的语法创建 Dockerfile，以定义创建 images 并运行 images 所需的步骤。 Dockerfile 中的每条指令都在 images 中创建一个 layer。 更改 Dockerfile 并重建 images 时，仅重建已更改的那些层。 与其他虚拟化技术相比，这是使 images 如此轻量，小巧和快速的部分原因。\nNote: 比较详细的工作原理可以看下文 Union file systems部分\nCONTAINERS # CONTAINERS 是 images 的可运行实例。 您可以使用 Docker API 或 CLI 创建，启动，停止，移动或删除容器。 您可以将容器连接到一个或多个网络，将存储连接到它，甚至可以根据其当前状态创建新 images。\n默认情况下，CONTAINERS 与其他 CONTAINERS 及其主机相对隔离。 您可以控制 CONTAINERS 的网络，存储或其他基础子系统与其他 CONTAINERS 或主机的隔离程度。\nCONTAINERS 由其 images 以及您在创建或启动时为其提供的任何配置选项定义。 删除 containers 后，对其状态的任何未存储在持久存储中的更改都将消失。\nSERVICES # services 允许您跨多个 Docker 守护程序扩展容器，这些守护程序一起作为具有多个管理器和工作程序的群组一起工作。 swarm 的每个成员都是 Docker 守护程序，守护进程都使用 Docker API 进行通信。 服务允许您定义所需的状态，例如在任何给定时间必须可用的服务的副本数。 默认情况下，服务在所有工作节点之间进行负载平衡。 对于消费者来说，Docker 服务似乎是一个单独的应用程序。 Docker Engine 支持 Docker 1.12 及更高版本中的 swarm 模式。\nDocker 底层技术 # Docker 是用 Go 编写的，它利用 Linux 内核的几个功能来提供其功能。使用到的内核特性包括 namespaces、cgroups、Union file systems\nnamespaces # Docker 使用称为 namespaces 的技术来提供称为容器的隔离工作空间。 运行容器时，Docker 会为该容器创建一组 namespaces。\n这些 namespaces 提供了一层隔离。 容器的每个方面都在一个单独的命名空间中运行，其访问权限仅限于该 namespaces。\nDocker Engine 在 Linux 上使用以下命名空间：\npid 命名空间：进程隔离（PID：进程 ID）。 net 命名空间：管理网络接口（NET：Networking）。 ipc 名称空间：管理对 IPC 资源的访问（IPC：进程间通信）。 mnt 名称空间：管理文件系统挂载点（MNT：Mount）。 uts 命名空间：隔离内核和版本标识符。 （悉尼科技大学：Unix 分时系统）。 cgroups # Linux 上的 Docker Engine 还依赖于另一种称为控制组（cgroups）的技术。 cgroup 将应用程序限制为特定的资源集。 cgroups 允许 Docker Engine 将可用的硬件资源共享给容器，并可选择强制执行限制和约束。 例如，您可以限制特定容器的可用内存。\nUnion file systems # 联合文件系统或 UnionFS 是通过创建 layers 来操作的文件系统，使它们非常轻量和快速。 Docker Engine 使用 UnionFS 为容器提供构建块。 Docker Engine 可以使用多种 UnionFS 变体，包括 AUFS，btrfs，vfs 和 DeviceMapper。\nNote: Docker 默认为 overylay2\nContainer format # Docker Engine 将 namespaces，cgroups 和 UnionFS 组合成一个称为容器格式的包装器。 默认容器格式是 libcontainer。 将来，Docker 可以通过与 BSD Jails 或 Solaris Zones 等技术集成来支持其他容器格式。\nNote: 如上部分翻译自 docker overview，同时添加自己本人理解。\nmoby 等代码的依赖介绍，相关调用方式，架构 # Docker 从一个单一的软件转移到一组独立的组件和项目。\nDocker 如何运行容器？ # Docker 引擎创建 images， 把 images 传递给 containerd， containerd 调用 containerd-shim， containerd-shim 使用 runC 来运行 container， containerd-shim 允许运行时（在本例中为 runC）在启动容器后退出 这个模型的两个主要好处是 # deamon 运行较少的容器 能够在不破坏正在运行的容器的情况下重启或升级引擎 containerd # containerd 架构 related link # moby\nlibnetwork\ncontainerd\nrunc\nVisualizing Docker Containers and Images\n"},{"id":32,"href":"/posts/docker/docker-contents/","title":"Docker 源码阅读: 目录","section":"Blog","content":"文章简介：资料汇总以及源码阅读目录\nContents # Docker 架构介绍 Docker 编译开发环境搭建 Docker Run Docker Build Docker Volume todo docker network 资料汇总 # Visualizing Docker Containers and Images容器、镜像可视化 Docker 核心技术与实现原理 docker-debug related links # Visualizing Docker Containers and Images "},{"id":33,"href":"/posts/contribute-to-opensource/","title":"Contribute to Opensource","section":"Blog","content":"文章简介：如何参与开源项目。在这里分享一些思路和开源资源。\n简介 # 自从学习计算机开始，很多时候希望自己能够也为 opensource 贡献一些什么。这里会总结一些思路为开源做些什么。\n思路 # Start Your Open Source Career这里简述了如何参与开源项目。对自己有很多启示。我们在工作学习中也会有一些自己感觉很好的对某个技术问题的解决方式,希望可以分享给大家，或者希望学习新的知识，成为某个工具的核心维护者。 这里会总结一些比较好的参与开源项目的思路。\ngood first issue or help wanted # 很多开源项目的issue中已经标记出很多类似good first issue or help wanted的 label，这些 label 表示新人可以来帮忙。可以通过一些网站找到打相应 label 的项目，这可能成为你贡献开源项目的开端。 或许这些网站可以帮到你( 来自 github)：\n开源星期五 - opensourcefriday 统计自己参与的项目，同时推荐如何开始 github-explore github 会为你推荐一些你感兴趣的项目 project-based-learning Curated list of project-based tutorials first timers only 贡献 pr 需要的 git 知识，label 搜索，相关订阅提醒等 codetriage 订阅 github 中的项目，issue 等，方便通知自己感兴趣的项目 issuehub 按照标签搜索项目 pullrequestroulette 检索需要 reviews 的 pr up-for-grabs.net 根据 label 等检索项目 how-to-contribute/#a-checklist-before-you-contribute 思路 # 构建一些工具 # 比如构建一些项目模版，比如 graphql+mongoboilerplate. 比如编写一些平时可以提高工作效率的工具， alibaba/arthas\n成为新的维护者 # 有很多有价值的项目因为没有维护者渐渐被人遗弃。你是否可以成为新的维护者呢？可以通过邮件、twiter 等联系原作者，成为项目维护者是不是很棒？\n创建自己的项目 # 如果自己有对新的技术问题的解决办法，可以开源出来，分享自己是如何解决的\n发布，推广，分享 # 为了确保每个有需要的人都乐意来找到你的模块，你必须：\n撰写 readme 等 license README 版本徽章 贡献指南 提供 ISSUE_TEMPLATE 使用本项目的产品 为项目撰写精心设计的在线网站，和文档，可以使用静态网站工具生成，如 vuepress 在 StackOverflow 和 GitHub 等社交媒体中寻找相关问题并贴出你的项目，并解答 将项目发布到汇集开源项目的社区中，如 HackerNews、 reddit、 producthunt、 hashnode 参与一些线下分享、讨论会、演讲等中介绍你的项目 链接 # octobox 将你的 GitHub 通知转成邮件的形式，这是避免因堆积「太多问题」以至于影响关注重要问题的很好的方法 probot GitHub App 可以自动化和改善你的工作流程 refined-github 浏览器扩展，简化了 GitHub 界面并添加了有用的功能 Start Your Open Source Career https://www.atlassian.com/git/tutorials/merging-vs-rebasing "},{"id":34,"href":"/posts/Java/openapi/","title":"Openapi","section":"Blog","content":"swagger and openapi 代码生成和文档自动生成一些体会\n说在前面 # 平时使用 gqlgen，一般的 workflow 是先写 graphql 的 schema，然后 code generate 对应的 model 和 api 的实现的空接口，自己对应实现对应的 resolver 即可。使用起来很流程。最近调研一下 java 下类似的工具。找到在 java 下 star 最多的项目 graphql-java，但并不是这里讨论的。 这里讨论的是基于 spring 的 openapi 的实现和 code generate 方案。\n基于 openapi 的 code generate 方案 # 首先简单介绍一下 openapi，他是语言无关的 restful 描述语言，可以使用 yaml 进行编写接口文档，通过 generate，生成不同语言的 client 和 server。 这里有官方的简介。自己比较关注的语言是 python、java、go、rust，js。\n踩到的一些坑，这里简单试用了一下generate java。首先，maven下有对应的openapi的plugin， openapi-generator-maven-plugin, 但每次compile都会进行generate，并不是我所希望的，所以这里使用了基于docker的使用方案。代码见 exfly:gorgestar/isn,使用 generator.sh生成对应的代码，配置文件可以看 generator.json，\n我的使用策略是，先修改 openapi.yaml，创建或者修改restful api，然后 bash ./generator.sh，生成对应接口默认空实现，之后由我们对应的实现接口即可。\n一些自己没有进行操作的方案：\n如何保证api和接口文档一致：可以对应的使用swagger的api-doc json进行转换为yaml，对原本手写的openapi.yaml进行比较，确认文档的一致性 这个版本的generate每次都会将所有的文件覆盖，需要编辑 .openapi-generator-ignore，类似gitignore的东西，类比修改即可。被忽略的文件，即使被删除，也不会自动生成对应文件，生成逻辑看起来比较傻。 其他资源 # exfly:gorgestar/isn,本文项目代码\nswagger\nraml\nOpenAPI-Specification\nopenapi-generator\nopenapi generator maven plugin\nspring rest docs\n"},{"id":35,"href":"/posts/tools/wsl/","title":"WSL(windows subsystem for linux) win子系统","section":"Blog","content":"\u0008win 子系统安装与 cmder+zsh 开发环境搭建\n说在前面 # 这里只展示可以做到什么程度，具体怎么做，网上教程很多，后边会贴出自己感觉比较好的网址\n大致操作思路 # 先在 windows 下 打开 win subsystem for linux 功能，之后去 win store 中下载对应的 linux 发行版.之后就是打开对应的 linux 发行版的 bash、配置 zsh 了，详细步骤见 这里。具体 zsh 怎么折腾，可以看一下 这里\n贴一张自己配置之后，使用 zsh 和 tmux 之后的截图 给我的体验是，基本可以满足大部分日常开发工作\n其他 # win 下的 CDEF 盘被挂载到/mnt下，为了方便使用，可以将他们ln -s /mnt/d $HOME/windir，这样方便自己使用 因为之前安装过 vscode，可以直接使用code filename 打开系统中的文件 类似 jdk 这种需要在 subsystem 中重新安装才可以在 wsl 中使用 图中使用的 Cmder 是非常远古的版本，所以请忽略 cmd 之前的框 链接 # Windows10 终端优化方案：Ubuntu 子系统+cmder+oh-my-zsh "},{"id":36,"href":"/posts/frontend/front-end-separation-architecture-environment-construction/","title":"前后端分离架构的Vue环境搭建指南","section":"Blog","content":"使用 Vue 前后端+Go 后端，基于 webpack 代理转发，配置前后端分离架构开发环境\n原文地址\nWeb 研发模式演变 # 最近研究一下前后端的开发模式，看到一个很好的入门路径 developer-roadmap: frontend backend DevOps,可以看一下，效果还是不错的。\n之前看到一个说 web 研发演进这里总结一下。\n很久之前，前后端的分工是，前端从设计师那里拿到设计图纸，转化静态页面模板，由后端工程师进行数据库设计等一系列设计之后，套前端给的模板。如上也即后端渲染。\n但是这样的流程，所有工作的 Block 在后端，想进一步提高研发速度，应该如何分工？到如今给出的答案是基于 Nodejs 的前后端分离架构。这时前后端分工是这样的：\n前端的工作 # UI 设计 前端路由设计 处理浏览器层的展现逻辑 通过 CSS 渲染样式，通过 JavaScript 添加交互功能，HTML 的生成也可以放在这层，具体看应用场景\n后端的工作 # 业务逻辑和 API 的设计和实现 数据库设计和维护 后端缓存设计 前后端分离下协作体系 # 前后端分离下的协作方式一般是，前后端各司其职，互不影响。\n首先，对于后端来说，后端的主要工作依然是传统的数据库设计、业务逻辑设计，但不需要套模板了，而是为前端提供数据接口\n其次，对于前端来说，前端的主要工作是，前端的 ui，以及获取数据，在前端渲染。\n工作流程是，先进行 API 设计。前后端一起设计数据接口以及数据返回的格式，现在比较常见的是 json 数据。可以根据接口生成一些 mock 用的 json 数据文件，供前端开发使用。后端根据这个 API 规范实现真正的接口。两端分别并行开发。开发结束时候联调，打通前后端之后进行调试。\n具体，可以看一下 网易前后端分离实践.\n基于 Vue 前后端分离环境搭建 # 这里对前后端分离 Vue 的开发环境进行演示。思路是，前后端分离，后端可以设置 cookie，前端可以接收到配置的 cookie\nnode 环境安装 # 安装方法见 这里\nVue 安装 # npm config set registry \u0026#39;https://registry.npm.taobao.org\u0026#39; npm install -g @vue/cli vue init webpack demo cd demo npm dev run 即可在浏览器中看到效果，熟悉的 vue 页面\n安装 axios，实现前后端交互，并实现后端设置 cookie，在前端可以生效 # 安装 axios\nnpm install axios 修改/src/components/HelloWorld.vue 中对应的 srcipt\n\u0026lt;script\u0026gt; import axios from \u0026#39;axios\u0026#39; axios.get(\u0026#39;/sc\u0026#39;) // 使用ajax export default { name: \u0026#39;HelloWorld\u0026#39;, data () { return { msg: \u0026#39;Welcome to Your Vue.js App\u0026#39; } } } \u0026lt;/script\u0026gt; 最终完成的时候，访问 \u0026lsquo;/\u0026rsquo;，可以看到 cookie 添加了一个 kv 对。现在暂时看不到效果，因为接口后端没有实现。\n后端接口实现,这里使用的 go，具体 go 编译器的安装方法 见这里\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) func LoggingMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { log.Println(\u0026#34;-\u0026gt;\u0026#34;, r.URL) next.ServeHTTP(w, r) log.Println(\u0026#34;\u0026lt;-\u0026#34;) }) } func main() { http.Handle(\u0026#34;/sc\u0026#34;, LoggingMiddleware(http.HandlerFunc(indexHandler))) port := \u0026#34;:8081\u0026#34; log.Println(\u0026#34;starting on http://localhost\u0026#34; + port) log.Fatal(http.ListenAndServe(port, nil)) } func indexHandler(w http.ResponseWriter, req *http.Request) { expire := time.Now().AddDate(0, 0, 1) cookie := http.Cookie{Name: \u0026#34;csrftoken\u0026#34;, Value: expire.String(), Expires: expire} http.SetCookie(w, \u0026amp;cookie) } 此时前后端都已经实现了，但是因为前端开在了端口 8080，后端开在了 8081，涉及到跨域，相互没法访问。需要配置一下 webpack 的配置才可以。\n// 修改一下文件/config/index.js: 13 line proxyTable: { \u0026#39;/\u0026#39;: { target: \u0026#39;http://localhost:8081\u0026#39;, changeOrigin: true } }, 之后，开启前后端服务：\nnpm dev run go run server.go 之后访问前端页面，localhost:8080，按 F12 -\u0026gt; Application -\u0026gt; Cookies， 即可看到每次刷新都会改变的 csrftoken 的 cookie\n最终的网站见 ExFly/FrontBackSep\n后记 # 既然在前后端分离后的后端可以配置cookie，其他的所有操作都可以进行了。进一步可以实现其他的操作。如上。\n引用 # developer-roadmap: frontend backend DevOps web 研发演进 网易前后端分离实践 "},{"id":37,"href":"/posts/Architecture/ITabstract/","title":"技术变革","section":"Blog","content":"总结一下数据库、分布式等技术产生的场景的使用场景。包括数据库、缓存、分布式、容器\n从使用到的技术来说，如今各大网站平台趋向于使用经过长期检验过的技术，包括不限于动态网站技术、数据库技术、高速缓存技术、负载均衡、分布式相关技术。\n首先动态网站技术，开始没有动态网站时期，大部分 BBS 使用基于 Telnet 协议为基础的 BBS 进行交流。HTTP 协议的出现，使得 BS 架构的网站能够展现多媒体等信息如视频、音乐等。起初 HTTP 协议一般仅展示一些静态数据，其表现能力不强，不能更好的动态的为用户交互。之后动态网站技术如 CGI 的出现，使得网站拥有的更加丰富的交互功能。经过数十年的技术积累，动态网站技术已经能够支撑起现如今的大部分信息的获取。\n其次是存储技术。开始的时候数据仅仅使用本地文件进行存储。文件的读写效率很高，但是需要程序员重复的使用操作系统提供的较底层的操作来操作文件，对程序员的要求太大。而且对这种结构化的数据来说，可以使用相同的模式进行操作。所以有了数据库关系系统，尤其关系型数据库。从此，对于一些结构化的数据只需要使用 SQL 这种数据操作语言，就可以很方便的操作数据，而真正的数据管理维护工作由数据库管理系统进行维护，人可以通过配置数据库，使数据库获得更好的性能。当用户量很大时候，单机响应出现瓶颈，单台数据库不能够满足这样的请求。可以通过复制的方式，将相同的数据复制到多台机器上，多台机器为用户提供数据，对于读操作进行扩展。对于读多写少的网站，这种方式基本可以实现线性的性能提升。数据量再大一些，使用分库分表方式，把一份数据切片，分布到多台机器上提高性能。进而使用复制与分库分表的方式，进一步压榨单机的性能潜力。单机数据库很强大，但数据量达到一定的数量级，数据库的性能完全不能满足需求。大数据时代，数据量达到 PB、TB 级别。而每一块硬盘仅仅几 T，单机完全不可能将所有的数据都存储下来。如上分库分表的方式基本已经不能完全解决如上的问题，所以出现了分布式数据库，比如 MySQL Cluster 等通过两阶段提交等方式，维护线上数据的强一致性，分布式存储系统，比如 Google 研发的 GFS，现如今正火热的 HDFS 等，基本可以满足海量数据的存储。\n高速缓存技术，如一些场景重复读的场景，数据库的查询很昂贵，如何减少数据库的访问次数？使用类似 Redis 的缓存。redis 的数据全部存储在内存中，内存的速度远远超过数据库的查询速度。维护 redis 中的缓存数据一致性，防止缓存穿透、缓存击穿、缓存雪崩的问题，以及如何正确的使用缓存等一系列问题，现如今已经基本有了很好的解决办法。\n负载均衡与反向代理，传统中使用 nginx 作为网关，接受的请求分发到多台服务器中，一定程度上分担单台服务器的压力。\n对于分布式技术。首先是 Google 研发并使用的 GFS 以及搭建在 GFS 基础上的 BigTable，以及 MapReduce 算法，使得分布式存储与分布式计算成为了可能。对应的开源版本为 HDFS、Hadoop、Spark 等一系列分布式基础设施。存储的分布式，也对应着应用部署的分布式，也就是现如今使用比较广泛的微服务（将一个整体的应用拆分成不同的服务，服务之间分别开发和部署，通过统一的接口进行协作）。微服务部署少则几个，多则成百上千，如此多的服务需要开发部署工作量巨大。同时很常见的问题是开发环境可以正常工作，上线却无法工作。这个问题现如今的解决办法是使用 docker。首先 docker 是一种使用 Linux 内核提供的 cgroup 和 namespace 功能，它可以将计算机的资源进行隔离。使用场景是通过容器编排工具，将服务所依赖的资源通过配置文件进行定义，由编排工具统一对所有的服务进行启动部署。现如今实质上的容器编排标准 Kubernetes，已经被不限于 Google、百度、阿里巴巴、京东等使用。对于 Google 等基本实现容器化、Kubernetes 化。\n"},{"id":38,"href":"/posts/tools/realize/","title":"Realize代码分析","section":"Blog","content":" realize 代码分析\nRealize # realize 是 Go 写的 workfloaw 工具，可以配置自己的工作流。项目在修改之后需要进行编译、测试，可以还有其他的一系列流程需要走，可以使用 realize 进行自动化。抽点时间研究了一下源码。这里总结一下思路，不是很完整的解释，简单说一下思路。\n原理 # 从 Linux 2.6.13 内核开始，Linux 就推出了 inotify，允许监控程序打开一个独立文件描述符，并针对事件集监控一个或者多个文件，例如打开、关闭、移动/重命名、删除、创建或者改变属性。glib 对对此进行了封装 glib/inotify.h,同时各个操作系统都有对应的实现，win 下的 ReadDirectoryChangesW，mac 下的 FSEvents。同时 go 下已经有写好的封装库 fsnotify/fsnotify，对不同的平台进行了封装。\n简单来讲，内核为应用程序提供了系统级文件修改事件的监视器。当文件进行修改后，会通知应用程序监视的文件已经修改了，之后有realize进行事件的处理即可。\n比较有意思的是，yaml文件的marshal和unmarshal。之后可以研究一下。\n核心代码 # // github.com/oxequa/realize/realize/projects.go func (p *Project) Watch(wg *sync.WaitGroup) { var err error // change channel p.stop = make(chan bool) // init a new watcher p.watcher, err = NewFileWatcher(p.parent.Settings.Legacy) if err != nil { log.Fatal(err) } defer func() { close(p.stop) p.watcher.Close() }() // before start checks p.Before() // start watcher go p.Reload(\u0026#34;\u0026#34;, p.stop) L: for { select { case event := \u0026lt;-p.watcher.Events(): if p.parent.Settings.Recovery.Events { log.Println(\u0026#34;File:\u0026#34;, event.Name, \u0026#34;LastFile:\u0026#34;, p.last.file, \u0026#34;Time:\u0026#34;, time.Now(), \u0026#34;LastTime:\u0026#34;, p.last.time) } if time.Now().Truncate(time.Second).After(p.last.time) { // switch event type switch event.Op { case fsnotify.Chmod: case fsnotify.Remove: p.watcher.Remove(event.Name) if p.Validate(event.Name, false) \u0026amp;\u0026amp; ext(event.Name) != \u0026#34;\u0026#34; { // stop and restart close(p.stop) p.stop = make(chan bool) p.Change(event) go p.Reload(\u0026#34;\u0026#34;, p.stop) } default: if p.Validate(event.Name, true) { fi, err := os.Stat(event.Name) if err != nil { continue } if fi.IsDir() { filepath.Walk(event.Name, p.walk) } else { // stop and restart close(p.stop) p.stop = make(chan bool) p.Change(event) go p.Reload(event.Name, p.stop) p.last.time = time.Now().Truncate(time.Second) p.last.file = event.Name } } } } case err := \u0026lt;-p.watcher.Errors(): p.Err(err) case \u0026lt;-p.exit: p.After() break L } } wg.Done() } Reference # 用 inotify 监控 Linux 文件系统事件 oxequa/realize fsnotify/fsnotify "},{"id":39,"href":"/posts/Bigdata/SparkStream/","title":"SparkStream等相关产品选型以及Spark安装与简单使用","section":"Blog","content":"比较SparkStream类似产品如Samza、Storm，介绍Spark和Spark Stream安装和简单使用方法\n各产品比较 # Samza # Samza 是一个分布式的流式数据处理框架（streaming processing），Linkedin 开源的产品， 它是基于 Kafka 消息队列来实现类实时的流式数据处理的。更为准确的说法是，Samza 是通过模块化的形式来使用 Apache Kafka 的，因此可以构架在其他消息队列框架上，但出发点和默认实现是基于 Apache Kafka。\n本质上说，Samza 是在消息队列系统上的更高层的抽象，是一种应用流式处理框架在消息队列系统上的一种应用模式的实现。\n总的来说，Samza 与 Storm 相比，传输上完全基于 Apache Kafka，集群管理基于 Hadoop YARN，即 Samza 只负责处理这一块具体业务，再加上基于 RocksDB 的状态管理。由于受限于 Kafka 和 YARN，所以它的拓扑结构不够灵活。\nStorm # Storm 框架与其他大数据解决方案的不同之处，在于它的处理方式。Apcahe Hadoop 本质上来说是一个批处理系统，即目标应用模式是针对离线分析为主。数据被引入Hadoop的分布式文件系统 (HDFS)，并被均匀地分发到各个节点进行处理，HDFS 的数据平衡规则可以参照本文作者发表于IBM的文章《HDFS数据平衡规则及实验介绍》，进行深入了解。当处理完成时，结果数据返回到HDFS，然后可以供处理发起者使用。Storm则支持创建拓扑结构来转换没有终点的数据流。不同于Hadoop作业，这些转换从不会自动停止，它们会持续处理到达的数据，即Storm的流式实时处理方式。\nSpark Streaming # Spark Streaming 类似于 Apache Storm，用于流式数据的处理。根据其官方文档介绍，Spark Streaming 有高吞吐量和容错能力强这两个特点。Spark Streaming 支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ 和简单的 TCP 套接字等等。数据输入后可以用 Spark 的高度抽象原语如：map、reduce、join、window 等进行运算。而结果也能保存在很多地方，如 HDFS，数据库等。另外 Spark Streaming 也能和 MLlib（机器学习）以及 Graphx 完美融合。\n在 Spark Streaming中，处理数据的单位是一批而不是单条，而数据采集却是逐条进行的，因此 Spark Streaming系统需要设置间隔使得数据汇总到一定的量后再一并操作，这个间隔就是批处理间隔。批处理间隔（0.2s-2s）是 Spark Streaming 的核心概念和关键参数，它决定了 Spark Streaming 提交作业的频率和数据处理的延迟，同时也影响着数据处理的吞吐量和性能。\nKafka Sreeam # Kafka Streams是一个用于处理和分析数据的客户端库。它先把存储在Kafka中的数据进行处理和分析，然后将最终所得的数据结果回写到Kafka或发送到外部系统去。它建立在一些非常重要的流式处理概念之上，例如适当区分事件时间和处理时间、窗口支持，以及应用程序状态的简单（高效）管理。同时，它也基于Kafka中的许多概念，例如通过划分主题进行扩展。此外，由于这个原因，它作为一个轻量级的库可以集成到应用程序中去。这个应用程序可以根据需要独立运行、在应用程序服务器中运行、作为Docker容器，或通过资源管理器（如Mesos）进行操作。\nKafka Sreeam直接解决了流式处理中的很多困难问题:毫秒级延迟的逐个事件处理。有状态的处理，包括分布式连接和聚合。方便的DSL。使用类似DataFlow的模型对无序数据进行窗口化。具有快速故障切换的分布式处理和容错能力。无停机滚动部署。\n主要比较Spark Stream和Storm和选择 # 比较项 SparkStream Storm 血统 UC Berkeley AMP lab Twitter 开源时间 2011.05 2011.09 依赖环境 Java Zookeeper Java Python 开发语言 Scala Java Clojure 支持语言 Scala Java Python R Any 硬盘IO 少 一般 集群支持 超过1000节点 好 吞吐量 好 较好 使用公司 intel 腾讯 淘宝 中移动 Goole 淘宝 百度 Twitter 雅虎 适用场景 较大数据块\u0026amp;需要高时效性的小批量计算 实时小数据块的分析计算 延时 准实时：一次处理一个即将到达的事件 实时：处理在一定的时间内（时间间隔可自己设置）在窗口中收到的一批事件 容错 在批处理级别进行跟踪处理，因此即使发生节点故障等故障，也可以有效地保证每个小批量都能够被精确处理一次 每个单独的记录必须在其通过系统时被跟踪，因此Storm仅保证每个记录至少被处理一次，但是从故障中恢复期间允许出现重复。 这意味着可变状态可能不正确地更新了两次 1.处理模型以及延迟\n虽然这两个框架都提供可扩展性(Scalability)和可容错性(Fault Tolerance),但是它们的处理模型从根本上说是不一样的。Storm处理的是每次传入的一个事件，而Spark Streaming是处理某个时间段窗口内的事件流。因此，Storm处理一个事件可以达到亚秒级的延迟，而Spark Streaming则有秒级的延迟。\n2.容错和数据保证\n在容错数据保证方面的权衡方面，Spark Streaming提供了更好的支持容错状态计算。在Storm中，当每条单独的记录通过系统时必须被跟踪，所以Storm能够至少保证每条记录将被处理一次，但是在从错误中恢复过来时候允许出现重复记录，这意味着可变状态可能不正确地被更新两次。而Spark Streaming只需要在批处理级别对记录进行跟踪处理，因此可以有效地保证每条记录将完全被处理一次，即便一个节点发生故障。虽然Storm的 Trident library库也提供了完全一次处理的功能。但是它依赖于事务更新状态，而这个过程是很慢的，并且通常必须由用户实现。\n简而言之,如果你需要亚秒级的延迟，Storm是一个不错的选择，而且没有数据丢失。如果你需要有状态的计算，而且要完全保证每个事件只被处理一次，Spark Streaming则更好。Spark Streaming编程逻辑也可能更容易，因为它类似于批处理程序，特别是在你使用批次(尽管是很小的)时。\n3.实现和编程API\nStorm主要是由Clojure语言实现，Spark Streaming是由Scala实现。如果你想看看这两个框架是如何实现的或者你想自定义一些东西你就得记住这一点。Storm是由BackType和 Twitter开发，而Spark Streaming是在UC Berkeley开发的。\nStorm提供了Java API，同时也支持其他语言的API。 Spark Streaming支持Scala和Java语言(其实也支持Python)。另外Spark Streaming的一个很棒的特性就是它是在Spark框架上运行的。这样你就可以想使用其他批处理代码一样来写Spark Streaming程序，或者是在Spark中交互查询。这就减少了单独编写流批量处理程序和历史数据处理程序。\n4.生产支持\nStorm已经出现好多年了，而且自从2011年开始就在Twitter内部生产环境中使用，还有其他一些公司。而Spark Streaming是一个新的项目，并且在2013年仅仅被Sharethrough使用(据作者了解)。\nStorm是 Hortonworks Hadoop数据平台中流处理的解决方案，而Spark Streaming出现在 MapR的分布式平台和Cloudera的企业数据平台中。除此之外，Databricks是为Spark提供技术支持的公司，包括了Spark Streaming。\n5.集群管理集成\n尽管两个系统都运行在它们自己的集群上，Storm也能运行在Mesos，而Spark Streaming能运行在YARN 和 Mesos上。\n这里总结了Kafka Stream-Spark Streaming-Storm流式计算框架比较选型的相关资料。\n这里由更多的相关产品的差异比较资源：\nStorm介绍 Spark Streaming vs. Kafka Stream 哪个更适合你？ 大数据框架对比：Hadoop、Storm、Samza、Spark和Flink Spark Streaming与Storm的对比分析 Storm和Spark Streaming的横向比较 Spark Streaming和Storm如何选择？搭建流式实时计算平台，广告日志实时花费 Spark Streaming 新手指南 Spark 介绍 Spark生态 # Spark官网简单介绍了spark的的优势。\n这里非常详细了介绍Spark生态、各大厂应用场景、Spark基本原理。\nSpark 和 Spark Stream的安装和使用 # Spark介绍 # Spark Streaming 是 Spark Core API 的扩展, 它支持弹性的, 高吞吐的, 容错的实时数据流的处理. 数据可以通过多种数据源获取, 例如 Kafka, Flume, Kinesis 以及 TCP sockets, 也可以通过例如 map, reduce, join, window 等的高级函数组成的复杂算法处理. 最终, 处理后的数据可以输出到文件系统, 数据库以及实时仪表盘中.事实上,你还可以在 data streams（数据流）上使用 机器学习以及 图计算 算法\n在内部, 它工作原理如下, Spark Streaming 接收实时输入数据流并将数据切分成多个 batch（批）数据, 然后由 Spark 引擎处理它们以生成最终的 stream of results in batches（分批流结果）.\nSpark Streaming 提供了一个名为 discretized stream 或 DStream 的高级抽象, 它代表一个连续的数据流. DStream 可以从数据源的输入数据流创建, 例如 Kafka, Flume 以及 Kinesis, 或者在其他 DStream 上进行高层次的操作以创建. 在内部, 一个 DStream 是通过一系列的 RDDs 来表示.\n你可以使用 Scala , Java 或者 Python（Spark 1.2 版本后引进）来编写 Spark Streaming 程序.\n这里是一篇官方编程指南\nSpark安装 # 方式1 # wget http://mirror.bit.edu.cn/apache/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz tar -xzf spark-2.3.1-bin-hadoop2.7.tgz # 运行一个例子 cd spark-2.3.1-bin-hadoop2.7 ./bin/run-example SparkPi 方式二 # 推荐这种方式这里总结了自己搭建各种开发环境的就自动化安装脚本。第一次安装会比较麻烦，之后实现一条命令自动安装。需要vagrant\u0026amp;virtual。有一些依赖docker\ngit clone https://github.com/ExFly/ComputSciLab.git cd ComputSciLab vagrant up vagrant ssh cd /vagrant/Java source install-small.sh cd /vagrant/Spark ./install.sh cd /vagrant/.softwenv/spark-2.3.1-bin-hadoop2.7 ./bin/run-example SparkPi 结果图：\nspark集群 # 找到一个 中文的文档,可以看一下，部署很简单\n总结 # 如上\n"},{"id":40,"href":"/posts/Architecture/EventuallyConsistent/","title":"分布式数据的最终一致性","section":"Blog","content":"单体应用，需要借助分库分表、复制技术、读写分离提高服务并发访问量。微服务为代表的分布式系统，其高并发和微服务事务一致性该如何保证？\n简介 # 由于自己刚刚接触，自己理解的也不深。在这里，把我整理的一些资料汇总下来。\n微服务架构 # 微服务架构将单应用放在多个相互独立的服务，这个每个服务能够持续独立的开发和部署，难题是数据该如何存储？\n多个应用使用同一数据库 # 传统的单体应用一般采用的是数据库提供的事务一致性，通过数据库提供的提交以及回滚机制来保证相关操作的ACID，这些操作要么同时成功，要么同时失败。各个服务看到数据库中的数据是一致的，同时数据库的操作也是相互隔离的，最后数据也是在数据库中持久存储的。这样的架构不具备横向扩展能力，服务之间的耦合程度也比较高，会存在单点故障。\n典型微服务架构 # 在微服务架构中， 有一个database per service的模式， 这个模式就是每一个服务一个数据库。 这样可以保证微服务独立开发，独立演进，独立部署， 独立团队。\n由于一个应用是由一组相互协作的微服务所组成，在分布式环境下由于各个服务访问的数据是相互分离的， 服务之间不能靠数据库来保证事务一致性。 这就需要在应用层面提供一个协调机制，来保证一组事务执行要么成功，要么失败。\nCAP定律 # 一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。\n通过CAP理论，我们知道无法同时满足一致性、可用性和分区容错性这三个特性，那要舍弃哪个呢？\nCA without P：如果不要求P（不允许分区），则C（强一致性）和A（可用性）是可以保证的。但其实分区不是你想不想的问题，而是始终会存在，因此CA的系统更多的是允许分区后各子系统依然保持CA。\nCP without A：如果不要求A（可用），相当于每个请求都需要在Server之间强一致，而P（分区）会导致同步时间无限延长，如此CP也是可以保证的。很多传统的数据库分布式事务都属于这种模式。\nAP wihtout C：要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。现在众多的NoSQL都属于此类。\n对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。\n对于涉及到钱财这样不能有一丝让步的场景，C必须保证。网络发生故障宁可停止服务，这是保证CA，舍弃P。貌似这几年国内银行业发生了不下10起事故，但影响面不大，报到也不多，广大群众知道的少。还有一种是保证CP，舍弃A。例如网络故障事只读不写。\n常用的解决方法 # 这里总结了一些分布式数据一致性的解决方法。分布式事务保证强一致性，但为了保证数据的一致性，放弃了一些系统性能。另一种保证最终一致性，放弃了时时数据的一致性，但处理效率最好。\n这里有一些例子如何解决的。\nBASE # 这里实验了一个基于BASE协议的最终一致性demo。注意，这里使用到了Kafka，需要自己在本地开Kafka服务。\n其他资料 # 书：大规模分布式存储系统：原理解析与架构实现 书：微服务设计 分布式事务？No, 最终一致性 分布式事务实践 -花钱的，作为目录使用 多研究些架构，少谈些框架（3）\u0026ndash; 微服务和事件驱动 消息中间件（一）分布式系统事务一致性解决方案大对比，谁最好使？ Saga分布式事务解决方案与实践 解决业务代码里的分布式事务一致性问题 分布式事务实践 实战 基于Kafka消息驱动最终一致事务（二） "},{"id":41,"href":"/posts/Java/gradle_maven/","title":"Gradle和Maven使用方法总结","section":"Blog","content":"文章简介： 1.总结gradle和maven正确使用方法 2.开箱即用maven\u0026amp;gradle同时支持的项目配置。\nGradle和Maven使用起来都比较方便，而Gradle使用更灵活，配置更方便。而公司环境一般使用Maven。因此就有了取舍，是迁移到Gradle，还是继续使用Maven？其实不需要纠结，谁说必须取舍的，两个都用起来就是了！！！\n说在前面 # Gradle和Maven都是项目自动构建工具，编译源代码只是整个过程的一个方面，更重要的是，你要把你的软件发布到生产环境中来产生商业价值，所以，你要运行测试，构建分布、分析代码质量、甚至为不同目标环境提供不同版本，然后部署。整个过程进行自动化操作是很有必要的。\n整个过程可以分成以下几个步骤：\n编译源代码 运行单元测试和集成测试 执行静态代码分析、生成分析报告 创建发布版本 部署到目标环境 部署传递过程 执行冒烟测试和自动功能测试 两者都是项目工具，但是maven使用的最多，Gradle是后起之秀，想spring等都是使用gradle构建的。Gradle抛弃了Maven的基于XML的繁琐配置，采用了领域特定语言Groovy的配置，大大简化了构建代码的行数。\n比如maven要 这么写\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; gradle这么写\ncompile(\u0026#39;org.springframework:spring-core:2.5.6\u0026#39;) 详细的Gradle和Maven比较看 这里讲的很好了。 gradle官方也对两个工具进行了比较。\n我们可以使用其中一个，或者两个一起使用！！！这是可行的，当然前提是，有一个人在整个过程中维护相同功能的两份配置。实际上并不难。抽一个周末空余时间，自己把这两个都熟悉了一下，整理了一套Gradle\u0026amp;Maven日常开发中常用的包和插件的集合，作为项目的开始。比较通用，所以需要根据公司或个人项目实际情况加入私服的配置，以及你想使用的jar包，如此简单。如果使用过程中遇到什么问题，请联系我。别忘了，帮我star一下。\n接下来涉及到的内容：\nmaven 正确使用方法 gradle正确使用方法 gradle项目和maven项目相互转化 一个项目同时支持maven和gradle配置：一个好的开始 maven 正确使用方法 # maven版本不相同问题 # 我们大部分时候使用IDE进行项目开发的时候，大部分时候会直接使用IDE创建MAVEN项目，这是正确的。可是，您有没有发现，大家合作的时候，由于maven版本不相同，哪怕是3.5.1和3.5.2的区别，都会引发一场血案！我的可以正常打开项目，而其他人却会出现问题。除了IDE下载包损坏外，就是maven的版本不相同。其实通过一些工具，已经可以让这种情况不在发生，那就是 Wrapper。请看如下图(图没配错，maven的wrapper和gradle的wrapper流程上完全相同)\n前提条件：\n项目创建者系统中已经由maven的命令 其他人没有要求，mvn可有可无（原因之后说） 具体如何做：\n项目创建者执行 mvn -N io.takari:maven:wrapper -Dmaven=3.5.3 此时，项目目录会生成mvnw.cmd和mvnw，之后的所有操作都是基于此，也就是说，项目开发者不需要由任何依赖，除了jdk-_!!! 项目创建者执行 mvnw archetype:generate 此步是自动生成项目目录结构。同时，项目管理者需要搭建好基础的代码框架。之后可以开发了 项目开发者 mvnw.cmd compiler:compile mvnw.cmd exec:java -Dexec.mainClass=\u0026quot;org.exfly.LombokL.LombokLApplication\u0026quot; -q mvn.cmd clean mvn.cmd test 。。。 注意:\n当第一次执行mvnw.cmd时候，会自动下载对应版本的Maven，maven的$HOME/.m2/wrapper/dists/\u0026lt;version\u0026gt;/下。 初网络问题，如果出现错误，依赖包已经下好，只需要到1所说的位置去掉后缀.pack，重新运行即可。 使用 dependencyManagement集中管理版本依赖 # dependencyManagement这里已经很好的解释如何做。同时可以借鉴springboot-parent 多模块项目管理方法 # 多模块项目的POM重构 通过parent的方式，将多模块依赖集中管理， 如何更好的使用maven进行项目管理 几点建议 # 尽量使用wapper多 使用 dependencyManagement集中管理版本依赖 bin下有mvn和mvnDebug(运行mvn时开始debug) M2_HOME maven主程序的安装目录 ~/.m2 本地包下载位置 http代理 setting.xml中的proxies MAVEN_OPTS 运行mvn时候相当于运行java命令，MAVEN_OPTS可以配置为任何java的命令参数 设置MAVEN_OPTS环境变量 配置用户范围settings.xml %M2_HOME%/conf/settings.xml 为全局配置文件 ~/.m2/settings.xml 为用户配置文件 不要使用IDE内嵌的Maven，应该配置IDE中为自己安装的maven 显示声明所有用到的依赖 我的maven常用命令笔记 # 我的maven常用命令笔记\ngradle正确使用方法 # 理由同上节，直接说使用方法。可以对照 我的笔记查看。\ngradle init \u0026ndash;type java-library 这里自动生成gradlew，并创建项目目录结构 之后所有命令使用gradlew即可 gradle项目和maven项目相互转化 # gradle和maven可以相互转化，意味着，我们可以使用gradle为主的开发，之后导出为maven项目，供生产环境使用。前提，你足够了解gradle和maven。\nmaven -\u0026gt; gradle # cd /path/to/mavenproject gradle init gradle wrapper gradle -\u0026gt; maven # cd /path/to/gradleproject gradlew install 将项目转换为maven和gradle项目后，目录结构如下： 之后，我们习惯使用mavnw或者gradlew，都可以。如此，做到了共存。\n一个项目同时支持maven和gradle配置：一个好的开始 # 抽时间，做了常用jar包和插件整合包，一个项目同时支持maven和gradle。\n共同的依赖：\n内容包括： 日志、通用工具库、单元测试、代码质量度量、文档生成等 jar: slf4j、logback、lombok、guava、junit、mockito 配置中整合的工具：\n代码质量分析报告工具：pmd、findbugs、checkstyle、jdepend 单元测试报告工具、javadoc、依赖管理、项目信息汇总等可视化信息 maven具体内容\nmaven-compiler-plugin、maven-javadoc-plugin、cobertura-maven-plugin、maven-checkstyle-plugin、findbugs-maven-plugin、maven-pmd-plugin、jdepend-maven-plugin、maven-jar-plugin、maven-surefire-plugin、maven-surefire-report-plugin gradle具体内容\njava、maven、checkstyle、pmd、findbugs、jdepend、eclipse、idea、javadoc 首先maven配置见 此文件\n其次gradle配置见 此文件\n资料汇总 # 完整的整合项目，支持maven和gradle，点我下载 我的Gradle笔记，点我查看 我的maven笔记，点我查看 "},{"id":42,"href":"/posts/Lua/openresty_awesome/","title":"OpenResty最佳实践","section":"Blog","content":"如果你正在学Lua与openresty，那你就一定知道在开发过程中，调试代码、单元测试是多么的麻烦。这里整理了一些lua开发的最佳实践。\n简介 # openresty中lua ide调试，单元测试比较麻烦；lua对库的管理比较散漫。在公司生产环境，一般没有外网环境，OpenResty的安装和lua项目的部署都比较麻烦。 结合Python的一些经验，在这里整理一下自己对Lua的理解，以及Lua最佳实践。\nOpenResty安装 # 对于软件，使用编译方式安装比较好，比如Ubuntu，apt-get安装的包一般都会比较旧。如下介绍我的编译参数。这里需要自己下载自己的依赖包：naxsi, nginx-goodies-nginx-sticky-module-ng，pcre，openssl，zlib，并根据我的配置进行修改相应参数\n./configure --prefix=$HOME/openresty \\ --add-module=$HOME/openresty/setupfile/third/naxsi-0.55.3/naxsi_src \\ --add-module=$HOME/openresty/setupfile/third/nginx-goodies-nginx-sticky-module-ng \\ --with-pcre=$HOME/openresty/setupfile/depency/pcre-8.41 \\ --with-openssl=$HOME/openresty/setupfile/depency/openssl-1.0.2k \\ --with-zlib=$HOME/openresty/setupfile/depency/zlib-1.2.11 \\ --with-http_v2_module \\ --with-http_sub_module \\ --with-http_stub_status_module \\ --with-http_realip_module \\ --with-cc-opt=-O2 \\ --with-luajit 这里是一个比较好的 nginx的笔记，可以过一遍 我在学习的时候，看了这本书 深入理解Nginx模块开发与架构解析,毕竟讲的比较系统，可以借鉴一下 有问题，知乎，搜索引擎 安装luarocks # 下载地址 http://luarocks.github.io/luarocks/releases/ 编译安装 ./configure --prefix=$HOME/openresty/luajit \\ --with-lua=$HOME/openresty/luajit \\ --lua-suffix=jit \\ --with-lua-include=$HOME/openresty/luajit/include/luajit-2.1 --prefix 设定 luarocks 的安装目录 --with-lua 则是系统中安装的 lua 的根目录 --lua-suffix 版本后缀，此处因为openresyt的lua解释器使用的是 luajit ,所以此处得写 jit --with-lua-include 设置 lua 引入一些头文件头文件的目录 make build \u0026amp;\u0026amp; make install lua面向对象 # lua 借助table以及metatable的概念进行oo的。这里摘了一个博客的代码，看起来还可以。以后可以使用这个。 Lua 中实现面向对象。 这里要说一下lua中 .运算和:的区别，a={};a.fun(a, arg) 等价于 a:fun(arg)，其实就是:可以省略self参数。\nlocal _class={} function class(super) local class_type={} class_type.ctor=false class_type.super=super class_type.new=function(...) local obj={} do local create create = function(c,...) if c.super then create(c.super,...) end if c.ctor then c.ctor(obj,...) end end create(class_type,...) end setmetatable(obj,{ __index=_class[class_type] }) return obj end local vtbl={} _class[class_type]=vtbl setmetatable(class_type,{__newindex= function(t,k,v) vtbl[k]=v end }) if super then setmetatable(vtbl,{__index= function(t,k) local ret=_class[super][k] vtbl[k]=ret return ret end }) end return class_type end 基本编码规范 设计 # 可以参考OpenResty的最佳实践，平时用起来，大部分跟c的风格差不多吧。主要是所使用的代码风格要统一。\n包管理 # lua下有两个包管理系统，LuaDist和LuaRocks\n单元测试 # 重点 如下方法请在命令行中使用类似curl localhost/unittest进行测试，浏览器中看会很痛苦 OpenResty最佳实践-单元测试给出一种方法。我的处理方法是，在nginx.conf中的server中建一个单独的location，content_by_lua_file 设置unittest.lua。公司用的verynginx，所以我把此配置放到了router.lua中(当然配置方法类似，这个很容易研究，就不放到这里了)。 -- file: unittest.lua local _M = {} local csrf_test = require(\u0026#34;test.test_csrf\u0026#34;) local tmp_test = require(\u0026#34;test.tmp_test\u0026#34;) function _M:run_unittest() csrf_test:run() end return _M -- file: test_csrf.lua local iresty_test = require(\u0026#34;resty.iresty_test\u0026#34;) local json = require(\u0026#34;json\u0026#34;) local config = require(\u0026#34;config\u0026#34;) local csrf_config = require(\u0026#34;csrf_config\u0026#34;) local token = require(\u0026#34;token\u0026#34;) local tabletls = require(\u0026#34;tabletls\u0026#34;) local tb = iresty_test.new({unit_name=\u0026#34;test_csrf\u0026#34;}) local function assert_eq(wanted, real, msg) if wanted ~= real then error(msg or \u0026#34;error\u0026#34;, 2) -- 请注意参数 2 end end local function assert_not_eq(wanted, real, msg) if wanted == real then error(msg or \u0026#34;error\u0026#34;, 2) end end function tb:test_geturl() assert_eq(\u0026#34;/unittest\u0026#34;, ngx.var.uri, \u0026#34;the unittest url changed\u0026#34;) end function tb:run_unittest() tb:run() end return tb 如上有一个很有意思的地方，error(msg or \u0026quot;error\u0026quot;, 2),其中的2有些讲究，表示返回调用函数所在行，还有0（忽略行号），1（error调用位置行号） 性能测试 代码覆盖率 API测试等，都可以去 OpenResty最佳实践中找，配置很简单。 远程调试 OpenResty # 对于此部分，对于有些人来说，使用日志就已经足够了。可对于有些时候，在代码中太多的日志有不利于维护。这里自己要尽力做好日志和调试的平衡吧。 此调试方法适用于 win linux osx 先贴这里用到的luaIDE地址： ZeroBraneStudio 如下为安装步骤： 下载这个项目， ZeroBraneStudio，解压可以直接用【调试方法在下载好的文件中README.md中有相应的链接】 启动ZBS，Project -\u0026gt; Start Debugger Server 复制/lualibs/mobdebug/mobdebug.lua -\u0026gt; nginx lua path, 复制/lualibs/socket.lua -\u0026gt; nginx lua path， 复制/bin/clibs/socket/core -\u0026gt; socket设为nginx lua cpath（调试时候，使用的是require(\u0026ldquo;socket.core\u0026rdquo;)形式导入包。这里需要注意core文件后缀，win是dll，linux是so，） nginx配置好,将如上依赖加到nginx.conf中，让lua可以找到这些文件即可 创建需要调试的lua文件 require(\u0026#39;mobdebug\u0026#39;).start(\u0026#39;192.168.1.22\u0026#39;) local name = ngx.var.arg_name or \u0026#34;Anonymous\u0026#34; ngx.say(\u0026#34;Hello, \u0026#34;, name, \u0026#34;!\u0026#34;) ngx.say(\u0026#34;Done debugging.\u0026#34;) require(\u0026#39;mobdebug\u0026#39;).done() 注：start()呼叫需要运行IDE的计算机的IP 。默认情况下使用“localhost”，但是由于您的nginx实例正在运行，因此您需要指定运行IDE的计算机的IP地址（在我的例子中192.168.1.22）\n在ide中打开需要调试的如上lua文件 Project -\u0026gt; Project Directory -\u0026gt; Set From Current File。 此时，打开浏览器，访问需要此文件处理的链接 此时开始调试 注：在最下侧有Remote console，在这里可以执行任何ngx lua语句 如上流程没有截图，或者没有说清楚，可以来 这里 nginx一些技巧 # 看我配置的nginx.conf lua_package_path \u0026#39;$prefix/lua_script/?.lua;;\u0026#39;; 我的笔记 资料 # 章亦春 OpenResty OpenResty最佳实践 Lua 5.1 参考手册 Lua 5.3 参考手册 云风 github awesome-lua awesome-resty Nginx-Lua-OpenResty-ResourcesA collection of resources covering Nginx, Nginx + Lua, OpenResty and Tengine "},{"id":43,"href":"/posts/Java/spring/beans/","title":"Spring Beans的装配规则总结","section":"Blog","content":"文章简介：spring中bean的装配有一定规则，在这里进行总结。本文主要讲解一些概念和java配置方法。demo代码见文末。\n目录 # 手动装配 使用@Bean 自动装配 使用@ComponentScan 自动装配的歧义性 条件生效 Bean profile bean作用域 手动装配 # 手动装配可以通过声明xml文件和java配置文件两种手段。在这两种方式中，更加推荐使用java配置的方式。两种配置不是相互替代的关系，一般将业务相关的配置放到java配置中，对于非业务，如数据库等，可以放到xml中。 通过使用@Bean注解方法，可以声明并注册一个以方法名为name的bean。@Bean(name= {\u0026ldquo;sayAndPlayServiceNewName\u0026rdquo;, \u0026ldquo;sayAndPlayService\u0026rdquo;})\npublic interface SayAndPlayService { String say(); String play(); } public class PeopleSayAndPlayServiceImpl implements SayAndPlayService { @Override public String say() { return \u0026#34;People Service implements say.\u0026#34;; } @Override public String play() { return \u0026#34;People Service implements play.\u0026#34;; } } @Configuration public class SimpleManualwireConfig { @Bean public SayAndPlayService sayAndPlayService() { return new PeopleSayAndPlayServiceImpl(); } } @RunWith(SpringRunner.class) @ContextConfiguration(classes= {org.exfly.demo.config.SimpleManualwireConfig.class}) public class SimpleBeanManualwireTest { @Autowired private SayAndPlayService service; @Test public void testTestOneAutowiredService() { Assert.assertEquals(\u0026#34;People Service implements say.\u0026#34;, service.say()); } @Test public void testAnnotationConfigAppContext() { ApplicationContext context = new AnnotationConfigApplicationContext(org.exfly.demo.config.SimpleManualwireConfig.class); SayAndPlayService service = (SayAndPlayService) context.getBean(\u0026#34;sayAndPlayService\u0026#34;); Assert.assertEquals(\u0026#34;People Service implements say.\u0026#34;, service.say()); } } 自动装配 # 使用@ComponentScan可以自动扫描，@ComponentScan告诉Spring 哪个packages 的用注解标识的类 会被spring自动扫描并且装入bean容器。自动扫描，会扫描相应包以及子包，并为所有bean生成name，name命名规则为其类首字母变小写，如interface UserService被唯一的UserServiceImpl实现，则经过扫描，bean被声明为name为userServiceImpl。如果接口被多各类实现，需要转到下文消除歧义部分进行了解。\npublic interface SpeakService { String speak(); } @Service public class PeopleSpeakServiceImpl implements SpeakService { @Override public String speak() { return \u0026#34;People speak\u0026#34;; } } @Configuration @ComponentScan(basePackageClasses={org.exfly.demo.service.SpeakService.class}) public class SimpleConfigScanConfig {} @RunWith(SpringRunner.class) @ContextConfiguration(classes= {org.exfly.demo.config.SimpleManualwireConfig.class}) public class SimpleBeanManualwireTest { @Test public void testAnnotationConfigAppContextAutoScan() { ApplicationContext context = new AnnotationConfigApplicationContext(org.exfly.demo.config.SimpleConfigScanConfig.class); SpeakService service = (SpeakService) context.getBean(\u0026#34;peopleSpeakServiceImpl\u0026#34;); Assert.assertEquals(\u0026#34;People speak\u0026#34;, service.speak()); } } //如果希望使用@Autowired自动装配， @RunWith(SpringRunner.class) @ContextConfiguration(classes= {org.exfly.demo.config.SimpleConfigScanConfig.class}) public class SimpleAutoScanTest { @Autowired //根据类型进行自动注入 private SpeakService sservice; @Test public void testAnnotationConfigAppContextAutoScanAutoWire() { Assert.assertEquals(\u0026#34;People speak\u0026#34;, sservice.speak()); } } @Autowired # 可以在属性、构造方法、set函数中进行自动注入\n消除歧义 # 通过使用@Bean注解方法，可以声明并注册一个以方法名为name的bean。如果使用@Bean(name= {\u0026ldquo;sayAndPlayServiceNewName\u0026rdquo;, \u0026ldquo;sayAndPlayService\u0026rdquo;})对bean进行命名，可以用不同的名字取用（在@Autowired处再加一个@Qualifier(\u0026ldquo;sayAndPlayServiceNewName\u0026rdquo;)）; 如果使用@ComponentScan，相应的Bean定义需要使用Component等进行注解，同时使用@Qualifier(\u0026ldquo;BeanId\u0026rdquo;)限定符，如下\n@Configuration public class SimpleManualwireConfig { @Bean(name={\u0026#34;sayAndPlayServiceNewName\u0026#34;, \u0026#34;sayAndPlayService\u0026#34;}) public SayAndPlayService sayAndPlayService() { return new PeopleSayAndPlayServiceImpl(); }\t} @Service @Qualifier(\u0026#34;peopleSayServ\u0026#34;) public class PeopleSayServiceImpl implements SayService {} //or @Service(\u0026#34;peopleSayServ\u0026#34;) public class PeopleSayServiceImpl implements SayService {} //如何使用 @Autowired @Qualifier(\u0026#34;peopleSayServ\u0026#34;) SayService service; 条件生效 # 如下的解释：在当前上下文中，如果Conditional注解中的MagitExistsCondition.matches方法返回true，则当前bean：magicBean生效。@Profile和springboot自动配置都是基于此种原理实现的。\n@Bean @Conditional(MagitExistsCondition.class) public MagicBean magicBean(){ return new MagitBean(); } public class MagitExistsCondition implements Condition { boolean matches(ConditionContext ctxt, AnnotatedTypeMetadat metadate){ Environment env = ctxt.getEnvironment(); return env.containsProperty(\u0026#34;magic\u0026#34;); } } @Profile # bean作用域 # Singleton 默认 只创建一个实例 Prototype 每次创建新的实例 Session 每个会话创建一个实例 Request 每个请求创建一个实例 使用@Scope进行配置即可\n@Component @Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE) public class Notepad{} 运行时值注入 # 以后补充\n其他 # 项目代码\n"},{"id":44,"href":"/posts/Java/CollectionsLearn/","title":"Collections知识整理","section":"Blog","content":"文章简介：学Java Collections集合，对其中一些知识进行整理\nCollections结构 # 使用例子 # Iterator # public void testIterator(){ //创建一个集合 Collection books = new HashSet(); books.add(\u0026#34;轻量级J2EE企业应用实战\u0026#34;); books.add(\u0026#34;Struts2权威指南\u0026#34;); books.add(\u0026#34;基于J2EE的Ajax宝典\u0026#34;); //获取books集合对应的迭代器 Iterator\u0026lt;String\u0026gt; it = books.iterator(); while(it.hasNext()) { String book = it.next(); System.out.println(book); if (book.equals(\u0026#34;Struts2权威指南\u0026#34;)) { it.remove(); //使用Iterator迭代过程中，不可修改集合元素,下面代码引发异常 //books.remove(book); } //对book变量赋值，不会改变集合元素本身 book = \u0026#34;测试字符串\u0026#34;; } System.out.println(books); } List # 实现List接口的常用类有LinkedList，ArrayList\nList\u0026lt;String\u0026gt; list = new LinkedList\u0026lt;\u0026gt;(); Set # Set接口有以下几种实现：\nHashSet : 为快速查找设计的Set，主要的特点是：不能存放重复元素，而且采用散列的存储方法，所以没有顺序。这里所说的没有顺序是指元素插入的顺序与输出的顺序不一致。 TreeSet : 保存次序的Set, 底层为树结构。使用它可以从Set中提取有序的序列。 LinkedHashSet : 具有HashSet的查询速度，且内部使用链表维护元素的顺序(插入的次序)。于是在使用迭代器遍历Set时，结果会按元素插入的次序显示。 Set\u0026lt;String\u0026gt; hs = new HashSet\u0026lt;\u0026gt;(); Map # Map接口有以下几种实现： HashMap、LinkedHashMap、HashTable和TreeMap\nMap\u0026lt;String, String\u0026gt; m1 = new HashMap\u0026lt;\u0026gt;(); m1.put(\u0026#34;Zara\u0026#34;, \u0026#34;8\u0026#34;); m1.get(\u0026#34;Zara\u0026#34;); // 8 m1.containsKey(\u0026#34;Zara\u0026#34;); // true Java8的HashMap详解（存储结构，功能实现，扩容优化，线程安全，遍历方法） Queue # Queue\u0026lt;String\u0026gt; queue = new LinkedList\u0026lt;String\u0026gt;(); //添加元素 queue.offer(\u0026#34;a\u0026#34;); queue.offer(\u0026#34;b\u0026#34;); queue.offer(\u0026#34;c\u0026#34;); queue.offer(\u0026#34;d\u0026#34;); queue.offer(\u0026#34;e\u0026#34;); for(String q : queue){ System.out.println(q); } System.out.println(\u0026#34;===\u0026#34;); System.out.println(\u0026#34;poll=\u0026#34;+queue.poll()); //返回第一个元素，并在队列中删除 for(String q : queue){ System.out.println(q); } System.out.println(\u0026#34;===\u0026#34;); System.out.println(\u0026#34;element=\u0026#34;+queue.element()); //返回第一个元素 for(String q : queue){ System.out.println(q); } System.out.println(\u0026#34;===\u0026#34;); System.out.println(\u0026#34;peek=\u0026#34;+queue.peek()); //返回第一个元素 for(String q : queue){ System.out.println(q); } /* a b c d e === poll=a b c d e === element=b b c d e === peek=b b c d e */ 转成线程安全 # List\u0026lt;String\u0026gt; list = Collections.synchronizedList(new LinkedList\u0026lt;\u0026gt;()); 资源 # 官方 Collections Api reference Java集合框架面试题 比较细致的讲解 面试整理-Java综合高级篇（吐血整理） 最全的BAT大厂面试题整理 "},{"id":45,"href":"/posts/tools/wox_everything/","title":"Wox+Everything改变日常使用电脑的流程神器，墙裂推荐","section":"Blog","content":"文章简介：你可以将 Wox 看作一个高效的本地快速搜索框，通过快捷键呼出（默认alt+空格），然后输入关键字（支持拼音模糊查询）来搜索程序进行快速启动，或者搜索本地硬盘的文件，打开百度、Google 进行搜索，甚至是通过一些插件的功能实现单词翻译、关闭屏幕、查询剪贴板历史、查询编程文档、查询天气等更多功能。\n软件准备 # everything wox 介绍一些软件的特性 # 直接使用搜索引擎搜索 可以搜索软件，直接回车即可运行 搜索文件、文件夹，回车后使用系统默认软件打开 配置系统命令，可以直接运行(类似Win+R) 通过插件可以实现单词翻译等功能 安装方法 # 链接中有具体的使用方法。我更喜欢绿色软件，下载下来直接可以使用。\n安装之后 # 看一下使用效果： 引用 # Wox一款国产开源的快捷启动器辅助工具神器 具体怎么配置可以看这个 wox程序开源地址 "},{"id":46,"href":"/posts/Algorithm/wiki_zh_practice_Word2Vec/","title":"使用gensim训练word2vec模型--中文维基百科语料","section":"Blog","content":"文章简介：为了写论文，使用gensim训练word2vec模型，如下记录了进行训练的过程\n准备 # 中文维基百科预料： zhwiki-latest-pages-articles.xml.bz2 python3 wiki_zh_word2vec 繁体转简体： opencc一定要下*-win32.7z,win64的在我电脑上无法运行。如果使用我的 wiki_zh_word2vec,则项目中包含可以直接使用的opencc TODO # 依赖准备 # 下载 中文维基百科预料 git clone https://github.com/ExFly/wiki_zh_word2vec.git 将zhwiki-latest-pages-articles.xml.bz2放到build文件夹下 cd path/to/wiki_zh_word2vec pip install pipenv pipenv install \u0026ndash;dev 将XML的Wiki数据转换为text格式 # pipenv run python 1_process.py build/zhwiki-latest-pages-articles.xml.bz2 build/wiki.zh.txt 31分钟运行完成282855篇文章，得到一个931M的txt文件\n中文繁体替换成简体 # opencc-1.0.1-win32/opencc -i build/wiki.zh.txt -o build/wiki.zh.simp.txt -c opencc-1.0.1-win32/t2s.json 大约使用了15分钟\n结巴分词 # pipenv run python 2_jieba_participle.py 大约使用了30分钟\nWord2Vec模型训练 # pipenv run python 3_train_word2vec_model.py 大约使用了30分钟，且全程cpu使用率达到90%+\n模型测试 # pipenv run python 4_model_match.py d:\\Project\\wiki_zh_word2vec (develop) λ pipenv run python 4_model_match.py 国际足球 0.5256255865097046 足球队 0.5234458446502686 篮球 0.5108680725097656 足球运动 0.5033905506134033 国家足球队 0.494105726480484 足球比赛 0.4919792115688324 男子篮球 0.48382389545440674 足球联赛 0.4837716817855835 体育 0.4835757911205292 football 0.47945135831832886 查看结果 # 可以使用linux的head或者tail命令查看运行的结果。\nhead -n 100 wiki.zh.simp.txt \u0026gt; wiki.zh.simp_head_100.txt,直接查看wiki.zh.simp_head_100.txt即可 没有head命令，可以安装 gow，或者直接下载 cmder,进入就可以使用head命令了 结果 # 至此，使用python对中文wiki语料的词向量建模就全部结束了，wiki.zh.text.vector中是每个词对应的词向量，可以在此基础上作文本特征的提取以及分类。所有代码都已上传至 本人GitHub中，欢迎指教！ 感谢 AimeeLee77,其代码为Python2，我的项目 exfly/wiki_zh_word2vec已经完全迁移到python3,并向 AimeeLee77提交了pull request wiki_zh_word2vec "},{"id":47,"href":"/posts/diary/2018.02.01-2017-holiday-summary/","title":"2017年终总结，未来规划","section":"Blog","content":"对2017年全年的总结，并为未来做一些安排。\n年前一天，依旧忙碌 # 往事 # 如今 # 未来 # 以后该怎么如何进步？\n其他 # 继续学习，共勉！\n"},{"id":48,"href":"/posts/tools/vim/","title":"Vim学习","section":"Blog","content":"学习vim基本使用的笔记\n引言 # 想学一下Vim的键位，结合sublime text和vscode的vim插件加快编码速度编码\n正文 # 资源 # 使用方法 # Vim 简体中文 cheatsheet # vim_cheat_sheet vim_表格形式 vim_脑图形式 vim_中文形式 "},{"id":49,"href":"/posts/sercurity/2017.12.19-%E4%B8%AA%E4%BA%BA%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86-Android%E8%A3%85%E6%9C%BA%E6%8C%87%E5%8D%97/","title":"个人密码管理+Android装机指南","section":"Blog","content":"分享自己的密码管理体系: keepass+ 坚果云+ keepass2Android，以及使用 容器，搭建防止手机越用越卡的日常使用app体系，该应用不需要root。\n个人密码管理 # 准备 # win: keepass 自助云存储: 坚果云账号 Android: keepass2Android 开始 # 工具准备好就开始吧，按如下步骤：\n电脑端keepass本地建一个密码文件（需要一个主密码，以后可以修改，主密码一定要复杂），后上传到坚果云里，（这时候就可以删除本地的密码文件了）； 坚果云中配置第三方应用授权。（坚果云记得开二步验证，这样每次登陆需要微信接收验证码才可以登陆，更安全一些）； 电脑keepass打开url，以及Android手机keepass2Android打开url； 完。 具体如何创建请看这个 链接， 如我这般建密码维护基本不会出现问题。\n到这里密码管理体系基本完成了。可是对于一个新手机，用好久就会卡，很卡，超级卡，为了解决这个问题，就要耍一些小手段，具体如下。\nAndroid装机指南 # 准备 # 容器开源产品，基于virtualapp框架，有点像Docker；又是双开工具，他自己有自己的运行环境；又可以说是Android下的免安装应用的运行平台。之后会告诉你怎么用，超级棒 酷安应用商店 apkpure.com下载中国下载不了的应用，有一个没被墙的网址，下载好app后，app不需要vpn，懂了没？域名没找到，懒得找了。google商店里所有的应用这里都可以下，自己想像吧 开始 # 先截个图，看一看 最屌炸天的是，我把淘宝、王者荣耀和吃鸡都放到容器了，而且完全没有性能损失\n基本思路：把必须装到手机里的（比如支付宝，微信等）装到手机里，非必需（比如淘宝，百度云等），都装容器里。\n类比真机，软件需要有一个执行环境和临时文件，容器里的软件文件都存到了/virtual中的。\n使用思路：平时把可以放到容器中的软件放到里边，不用的时候直接关闭容器，容器里所有的软件会关闭。这样就防止软件的后台自动启动，浪费内存，手机越用越慢的现象。\n具体看我平时常用的一些软件，我把他们分为主机（就是安装到真机中）和容器中的。地址分别如下\n真机 容器 最后 # 有什么问题请直接在我的酷安@我，或者邮箱我：exflycat@gmail.com。\nEnjoy!!!\n"},{"id":50,"href":"/posts/sercurity/2017.09.17-black-phone/","title":"oneplus one 刷Kali Linux NetHunter","section":"Blog","content":"看了余弦大大的知乎live后，发现真的需要对自己的隐私安全做点什么了。一激动，淘宝500大洋买了一部oneplus one，刷kali。 这里记录使用oneplus one手机打造黑手全过程，以及一些使用到的资源，以及经验汇总。\n一 # 使用oneplus one搭建黑手还是比较简单的，因为对Android平台不熟悉，刷kali过程中踩了许多坑。比如 二清、三清、四清等等。如下一步一步的说如何刷。\n二 # 刷机四步走 # 第一步， 仔细阅读 官方Wiki # 仔细研读仔细阅读 官方Wiki，可以减少刷机过程中各种坑。\n第二步， 刷 TWRP-oneplus1 # 一句话总结就是：解锁、刷进对应的 twrp.img。整个过程本质是一加 3T 开启了开发者模式，同时电脑上基于预配置好的adb、fastboot命令完成这一系列操作。这个\u0026quot;预配置\u0026quot;在 Windows 下，也可以参考\u0026quot; Bacon Root Toolkit\u0026quot;（这是专为一加打造的GUI工具集，当时还是一加1时，用这个很方便，虽然很久没更新了，但作为参考还是很好的）。\n第三步，下载最新的 NetHunter，并进入TWRP的recovery模式刷入kali # kernel-nethunter-[device]-[os]-*.zip nethunter-generic-[arch]-kalifs-*.zip 对于oneplus1手机来说，其对应的[arch]为armhf。 随后进行刷入操作。进入TWRP，选择安装。先找到CM13.0刷到oneplus中，后进行默认的WIPE。之后再TWRP安装中选择kernel-nethunter-[device]-[os]-*.zip，安装结束后，在选择nethunter-generic-[arch]-kalifs-*.zip，最后这个文件安装话费的时间比较久，大约10多分钟的样子。\n第四步，都刷顺利后，开机进入kali ，用已经“预装”上的 SuperSU App 来完成之后一系列的 Root授权即可。 # 三 # 如果安装Kali Linux NetHunter,需要下载的文件如下：\nCM13.0 TWRP：第三方recovery brt：oneplus的解锁、root工具 Kali Linux NetHunter：如果刷其他系统，可能需要的文件如下： TWRP：第三方recovery lineageos rom：cm的重生 supersu：root工具 四 # Enjoy！\n最后 # 安装了Kali Linux NetHunter，手机便有了完整的python环境。剩下的，你懂的。\n"},{"id":51,"href":"/posts/Algorithm/sort/","title":"各种排序算法实现方法","section":"Blog","content":"总结一些常用的排序算法，如冒泡排序、插入排序、快速排序、计数排序、二分排序、归并排序等。\nSource # 排序算法比较-wiki visualgo 排序算法的动态图 排序算法复杂度 冒泡排序 # 伪代码 # function bubble_sort (array, length) { var i, j; for(i from 0 to length-1){ for(j from 0 to length-1-i){ if (array[j] \u0026gt; array[j+1]) swap(array[j], array[j+1]) } } } 函数 冒泡排序 输入 一个数组名称为array 其长度为length i 从 0 到 (length - 1) j 从 0 到 (length - 1 - i) 如果 array[j] \u0026gt; array[j + 1] 交换 array[j] 和 array[j + 1] 的值 如果结束 j循环结束 i循环结束 函数结束 python实现 # def bubble(List): for j in range(len(List)-1,0,-1): for i in range(0, j): if List[i] \u0026gt; List[i+1]: List[i], List[i+1] = List[i+1], List[i] return List 插入排序 # python实现 # def insert_sort(lst): n=len(lst) if n==1: return lst for i in range(1,n): for j in range(i,0,-1): if lst[j] \u0026lt; lst[j-1]: lst[j], lst[j-1] = lst[j-1], lst[j] return lst 快速排序 # python实现 # def quicksort(a): if len(a) == 1: return a[0] if len(a) \u0026lt; 1: return 0 return quicksort([x for x in a[1:] if x \u0026lt; a[0]]), [a[0]], quicksort([x for x in a[1:] if x \u0026gt; a[0]]) C语言实现 # int partition(int arr[], int low, int high){ int key; key = arr[low]; while(low \u0026lt; high){ while(low \u0026lt; high \u0026amp;\u0026amp; arr[high]\u0026gt;= key ) high--; if(low \u0026lt; high) arr[low++] = arr[high]; while( low \u0026lt; high \u0026amp;\u0026amp; arr[low]\u0026lt;=key ) low++; if(low \u0026lt; high) arr[high--] = arr[low]; } arr[low] = key; return low; } void quick_sort(int arr[], int start, int end){ int pos; if (start \u0026lt; end){ pos = partition(arr, start, end); quick_sort(arr,start,pos-1); quick_sort(arr,pos+1,end); } return; } 归并排序 # python实现 # from collections import deque def merge_sort(lst): if len(lst) \u0026lt;= 1: return lst def merge(left, right): merged,left,right = deque(),deque(left),deque(right) while left and right: merged.append(left.popleft() if left[0] \u0026lt;= right[0] else right.popleft()) # deque popleft is also O(1) merged.extend(right if right else left) return list(merged) middle = int(len(lst) // 2) left = merge_sort(lst[:middle]) right = merge_sort(lst[middle:]) return merge(left, right) 计数排序 # C实现 # void counting_sort(int *ini_arr, int *sorted_arr, int n) { int *count_arr = (int *) malloc(sizeof(int) * 100); int i, j, k; for (k = 0; k \u0026lt; 100; k++) count_arr[k] = 0; for (i = 0; i \u0026lt; n; i++) count_arr[ini_arr[i]]++; for (k = 1; k \u0026lt; 100; k++) count_arr[k] += count_arr[k - 1]; for (j = n; j \u0026gt; 0; j--) sorted_arr[--count_arr[ini_arr[j - 1]]] = ini_arr[j - 1]; free(count_arr); } 二分查找 # int binary_search(int array[],int n,int value){ int left=0; int right=n-1; while (left\u0026lt;=right){ int middle=left + ((right-left)\u0026gt;\u0026gt;1); //防止溢出，移位也更高效。同时，每次循环都需要更新。 if (array[middle] \u0026gt; value) { right =middle-1; } else if(array[middle] \u0026lt; value) { left=middle+1; } else { return middle; } } return -1; } "},{"id":52,"href":"/posts/diary/2017.01.01-2016-holiday-summary/","title":"2016年终总结，未来规划","section":"Blog","content":"对2016年全年的总结，并为未来做一些安排。\n年前一天，依旧忙碌 # 往事 # 14年十月至今天，从0到1的转变。 开始的时候，不断的学习各种软件的使用，像什么PS之类的东西玩了个遍，总感觉这些东西都是别人做出来的东西，便生出自己也搞出一些想这类软件。然而继续的瞎搞。 不知什么时候，知道通过学习c可以做出来好多有意思的小玩意，比如贪吃蛇，便开始学习高级语言。 之后，知道原来学习计算机，需要系统的学习计算机的理论知识，从此入坑，一发不可收拾。 网易云课堂是自己计算机启蒙课程。跟着上边的课程学了好久，懂了计算机需要如何入门。学了导论、计组、C语言、算法和数据结构、操作系统、计算机网络、数据库原理等等。自己找视频看了Linux，读了鸟哥的linux书，甚至直接吧自己的电脑系统换成了Ubuntu/linuxmint，真正体会到Linux该如何使用。 假期用Flask完成了动态网站，部署到服务器里一段时间，后来因为网站太简单，给撤了。 后来深入web，用了两个月的时间，把前端学了一下，同时完成了自己第一个网页。学了Tornado,重构了几次Project Generator，也算初步掌握全栈开发。同学科研训练，找了一个Tornado开源项目，改成了论坛系统，还帮这个项目修了几个bug。 学了些东西。\n如今 # 如今，趁着假期，也是准备考研前的最后一个可以自由支配的假期，准备了些东西回来学。每天完成一个汇编的项目，( 假期汇编 )。准备好好学学数据结构和算法导论.\n越努力越迷茫 # 越努力，知道的越多，不知到的更多。为了选择自己的技术方向，越接触越不知道该学什么，越着急，越急躁。 同时还有一些其他的事让我烦心。慢慢来吧，急不来。\n未来 # 以后该怎么如何进步？\n其他 # 继续学习，共勉！\n"},{"id":53,"href":"/posts/linux/linux0.11-read-code/","title":"Linux0.11源码学习环境配置与相关资源汇总","section":"Blog","content":"记录学习Linux操作系统实现时候使用过的资源，一部分笔记，以及调试中用到的技巧。内容有点乱，仅供个人使用。\nlinux history # linux 0.11 linux 0.95 实现虚拟文件系统 linux 0.96 实现网络接口 linux # linux采用分段+分页机制结合管理内存\nlinux 调试方法 # linux0.11 调试方法 gdb tools/system target remote localhost:1234 gdb常用命令 b: 下中斷點 info b :u 列出目前中断点，也可简写成\u0026#34;i b\u0026#34; continue(c) 继续执行直到下一个中断点或结束 list(l): 列出目前上下文 step(s): 单步 (会进入 funciton) next(n) : 单步 (不会进入 funciton) until(u) 跳离一个 while for 循环 print(p): 显示某变量，如 p str info register(i r) : 显示 CPU 的 register GDB 打印出内存中的內容，格式為 x/nyz，其中 n: 要印出的數量 y: 显示的格式，可为C( char), d(整数), x(hex) z: 单位，可为 b(byte), h(16bit), w(32bit) cgdb\t可显示为上半部分为代码，下半部分命令部分 cgdb tools/system* [linux-0.11启动过程描述](http://labrick.cc/2015/08/13/linux-0-11-boot/) * [Linux0.11启动过程](http://linux.chinaunix.net/techdoc/install/2007/04/10/954810.shtml) * [80386保护模式的本质](http://www.jianshu.com/p/1cea7dc5d6b7) * [linux虚拟地址到线性地址的转化](http://luodw.cc/2016/02/17/address/) * [Linux内存寻址之分段机制-linux回避了分段机制](http://blog.xiaohansong.com/2015/10/03/Linux%E5%86%85%E5%AD%98%E5%AF%BB%E5%9D%80%E4%B9%8B%E5%88%86%E6%AE%B5%E6%9C%BA%E5%88%B6/) * [Linux内存寻址之分页机制/](http://blog.xiaohansong.com/2015/10/05/Linux%E5%86%85%E5%AD%98%E5%AF%BB%E5%9D%80%E4%B9%8B%E5%88%86%E9%A1%B5%E6%9C%BA%E5%88%B6/) * [逻辑地址、线性地址、物理地址和虚拟地址](http://www.voidcn.com/blog/will130/article/p-5705051.html) * [Intel 80386 程序员参考手册](http://www.kancloud.cn/wizardforcel/intel-80386-ref-manual/123838) * [linux0.11内核之文件系统](http://harpsword.leanote.com/post/Untitled-563d6103ab6441584f000164) source # 80386内存访问公式 # 32位线性地址 = 段基地址(32位) + 段内偏移(32位)\n48bit = 16 + 32 16位地段选择子 + 32虚拟地址 -\u0026gt; 32线性地址 32线性地址 -\u0026gt; 物理地址\n"}]